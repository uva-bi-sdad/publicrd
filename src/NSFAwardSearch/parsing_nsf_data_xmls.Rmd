---
title: "R Notebook"
output: html_notebook
---
```{python}
#############
#This code compiles the XML zip file into a csv for all awards in NSF Award Search 2016
#The raw data can be found in data/original/NSFAwardSearch--this code assumes zipped.
#The resulting file can be found in data/working/NSFAwardSearch
################
#NSF Award Search downloads a zip with an XML file for each grant issued.
#Those XMLs have built in hierarchies--so for example, under 'Investigator', there are all the embedded attributres of the 
#'Investigator', like Last Name. That can be tough, since you have to go through the whole "tree", 
#Seeing where they're embedded, so they can be parased.
#These attributes can differ between files--in particular, 
#You might have multiple investigators on one file but not another
#There's only a few attributes that can be duplicated according to the layout on the NSF website
################

#Import necessary libraries
import zipfile
import xml.etree.ElementTree as ET
import pandas as pd

###############
#Custom recursive function to traverse the tree and pull all nodes, 
#even if they're embedded under other nodes. That is, Investigator is a node (field) with other nodes (subfields) under it, like Last Name
#And Role. We don't just want a field that says "last name"--we want to know its the last name of the investigator
#So we add on the name of the node its embedded within to maintain our knowledge of the structure.
###############
def check_subfields(x,header=''):
    #This condition checks if its a list or not--if its a list, that means there's nodes embdeeded, that we want the
    #substructure of. 
    if len(x)>0:
        for y in x:
            #Recursion, adding the name of the parent node to the name of this variable 
            #(e.g. "Investigator" to "Investigator Last Name"), so as recursion happens, structure is maintained.
            check_subfields(y,header=header+x.tag)
    #This means this is only a single node, so add it to the dict with the variable name and value.
    else:
        if header=='':
            my_dict.update({x.tag:x.text})
        else:
            my_dict.update({header+x.tag:x.text})

###################
#Loop through files
#################
uber_series=[] #Will hold the list of series, where each series is one grant.
my_zip=zipfile.ZipFile('C:\\Users\\Samantha Cohen\\Downloads\\2016.zip') #Your file name here
for x in my_zip.namelist():
    tree=ET.parse(my_zip.open(x)) #Parse the xml file within the zipfile into its tree structure
    root=tree.getroot()[0] #0 is the node "Award", which is the overarching node.
    my_dict={} #There is where the values of each field will be held, as a dictionary
    #Some fields are good to go as is, because they are non-duplicated in each file
    #So we can simply pull out the unique structure using our function.
    for child in [child for child in root if not child.tag in ['Investigator','Institition','FoaInformation','ProgramElement','ProgramReference']]:
        check_subfields(child)
    #For these other fields, each file could contain an unknown number of nodes with identical titles
    #To have unique names for each column in our CSV, we need to enumerate them (see foot note)
    for node_name in ['Investigator','Institition','FoaInformation','ProgramElement','ProgramReference']:
        count=1 #Every time one of these fields is found more than once, we add a unique identifier indicating order
        for subnode in root.findall(node_name): #EG Investigator has First Name and Last Name
            #We don't need the recursive function here to check for subnodes, because it runs slow 
            #& there are no subnodes below the levels above according to website
            for x in range(len(subnode)): 
                #Add to Dictionary, Embedding Structure
                my_dict[node_name+subnode[x].tag+str(count)]=subnode[x].text
            count+=1
    #Create a series from the dictionary (easy!)
    uber_series.append(pd.Series(my_dict))

#Close zip files
my_zip.close()
#Make a single CSV--done!
pd.DataFrame(uber_series).to_csv('2016NSFAwardSearch.csv')

#Footnote
    #NOTE: Python will rename duplicate columns for you anyway
    #(e.g. if you have two columns named Investigator, it will rename it
    #Investigator_1 and Investigator_2 or something like that
    #But because each file structure is unique, some factors may be missing from one duplicate-named node to another
    #E.g. If there's 3 investigators,a ll with a first name
    #but the 2nd investigator may not have a last name listed, but the first and third do.
    #When this is made into a series, Python would rename the 3 investigators first names as FirstName1, FirstName2, First Name 3,
    #And the 2 last name files to 'LastName1' and LastName2, even though LastName2 goes with FirstName3.
```

