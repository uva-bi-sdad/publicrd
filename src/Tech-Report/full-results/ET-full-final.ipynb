{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emerging topics final code - Full (new code cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression as lm\n",
    "\n",
    "import matplotlib.collections as mcol\n",
    "from matplotlib.legend_handler import HandlerLineCollection, HandlerTuple\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full corpus\n",
    "df = pd.read_pickle(\"/project/biocomplexity/sdad/projects_data/ncses/prd/Tech-Report/FR_meta_and_final_tokens_21SEPT14.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needed for LDA, NMF (all from Scikit-Learn) is one string per document (not a list of strings)\n",
    "\n",
    "text = []\n",
    "docs = df[\"final_tokens\"]\n",
    "\n",
    "for abstract in docs:\n",
    "    text.append(\" \".join(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLDAvars(docs):\n",
    "\n",
    "    # Create the variables needed for LDA from df[final_frqwds_removed]: dictionary (id2word), corpus\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "    #Filter words to only those found in at least a set number of documents (min_appearances)\n",
    "    id2word.filter_extremes(no_below=20, no_above=0.6)\n",
    "    \n",
    "    # filter out stop words - \"use\" already filtered out by previous line\n",
    "    id2word.filter_tokens(bad_ids=[id2word.token2id['research'], id2word.token2id['study'], \\\n",
    "                               id2word.token2id['project']])\n",
    "\n",
    "    return id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = createLDAvars(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format topics as a \"list of list of strings\".\n",
    "# Needed for topic coherence function in Gensim\n",
    "\n",
    "# function modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def list_topics(topic_term_dist, vectorizer, top_n=10):\n",
    "\n",
    "    #input. top_n: how many words to list per topic.  If -1, then list all words.\n",
    "       \n",
    "    topic_words = []\n",
    "    \n",
    "    for idx, topic in enumerate(topic_term_dist):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "            \n",
    "        if top_n == -1:   \n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1]])\n",
    "        else:\n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_topics(topic_term_mat, vectorizer, top_n=10):\n",
    "\n",
    "    #input. top_n: how many words to list per topic.  If -1, then list all words.\n",
    "       \n",
    "    topic_words = []\n",
    "    \n",
    "    for idx, topic in enumerate(topic_term_mat):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "            \n",
    "        if top_n == -1:   \n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1]])\n",
    "        else:\n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "    \n",
    "    str_wds = []\n",
    "    \n",
    "    for wds in topic_words:\n",
    "        str_wds.append(\", \".join(wds))    \n",
    "    \n",
    "    return str_wds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF\n",
    "- 50 topics, random state = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_wds = ['research', 'study', 'project']  # use will be eliminated by max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct term document matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.6, min_df=20, lowercase=False, stop_words=stop_wds)\n",
    "tf_idf = tfidf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time topic model run\n",
    "num_topics = 50\n",
    "\n",
    "nmf_model = NMF(n_components = num_topics, random_state = 1)\n",
    "doc_topic = nmf_model.fit_transform(tf_idf)\n",
    "topic_term = nmf_model.components_\n",
    "\n",
    "with open(\"/project/biocomplexity/sdad/projects_data/ncses/prd/nmf_full_50.pkl\",\"wb\") as f:\n",
    "    pickle.dump((doc_topic, topic_term), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in fit topic model\n",
    "#num_topics = 50\n",
    "\n",
    "#with open(\"/project/biocomplexity/sdad/projects_data/ncses/prd/nmf_full_50.pkl\", \"rb\") as f:\n",
    "#    res = pickle.load(f)\n",
    "    \n",
    "#doc_topic = res[0]\n",
    "#topic_term = res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list_topics(topic_term, tfidf_vectorizer, top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence\n",
    "#cm = CoherenceModel(topics = topics,\n",
    "#                    #corpus = corpus,\n",
    "#                    dictionary = id2word,\n",
    "#                    texts = docs, \n",
    "#                    coherence = 'c_v', \n",
    "#                    processes = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emerging topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Year from PROJECT_START_DATE\n",
    "\n",
    "def getYear(a):   \n",
    "    a = str(a)\n",
    "    if a.find(\"/\"):\n",
    "        splitdate = a.split(\"/\")\n",
    "        if len(splitdate) == 3:\n",
    "            a = splitdate[2]\n",
    "        else:\n",
    "            a = splitdate[0]\n",
    "    year = str(a)\n",
    "    return year\n",
    "\n",
    "df['START_YEAR'] = df['PROJECT_START_DATE'].apply(getYear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start year to doc topic matrix\n",
    "topic_frame = pd.DataFrame(doc_topic, columns=[\"Topic\"+\" \"+str(i) for i in range(num_topics)])\n",
    "topic_frame[\"START_YEAR\"] = df[\"START_YEAR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute count of projects with weight > 0 for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_proj_count = topic_frame.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of docs with weight > 0\n",
    "topic_count_bool = (topic_proj_count.iloc[:,0:num_topics] > 0)\n",
    "topic_count_bool[\"START_YEAR\"] = topic_proj_count[\"START_YEAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of docs with weight > 0 BY YEAR\n",
    "topic_counts = topic_count_bool.groupby(\"START_YEAR\").sum().reset_index()\n",
    "topic_counts[\"START_YEAR\"] = topic_counts[\"START_YEAR\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get docs between 2010 and 2019\n",
    "topic_counts_filt = topic_counts[topic_counts[\"START_YEAR\"] > 2009] \n",
    "topic_counts_filt = topic_counts_filt[topic_counts_filt[\"START_YEAR\"] < 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_by_topic = topic_counts_filt.sum()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate mean topic weight by year and standard errors on means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_frame_se = topic_frame.groupby(\"START_YEAR\").sem().reset_index()\n",
    "topic_frame_se = topic_frame_se.sort_values(by = \"START_YEAR\")\n",
    "topic_frame_se[\"START_YEAR\"] = topic_frame_se[\"START_YEAR\"].astype(int)\n",
    "\n",
    "topic_wts_se_2010_2019 = topic_frame_se[topic_frame_se[\"START_YEAR\"] > 2009] \n",
    "#topic_wts_se_2010_2019 = topic_wts_se_2010_2019[topic_wts_se_2010_2019[\"START_YEAR\"] < 2020]\n",
    "topic_wts_se_2010_2019 = topic_wts_se_2010_2019[topic_wts_se_2010_2019[\"START_YEAR\"] < 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_frame = topic_frame.groupby(\"START_YEAR\").mean().reset_index()\n",
    "topic_frame = topic_frame.sort_values(by = \"START_YEAR\")\n",
    "topic_frame[\"START_YEAR\"] = topic_frame[\"START_YEAR\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter topic_frame for years 2010 - 2019\n",
    "\n",
    "topic_wts_2010_2019 = topic_frame[topic_frame[\"START_YEAR\"] > 2009] \n",
    "#topic_wts_2010_2019 = topic_wts_2010_2019[topic_wts_2010_2019[\"START_YEAR\"] < 2020] \n",
    "topic_wts_2010_2019 = topic_wts_2010_2019[topic_wts_2010_2019[\"START_YEAR\"] < 2019] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit OLS and get slope, SE, p-value\n",
    "topic_slopes = []\n",
    "topic_slopes_se = []\n",
    "topic_slopes_pval = []\n",
    "\n",
    "lm_x = topic_wts_2010_2019[\"START_YEAR\"].values.reshape(-1,1)\n",
    "lm_x = sm.add_constant(lm_x)\n",
    "\n",
    "for i in range(1,num_topics+1):\n",
    "\n",
    "    linear_fit = sm.OLS(topic_wts_2010_2019.iloc[:,i].values.reshape(-1,1),lm_x).fit()\n",
    "    \n",
    "    topic_slopes.append(linear_fit.params[1])\n",
    "    topic_slopes_se.append(linear_fit.bse[1])\n",
    "    topic_slopes_pval.append(linear_fit.pvalues[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ols_res = pd.DataFrame(\n",
    "    {\"Slope\": topic_slopes, \n",
    "     \"SE\": topic_slopes_se,\n",
    "     \"p-value\": topic_slopes_pval\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_topics = pd.DataFrame()\n",
    "\n",
    "topic_label_num = range(1, num_topics + 1)\n",
    "topic_label = [\"FR\" + str(x) for x in topic_label_num]\n",
    "reg_topics[\"Topic Label\"] = topic_label\n",
    "\n",
    "reg_topics[\"Topic Words\"] = str_topics(topic_term, tfidf_vectorizer, top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_res = pd.concat([reg_topics, topic_ols_res], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_res.to_csv(\"full_50_topics_2018_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create plot - top 10 hot and cold topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend\n",
    "leg = str_topics(topic_term, tfidf_vectorizer, top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_df = reg_topics.set_index('Topic Words')\n",
    "legend_df = legend_df.loc[leg]\n",
    "leg_topic_label = legend_df['Topic Label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_label = [i + \": \" + j for i, j in zip(leg_topic_label, leg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect slopes to sort\n",
    "topic_results = {}\n",
    "\n",
    "for i in range(1,num_topics+1):\n",
    "    linear_fit = lm().fit(topic_wts_2010_2019[\"START_YEAR\"].values.reshape(-1,1), topic_wts_2010_2019.iloc[:,i].values.reshape(-1,1))\n",
    "    topic_results[i] = [linear_fit.coef_[0][0], reg_topics.loc[i-1, \"Topic Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(x):\n",
    "    return sorted(x.items(), key=lambda l: l[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_slopes = sort_dict(topic_results)\n",
    "top_slopes = [x[0] for x in sort_slopes[:10]]\n",
    "bottom_slopes = [x[0] for x in sort_slopes[-10:]]\n",
    "topnbot_slopes = top_slopes + bottom_slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hottest and coldest topics on separate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Graphs\n",
    "\n",
    "color_list = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "line_return = []\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Top 10 Topics with Increasing Weights', fontsize=16)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.grid(True, color = \"whitesmoke\")\n",
    "line = [[(0, 0)]]\n",
    "i = 0\n",
    "for n in top_slopes:\n",
    "    zorder = 10\n",
    "    color = color_list[i] # \"#D3D3D3\"   \n",
    "    if i == 0:\n",
    "        zorder = 10 #20\n",
    "        color = color_list[i];\n",
    "    linear_fit = lm().fit(topic_wts_2010_2019[\"START_YEAR\"].values.reshape(-1,1), topic_wts_2010_2019.iloc[:,n].values.reshape(-1,1))\n",
    "    #ax.plot(topic_wts_2010_2019[\"START_YEAR\"], (topic_wts_2010_2019[\"START_YEAR\"]*linear_fit.coef_[0][0])+linear_fit.intercept_,linestyle = 'dashed', color = color_list[i])\n",
    "    ax.plot(topic_wts_2010_2019[\"START_YEAR\"], topic_wts_2010_2019.iloc[:,n], '-o', color = color, zorder = zorder)\n",
    "    ax.errorbar(topic_wts_2010_2019[\"START_YEAR\"], topic_wts_2010_2019.iloc[:,n], \n",
    "            yerr = np.array(topic_wts_se_2010_2019.iloc[:,n]), fmt = \"o\", color = color, zorder = zorder)\n",
    "    #line_return.append(mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=[color_list[i],color_list[i]]))\n",
    "    line_return.append(mcol.LineCollection(line, linestyles=['solid'], colors=[color]))\n",
    "    i+=1\n",
    "    #leg.append(\"Topic %d\"%(n+1))\n",
    "\n",
    "plt.xticks(np.arange(topic_wts_2010_2019[\"START_YEAR\"].min(), topic_wts_2010_2019[\"START_YEAR\"].max()+1, 1.0))\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Mean Topic Weight', fontsize=14)\n",
    "plt.ylim(bottom = 0, top = 0.0035)\n",
    "#ax.yaxis.set_label_coords(-0.14,0.5)\n",
    "\n",
    "# set up the proxy artist\n",
    "lc = mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=['blue','blue'])\n",
    "lc2 = mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=['orange','orange'])\n",
    "# create the legend\n",
    "#plt.legend(line_return, [leg[x-1] for x in top_slopes], handler_map={type(line_return[0]): HandlerDashedLines()},\n",
    "#          handlelength=2, handleheight=2,bbox_to_anchor=(1.05, 0.7, 0.3, 0.2), loc='upper left')\n",
    "\n",
    "plt.legend([leg_label[x-1] for x in top_slopes], bbox_to_anchor=(0.35, -0.35, 0.3, 0.2), \n",
    "           loc='upper center', fontsize = 'large', frameon = False)\n",
    "plt.savefig(\"full_increasing_50_topics_2018.png\", dpi = 800, bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Make Graphs\n",
    "#leg = []\n",
    "\n",
    "#bottom_slopes.reverse()  # so the plot legend has the coldest listed first\n",
    "\n",
    "line_return = []\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Top 10 Topics with Decreasing Weights', fontsize=16)\n",
    "ax = fig.add_subplot(111)\n",
    "plt.grid(True, color = \"whitesmoke\")\n",
    "i = 0\n",
    "for n in bottom_slopes:\n",
    "    zorder = 10\n",
    "    color = color_list[i]  #\"#D3D3D3\"  \n",
    "    if i == 4:\n",
    "        zorder = 10 #20\n",
    "        color = color_list[i]\n",
    "    linear_fit = lm().fit(topic_wts_2010_2019[\"START_YEAR\"].values.reshape(-1,1), topic_wts_2010_2019.iloc[:,n].values.reshape(-1,1))\n",
    "    #ax.plot(topic_wts_2010_2019[\"START_YEAR\"], (topic_wts_2010_2019[\"START_YEAR\"]*linear_fit.coef_[0][0])+linear_fit.intercept_,linestyle = 'dashed', color = color_list[i+5])\n",
    "    ax.plot(topic_wts_2010_2019[\"START_YEAR\"], topic_wts_2010_2019.iloc[:,n], '-o', color=color, zorder = zorder)\n",
    "    ax.errorbar(topic_wts_2010_2019[\"START_YEAR\"], topic_wts_2010_2019.iloc[:,n], \n",
    "            yerr = np.array(topic_wts_se_2010_2019.iloc[:,n]), fmt = \"o\", color=color, zorder = zorder)\n",
    "    #line_return.append(mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=[color_list[i+5],color_list[i+5]]))\n",
    "    line_return.append(mcol.LineCollection(line, linestyles=['solid'], colors=[color]))\n",
    "    i+=1\n",
    "    #leg.append(\"Topic %d\"%(n+1))\n",
    "\n",
    "plt.xticks(np.arange(topic_wts_2010_2019[\"START_YEAR\"].min(), topic_wts_2010_2019[\"START_YEAR\"].max()+1, 1.0))\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Mean Topic Weight', fontsize=14)\n",
    "plt.ylim(bottom = 0, top = 0.0035)\n",
    "#ax.yaxis.set_label_coords(-0.14,0.5)\n",
    "\n",
    "\n",
    "# set up the proxy artist\n",
    "lc = mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=['blue','blue'])\n",
    "lc2 = mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=['orange','orange'])\n",
    "\n",
    "# create the legend\n",
    "#plt.legend(line_return, [leg[x-1] for x in bottom_slopes], handler_map={type(line_return[0]): HandlerDashedLines()},\n",
    "#          handlelength=2, handleheight=2,bbox_to_anchor=(1.05, 0.7, 0.3, 0.2), loc='upper left')\n",
    "\n",
    "plt.legend([leg_label[x-1] for x in bottom_slopes], bbox_to_anchor=(0.35, -0.35, 0.3, 0.2), \n",
    "           loc='upper center', fontsize = 'large', frameon = False)\n",
    "plt.savefig(\"full_decreasing_50_topics_2018.png\", dpi = 800, bbox_inches = \"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
