{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-27 09:49:13 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-04-27 09:49:13 INFO: Use device: cpu\n",
      "2020-04-27 09:49:13 INFO: Loading: tokenize\n",
      "2020-04-27 09:49:13 INFO: Loading: pos\n",
      "2020-04-27 09:49:13 INFO: Loading: lemma\n",
      "2020-04-27 09:49:13 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#This is a very clunky way to \"parallelize\" lemmatization--\n",
    "#by running this script with different settings of 'i' on separate spun-up instances of one-node, saving them, and later combining them\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import TextCleaning\n",
    "import LDAvariables\n",
    "import stanza\n",
    "import gensim\n",
    "os.chdir('/home/sc2pg/src/prnd/publicrd/data/prd/RND Topic Modelling')\n",
    "\n",
    "# load saved df.  df['working_abstract'] contains clean text.\n",
    "df = pd.read_pickle(\"./clean_dataset.pkl\")\n",
    "df.reset_index(inplace = True)\n",
    "df.rename(columns={'index':'original index'}, inplace=True)\n",
    "\n",
    "batch_size=len(df)/20\n",
    "i=2 #I ran this in twenty instances, where the value of i changes and then all datasets are combined\n",
    "\n",
    "wa='working_abstract'\n",
    "nlp = stanza.Pipeline(lang='en',processors='tokenize,pos,lemma',tokenize_batch_size=500,lemma_batch_size=500)\n",
    "\n",
    "    \n",
    "def lemmatize_stanford(doc,pretokened=False,keep_numbers=True):\n",
    "    \"\"\"lemmatizes and tokenizes an abstract, keeping only lemmas that are noun, verb, adj, or adverb,\n",
    "    with option to include numbers, and non-lemmaed pronouns, 'X', and 'intj' if pretokened, dont use this function, as it hasnt been adapted for it\"\"\"\n",
    "    assert not pretokened\n",
    "    new_tokens=[]\n",
    "    if doc==' ':\n",
    "        return np.nan\n",
    "    else: \n",
    "        processed=nlp(doc)\n",
    "        for sent in processed.sentences:\n",
    "            for word in sent.words:\n",
    "                if word.pos in ['NOUN','VERB','ADJ','ADV']:\n",
    "                    new_tokens.append(word.lemma)\n",
    "                elif word.pos=='NUM' and keep_numbers:\n",
    "                    new_tokens.append(word.lemma)\n",
    "                elif word.pos in ['PROPN','X','INTJ']: \n",
    "                    new_tokens.append(word.text)\n",
    "        return new_tokens\n",
    "\n",
    "#to compare these functions, try test cases 463, 40, and 2247 (iloc)\n",
    "\n",
    "#To generate a batch, uncomment next two lines\n",
    "tokened_lemma_docs=df[wa].iloc[int(i*batch_size):int((i+1)*batch_size)].apply(lambda x: lemmatize_stanford(x,keep_numbers=False))\n",
    "#tokened_lemma_docs.to_pickle(\"src/prnd/publ/icrd/data/prd/RND Topic Modelling/lemma_docs/lemma_docs_\"+str(i)+\".pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
