{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval - Pandemic Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import TextCleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CLEANED AND PROCESSED DATA PULL - processed abstracts are in docs column.  Will need to create new\n",
    "# corpus and id2word\n",
    "\n",
    "# import NSF data\n",
    "#f = open('../../data/prd/RND Topic Modelling/nsf_stanford_lemma.sav', 'rb')\n",
    "\n",
    "# import entire dataset\n",
    "f = open('../../data/prd/RND Topic Modelling/lda_data_stanford_lemma.sav', 'rb')\n",
    "\n",
    "[temp1, temp2, docs] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# corpus - word frequency in docs\n",
    "# id2word - dictionary\n",
    "# docs - lemmatized abstracts\n",
    "\n",
    "# from Sam's code\n",
    "\n",
    "#docs=docs.reset_index(drop=True)  \n",
    "docs = docs.loc[docs.apply(lambda x: len(x)>0)] #No duplicates removed here\n",
    "docs = docs.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input needed for doc-term matrix creation is one string per document (not a list of strings).  \n",
    "\n",
    "text = []\n",
    "for doc in docs:\n",
    "    text.append(\" \".join(doc))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 nulls in  ABSTRACT . These rows removed.\n",
      "11 duplicate abstracts removed\n",
      "0 project ID duplicates - not removed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original index</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>FY</th>\n",
       "      <th>FIRST_CHAR</th>\n",
       "      <th>LAST_CHAR</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>AGENCY</th>\n",
       "      <th>IC_CENTER</th>\n",
       "      <th>PROJECT_NUMBER</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>PROJECT_TERMS</th>\n",
       "      <th>CONTACT_PI_PROJECT_LEADER</th>\n",
       "      <th>OTHER_PIS</th>\n",
       "      <th>ORGANIZATION_NAME</th>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <th>FY_TOTAL_COST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>89996</td>\n",
       "      <td>This is a project to explore Game-based, Metap...</td>\n",
       "      <td>2008</td>\n",
       "      <td>This is a project to explore Game-based, Metap...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0814512</td>\n",
       "      <td>RUI: CYGAMES: CYBER-ENABLED TEACHING AND LEARN...</td>\n",
       "      <td>Achievement; analog; base; Cognitive Science; ...</td>\n",
       "      <td>REESE, DEBBIE D</td>\n",
       "      <td>CARTER, BEVERLY; WOOD, CHARLES; HITT, BEN</td>\n",
       "      <td>WHEELING JESUIT UNIVERSITY</td>\n",
       "      <td>47.076</td>\n",
       "      <td>1999467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>89997</td>\n",
       "      <td>Institution: Franklin Institute Science Museum...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Institution: Franklin Institute Science Museum...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0741659</td>\n",
       "      <td>ARIEL - AUGMENTED REALITY FOR INTERPRETIVE AND...</td>\n",
       "      <td>Active Learning; Child; Computer software; des...</td>\n",
       "      <td>SNYDER, STEVEN</td>\n",
       "      <td>ELINICH, KAREN; YOON, SUSAN</td>\n",
       "      <td>FRANKLIN INSTITUTE</td>\n",
       "      <td>47.076</td>\n",
       "      <td>1799699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>89998</td>\n",
       "      <td>Through programs (including small group conver...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Through programs (including small group conver...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0813522</td>\n",
       "      <td>BRIGHTER FUTURES: PUBLIC DELIBERATION ABOUT TH...</td>\n",
       "      <td>Address; Age; Birth; Brain; Caregivers; Child;...</td>\n",
       "      <td>FINK, LAURIE KLEINBAUM</td>\n",
       "      <td>CADIGAN, KAREN; ELLENBOGEN, KIRSTEN</td>\n",
       "      <td>SCIENCE MUSEUM OF MINNESOTA</td>\n",
       "      <td>47.076</td>\n",
       "      <td>1505858.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>89999</td>\n",
       "      <td>In partnership with the American Chemical Soci...</td>\n",
       "      <td>2008</td>\n",
       "      <td>In partnership with the American Chemical Soci...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0838627</td>\n",
       "      <td>FOSTERING US-INTERNATIONAL COLLABORATIVE PARTN...</td>\n",
       "      <td>Advanced Development; American; Chemicals; Che...</td>\n",
       "      <td>JOST, JOHN W</td>\n",
       "      <td>MILLER, BRADLEY; BOWMAN, KATHERINE</td>\n",
       "      <td>INTERNATIONAL UNION OF PURE AND APPLIED CHEMISTRY</td>\n",
       "      <td>47.049</td>\n",
       "      <td>51000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>90000</td>\n",
       "      <td>Amphibian populations around the world are exp...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Amphibian populations around the world are exp...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0815315</td>\n",
       "      <td>COLLABORATIVE RESEARCH: EVOLUTION OF AMPHIBIAN...</td>\n",
       "      <td>Amphibia; Central America; Communicable Diseas...</td>\n",
       "      <td>ZAMUDIO, KELLY R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CORNELL UNIVERSITY ITHACA</td>\n",
       "      <td>47.074</td>\n",
       "      <td>370996.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original index  PROJECT_ID  \\\n",
       "0               0       89996   \n",
       "1               1       89997   \n",
       "2               2       89998   \n",
       "3               3       89999   \n",
       "4               4       90000   \n",
       "\n",
       "                                            ABSTRACT    FY  \\\n",
       "0  This is a project to explore Game-based, Metap...  2008   \n",
       "1  Institution: Franklin Institute Science Museum...  2008   \n",
       "2  Through programs (including small group conver...  2008   \n",
       "3  In partnership with the American Chemical Soci...  2008   \n",
       "4  Amphibian populations around the world are exp...  2008   \n",
       "\n",
       "                                          FIRST_CHAR LAST_CHAR DEPARTMENT  \\\n",
       "0  This is a project to explore Game-based, Metap...         .        NSF   \n",
       "1  Institution: Franklin Institute Science Museum...         .        NSF   \n",
       "2  Through programs (including small group conver...         .        NSF   \n",
       "3  In partnership with the American Chemical Soci...         .        NSF   \n",
       "4  Amphibian populations around the world are exp...         .        NSF   \n",
       "\n",
       "  AGENCY IC_CENTER PROJECT_NUMBER  \\\n",
       "0    NSF       NaN        0814512   \n",
       "1    NSF       NaN        0741659   \n",
       "2    NSF       NaN        0813522   \n",
       "3    NSF       NaN        0838627   \n",
       "4    NSF       NaN        0815315   \n",
       "\n",
       "                                       PROJECT_TITLE  \\\n",
       "0  RUI: CYGAMES: CYBER-ENABLED TEACHING AND LEARN...   \n",
       "1  ARIEL - AUGMENTED REALITY FOR INTERPRETIVE AND...   \n",
       "2  BRIGHTER FUTURES: PUBLIC DELIBERATION ABOUT TH...   \n",
       "3  FOSTERING US-INTERNATIONAL COLLABORATIVE PARTN...   \n",
       "4  COLLABORATIVE RESEARCH: EVOLUTION OF AMPHIBIAN...   \n",
       "\n",
       "                                       PROJECT_TERMS  \\\n",
       "0  Achievement; analog; base; Cognitive Science; ...   \n",
       "1  Active Learning; Child; Computer software; des...   \n",
       "2  Address; Age; Birth; Brain; Caregivers; Child;...   \n",
       "3  Advanced Development; American; Chemicals; Che...   \n",
       "4  Amphibia; Central America; Communicable Diseas...   \n",
       "\n",
       "  CONTACT_PI_PROJECT_LEADER                                  OTHER_PIS  \\\n",
       "0           REESE, DEBBIE D  CARTER, BEVERLY; WOOD, CHARLES; HITT, BEN   \n",
       "1            SNYDER, STEVEN                ELINICH, KAREN; YOON, SUSAN   \n",
       "2    FINK, LAURIE KLEINBAUM        CADIGAN, KAREN; ELLENBOGEN, KIRSTEN   \n",
       "3              JOST, JOHN W         MILLER, BRADLEY; BOWMAN, KATHERINE   \n",
       "4          ZAMUDIO, KELLY R                                        NaN   \n",
       "\n",
       "                                   ORGANIZATION_NAME CFDA_CODE  FY_TOTAL_COST  \n",
       "0                         WHEELING JESUIT UNIVERSITY    47.076      1999467.0  \n",
       "1                                 FRANKLIN INSTITUTE    47.076      1799699.0  \n",
       "2                        SCIENCE MUSEUM OF MINNESOTA    47.076      1505858.0  \n",
       "3  INTERNATIONAL UNION OF PURE AND APPLIED CHEMISTRY    47.049        51000.0  \n",
       "4                          CORNELL UNIVERSITY ITHACA    47.074       370996.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ORIGINAL DATA PULL\n",
    "\n",
    "# pull in original abstracts\n",
    "\n",
    "raw_df=pd.read_csv('../../data/prd/RND Topic Modelling/abstracts_federal_reporter_combined.csv',engine='python')\n",
    "\n",
    "# remove null abstracts and duplicates\n",
    "\n",
    "df = TextCleaning.remove_nulls(raw_df, \"ABSTRACT\")\n",
    "df = TextCleaning.remove_duplicates(df)\n",
    "\n",
    "df.reset_index(inplace = True)\n",
    "df.rename(columns={'index':'original index'}, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df[\"ABSTRACT\"]\n",
    "\n",
    "# input needed for doc-term matrix creation is one string per document (not a list of strings).  Original data is\n",
    "# already in this form!\n",
    "\n",
    "text = df[\"ABSTRACT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed for all approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query vector \n",
    "\n",
    "def create_query(words, terms):\n",
    "    \n",
    "    # words: search query words\n",
    "    # terms: terms in corpus\n",
    "    \n",
    "    q = np.zeros(len(terms))  # number of terms\n",
    "\n",
    "    idx = []\n",
    "    for word in query_words:\n",
    "        idx.append(terms.index(word))\n",
    "\n",
    "    q[idx] = 1\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_abstracts(docs, scores, top_n):\n",
    "    \n",
    "    '''\n",
    "    docs: Series that contains abstract\n",
    "    scores: scores of abstracts\n",
    "    top_n: return the top_n abstracts given by idx, if -1 return all abstracts\n",
    "    '''\n",
    "    # sort scores in descending order\n",
    "    scores_sorted_idx = np.argsort(scores)[::-1]\n",
    "    \n",
    "    if top_n == -1:\n",
    "        ix = scores_sorted_idx\n",
    "    else:\n",
    "        ix = scores_sorted_idx[:top_n]\n",
    "    \n",
    "    print(ix[0:10])\n",
    "    \n",
    "    return ix, docs[ix]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_df(abstracts, scores):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"abstracts\"] = abstracts\n",
    "    df[\"scores\"] = scores\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact word matches - Frequency Count Document-Term Matrix\n",
    "\n",
    "This will return all abstracts in the corpus with exact word matches to the query.  A query is just a list of words to search for.\n",
    "\n",
    "Results will be return in sorted order of how high the query scores with each abstract. A high score means more occurences of the query words in the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrix based on count frequencies\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "doc_term_matrix = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ackowledge',\n",
       " 'ackowledgedne',\n",
       " 'ackr',\n",
       " 'ackr1',\n",
       " 'ackr3',\n",
       " 'ackr3undergo',\n",
       " 'ackr4',\n",
       " 'acl',\n",
       " 'acl2',\n",
       " 'acl3',\n",
       " 'acl5',\n",
       " 'acl_accesscontrollist',\n",
       " 'acl_accesscontrollist_collaborate',\n",
       " 'acl_injure',\n",
       " 'acl_intact',\n",
       " 'acl_reconstruct',\n",
       " 'acl_reconstruction',\n",
       " 'acl_rupture',\n",
       " 'acl_tear',\n",
       " 'acl_transection',\n",
       " 'aclacinomycin',\n",
       " 'aclaim',\n",
       " 'aclam',\n",
       " 'aclam_board',\n",
       " 'aclam_certify',\n",
       " 'aclaris',\n",
       " 'aclass',\n",
       " 'aclassic',\n",
       " 'aclassical',\n",
       " 'aclassification',\n",
       " 'aclassroom',\n",
       " 'aclc',\n",
       " 'aclcan',\n",
       " 'acld',\n",
       " 'acldl',\n",
       " 'aclean',\n",
       " 'aclear',\n",
       " 'aclearance',\n",
       " 'aclearer',\n",
       " 'acleavage',\n",
       " 'acli',\n",
       " 'aclient',\n",
       " 'aclimate',\n",
       " 'aclimation',\n",
       " 'aclinic',\n",
       " 'aclinical',\n",
       " 'aclinically',\n",
       " 'aclinically_relevant',\n",
       " 'aclinician',\n",
       " 'aclinjury',\n",
       " 'acliprogram',\n",
       " 'aclm',\n",
       " 'aclministrative',\n",
       " 'aclonal',\n",
       " 'aclose',\n",
       " 'aclosely',\n",
       " 'acloser',\n",
       " 'aclot',\n",
       " 'acloud',\n",
       " 'aclp',\n",
       " 'aclr',\n",
       " 'aclrand',\n",
       " 'aclreconstruction',\n",
       " 'aclrepair',\n",
       " 'acls',\n",
       " 'aclsurgery',\n",
       " 'aclt',\n",
       " 'acltear',\n",
       " 'acluster',\n",
       " 'aclutter',\n",
       " 'acly',\n",
       " 'aclzip',\n",
       " 'acm',\n",
       " 'acm1',\n",
       " 'acm1degradation',\n",
       " 'acm_bcb',\n",
       " 'acm_ccs',\n",
       " 'acm_digital',\n",
       " 'acm_ieee',\n",
       " 'acm_ieee_symposium',\n",
       " 'acm_mobicom',\n",
       " 'acm_mobihoc',\n",
       " 'acm_mobisys',\n",
       " 'acm_multimedia',\n",
       " 'acm_nanocom',\n",
       " 'acm_siam',\n",
       " 'acm_siam_symposium',\n",
       " 'acm_sigaccess_conference',\n",
       " 'acm_sigbio',\n",
       " 'acm_sigchi',\n",
       " 'acm_sigcomm',\n",
       " 'acm_sigcomm_flagship',\n",
       " 'acm_sigcomm_internet',\n",
       " 'acm_sigcse',\n",
       " 'acm_siggraph',\n",
       " 'acm_sigkdd',\n",
       " 'acm_sigkdd_conference',\n",
       " 'acm_sigmetrics',\n",
       " 'acm_sigmod',\n",
       " 'acm_sigplan']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploring terms\n",
    "\n",
    "print(len(terms))\n",
    "terms[18400:18500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE QUERY WORDS HERE - this query vector can be used for all three IR approaches! term list from \n",
    "# CountVectorizer and TFIDF are the same and in same order so no need to create new query for each approach\n",
    "\n",
    "# find all tokens with pandemic\n",
    "\n",
    "s0 = \"1918\"\n",
    "q0 = [token for token in terms if s0 in token]  # length 79\n",
    "\n",
    "#s1 = \"pandemia\"\n",
    "#q1 = [token for token in terms if s1 in token]   # length 1\n",
    "\n",
    "query_words = q0 #+ q1\n",
    "\n",
    "#query_words = [\"influenza\"]\n",
    "q = create_query(query_words, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anti_1918', 'pre_1918']\n"
     ]
    }
   ],
   "source": [
    "print(q0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the score for each document against the query. Docs with more occurences of the query words \n",
    "# will score higher\n",
    "\n",
    "f_scores = doc_term_matrix.dot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_nnz = sum(f_scores >0)\n",
    "print(max_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 4., 2., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "f_scores_sorted = np.sort(f_scores)[::-1]\n",
    "f_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143617 190307  91351 160556 181131 181132 181133 181134 181130 181135]\n"
     ]
    }
   ],
   "source": [
    "f_idx, f_top_abstracts = return_top_abstracts(docs, f_scores, 100) #max_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143617    [influenza, virus, significant, human, pathoge...\n",
       "190307    [influenza, virus, significant, human, pathoge...\n",
       "91351     [influenza, virus, significant, human, pathoge...\n",
       "160556    [current, vaccine, induce, generation, cell, s...\n",
       "181131    [analysis, gene, expression, data, cancer, cla...\n",
       "                                ...                        \n",
       "181219    [sox2, transcription, factor, know, glcnacylat...\n",
       "181220    [cell, acute_lymphoblastic_leukemia, lymphoma,...\n",
       "181221    [caspase, conserve, caspase, different, specie...\n",
       "181222    [glycosylation, important, widespread, type, p...\n",
       "181193    [protease, encode, human, genome, involved, di...\n",
       "Name: final_tokens, Length: 100, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_top_abstracts.iloc[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = create_result_df(docs, f_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "f_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find doc-term matrix using TF-IDF weighting\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same list as \"terms\" from CountVectorizer\n",
    "\n",
    "tf_idf_terms = tf_idf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1058314"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploring tf_idf_terms\n",
    "\n",
    "len(tf_idf_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if terms list from CountVectorizer and TfidfVectorizer are the same - they should be, THEY ARE\n",
    "\n",
    "terms == tf_idf_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# I don't need this as it created the exact same query as the create_query function\n",
    "\n",
    "'''\n",
    "# transform query to be in same space as documents\n",
    "# q = q.reshape(1,-1)\n",
    "qhat = tf_idf_vectorizer.transform(query_words)\n",
    "\n",
    "temp = qhat.toarray()\n",
    "qhat = np.reshape(temp, qhat.shape[1])\n",
    "\n",
    "tf_idf_scores = pairwise_distances(qhat, tf_idf, metric='cosine', n_jobs=19)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the score for each document against the query. Docs with more occurences of the query words \n",
    "# will score higher\n",
    "\n",
    "tf_idf_scores = tf_idf.dot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1884\n"
     ]
    }
   ],
   "source": [
    "max_nnz = sum(tf_idf_scores >0)\n",
    "print(max_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84638909, 0.83490343, 0.78848695, 0.7683569 , 0.69023329,\n",
       "       0.67756645, 0.58684808, 0.57074084, 0.5554081 , 0.55530964])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "tf_idf_scores_sorted = np.sort(tf_idf_scores)[::-1]\n",
    "tf_idf_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147574 292930 346289 236762  85470 148102  62499 436945 298556 194186]\n"
     ]
    }
   ],
   "source": [
    "tfidf_idx, tfidf_top_abstracts = return_top_abstracts(docs, tf_idf_scores, 100) #max_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147574    Influenza A viruses (IAV) are significant huma...\n",
       "292930    DESCRIPTION (provided by applicant): Influenza...\n",
       "346289    PROJECT SUMMARY / ABSTRACTInfluenza pandemics ...\n",
       "236762    Influenza A viruses are significant human path...\n",
       "85470     DESCRIPTION (provided by applicant): The pande...\n",
       "                                ...                        \n",
       "45334     DESCRIPTION (provided by applicant): Increased...\n",
       "399404    DESCRIPTION (provided by applicant): An enormo...\n",
       "127235    DESCRIPTION (provided by applicant): The CDC's...\n",
       "147092    This contract supports the preclinical, noncli...\n",
       "19026     DESCRIPTION (provided by applicant): The long-...\n",
       "Name: ABSTRACT, Length: 100, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_top_abstracts.iloc[1881])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) scores order may differ from Count Vectorizer....why? --- term frequency (tf) weight depends on term frequency\n",
    "# in doc AND number of terms in doc\n",
    "\n",
    "# 2) if returning all relevant documents, the rel docs list from CountVectorizer and TFIDF should be the same, \n",
    "# just in a diff order --- YES, TRUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check #2\n",
    "\n",
    "# check if terms list from CountVectorizer and TfidfVectorizer are the same - they should be, THEY ARE\n",
    "\n",
    "set(f_idx) == set(tfidf_idx)  # when we take all relevant docs.  order doesn't matter, only inclusion in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df = create_result_df(docs, tf_idf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing (LSI) Approach\n",
    "\n",
    "Uses the TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Truncated SVD of the TF-IDF matrix -- \n",
    "\n",
    "lsa = TruncatedSVD(n_components=500, random_state=1)\n",
    "USigma = lsa.fit_transform(tf_idf)\n",
    "Vtrans = lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform query to be in same space as documents\n",
    "\n",
    "#q = q.reshape(1,-1)\n",
    "qhat = lsa.transform(q.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500)\n",
      "(550074, 500)\n",
      "(500, 1058314)\n"
     ]
    }
   ],
   "source": [
    "print(qhat.shape)\n",
    "print(USigma.shape)\n",
    "print(Vtrans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_scores = pairwise_distances(qhat, USigma, metric='cosine', n_jobs=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsa_scores.shape)\n",
    "print(type(lsa_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00842145, 1.00341966, 0.99659346, ..., 1.01601745, 1.00529736,\n",
       "        1.00621292]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00842145, 1.00341966, 0.99659346, ..., 1.01601745, 1.00529736,\n",
       "       1.00621292])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550074\n"
     ]
    }
   ],
   "source": [
    "max_nnz = sum(lsa_scores[0] > 0)\n",
    "print(max_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lsa_scores[0], bins=100, range = (0.95, 1.05));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0773929631152116\n",
      "[0.99564568 0.99981852 1.00359385]\n",
      "1.0798677473228107\n"
     ]
    }
   ],
   "source": [
    "print(lsa_scores[0].min())\n",
    "print(np.percentile(lsa_scores[0], [25, 50, 75]))\n",
    "print(lsa_scores[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07986775, 1.07961181, 1.07718647, 1.07648677, 1.07633429,\n",
       "       1.07581714, 1.07521699, 1.07322336, 1.07106421, 1.06939388])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "lsa_scores_sorted = np.sort(lsa_scores[0])[::-1]\n",
    "lsa_scores_sorted[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120887 412678 516916 410792 483108 499717 119396 222388 444202 523151]\n"
     ]
    }
   ],
   "source": [
    "lsa_idx, lsa_top_abstracts = return_top_abstracts(docs, lsa_scores[0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120887    Dengue fever is one of the most important emer...\n",
       "412678    Abstract A reduction in disease severity is th...\n",
       "516916    We initially focused on a PIV3-based vector to...\n",
       "410792    ﻿   DESCRIPTION (provided by applicant): Respi...\n",
       "483108    ﻿DESCRIPTION (provided by applicant): Respirat...\n",
       "                                ...                        \n",
       "28495     DESCRIPTION (provided by applicant): The long ...\n",
       "120489    The four serotypes of Dengue virus (DENV) caus...\n",
       "137522    This subproject is one of many research subpro...\n",
       "26215     DESCRIPTION (provided by applicant):  Chronic ...\n",
       "537058    Project Summary/AbstractAn estimated 2.3 billi...\n",
       "Name: ABSTRACT, Length: 100, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsa_top_abstracts.iloc[499])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_df = create_result_df(docs, lsa_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with relevant pandemic abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions needed for coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format topics as a \"list of list of strings\".\n",
    "# Needed for topic coherence function in Gensim\n",
    "\n",
    "# function slightly modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def list_topics(model, vectorizer, top_n=10):\n",
    "\n",
    "    #input. top_n: how many words to list per topic.  If -1, then list all words.\n",
    "       \n",
    "    topic_words = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        #print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        if top_n == -1:   \n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1]])\n",
    "        else:\n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function slightly modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ix = np.concatenate([f_idx, tfidf_idx, lsa_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_idx = np.unique(docs_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for topic modeling - needs each document as a string\n",
    "\n",
    "lim_docs = [text[i] for i in docs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_vectorizer = TfidfVectorizer(max_df=0.6, min_df=3, lowercase=False) #max_features=5000)\n",
    "nmf_tf_idf = nmf_vectorizer.fit_transform(lim_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = nmf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3063"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling with NMF\n",
    "\n",
    "nmf_model = NMF(n_components=10, random_state=1)\n",
    "W = nmf_model.fit_transform(nmf_tf_idf)\n",
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129, 10)\n",
      "(10, 2177)\n"
     ]
    }
   ],
   "source": [
    "print(W.shape)\n",
    "print(H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "('influenza', 0.6925119372770508)\n",
      "('vaccine', 0.5953179993911152)\n",
      "('vaccines', 0.4786075960368146)\n",
      "('against', 0.42866673950617196)\n",
      "('viruses', 0.4248486365251964)\n",
      "('pandemic', 0.40280025824357285)\n",
      "('H1N1', 0.37260985004338953)\n",
      "('HA', 0.30063005019873884)\n",
      "('protection', 0.27031331347618687)\n",
      "('H5N1', 0.25732732550273457)\n",
      "\n",
      "Topic 1:\n",
      "('RSV', 1.5243507965778025)\n",
      "('asthma', 0.2423471795762382)\n",
      "('airway', 0.21940452537878224)\n",
      "('bronchiolitis', 0.21403705374980714)\n",
      "('disease', 0.1926697547805739)\n",
      "('mucus', 0.17491567476433534)\n",
      "('induced', 0.16300473466497625)\n",
      "('expression', 0.16275253834346795)\n",
      "('PGI2', 0.15727626234660244)\n",
      "('IL', 0.15043161964051824)\n",
      "\n",
      "Topic 2:\n",
      "('ZIKV', 1.5104856876928994)\n",
      "('DENV', 0.2071207540742691)\n",
      "('Zika', 0.17189276639934517)\n",
      "('microcephaly', 0.15247728665228505)\n",
      "('NS3', 0.12291441836474021)\n",
      "('flavivirus', 0.10378746163123377)\n",
      "('exposure', 0.0899527126665448)\n",
      "('protease', 0.08444849103403383)\n",
      "('pregnancy', 0.08355858461180261)\n",
      "('maternal', 0.0827371176475094)\n",
      "\n",
      "Topic 3:\n",
      "('1918', 0.7849050470271608)\n",
      "('influenza', 0.601602927501581)\n",
      "('viruses', 0.3991385388025649)\n",
      "('pandemic', 0.3922160267387803)\n",
      "('RNA', 0.30927778719472343)\n",
      "('human', 0.27445977381114134)\n",
      "('avian', 0.27443713612609943)\n",
      "('were', 0.267079847615066)\n",
      "('autopsy', 0.25098060733338556)\n",
      "('cases', 0.2509467917779419)\n",
      "\n",
      "Topic 4:\n",
      "('CD8', 0.8376238510488733)\n",
      "('cell', 0.5125684807947367)\n",
      "('RSV', 0.48021717636926364)\n",
      "('cells', 0.3149843610484187)\n",
      "('dendritic', 0.2867746703828567)\n",
      "('responses', 0.24130353598803958)\n",
      "('elicited', 0.1910060724189806)\n",
      "('dictate', 0.19082363319876525)\n",
      "('following', 0.18190876104808232)\n",
      "('neonatal', 0.1658829483297001)\n",
      "\n",
      "Topic 5:\n",
      "('DENV', 0.9340608187060623)\n",
      "('dengue', 0.6031982775447855)\n",
      "('immune', 0.2116455934030841)\n",
      "('fever', 0.1778412134694523)\n",
      "('ADE', 0.16601383198066463)\n",
      "('disease', 0.1591619881457723)\n",
      "('responses', 0.15289853061507505)\n",
      "('Dengue', 0.1491007961644511)\n",
      "('DHF', 0.1407780227357428)\n",
      "('specific', 0.129974046446323)\n",
      "\n",
      "Topic 6:\n",
      "('HCMV', 1.2944062253951878)\n",
      "('CMV', 0.3497123828704044)\n",
      "('latency', 0.30978499440734153)\n",
      "('viral', 0.2197159151522502)\n",
      "('reactivation', 0.19847054003173473)\n",
      "('miRNAs', 0.161301200237522)\n",
      "('MHC', 0.14569629439937887)\n",
      "('lytic', 0.12881483919338013)\n",
      "('assembly', 0.1161640337585484)\n",
      "('cellular', 0.10971059633780508)\n",
      "\n",
      "Topic 7:\n",
      "('influenza', 0.5014358606507037)\n",
      "('patients', 0.4148618058475134)\n",
      "('challenge', 0.31283036505600126)\n",
      "('study', 0.29789747438255276)\n",
      "('Center', 0.29575582615110035)\n",
      "('Clinical', 0.2876166241326763)\n",
      "('volunteer', 0.28359117491971225)\n",
      "('significant', 0.26904678315996655)\n",
      "('natural', 0.2511227614497836)\n",
      "('immunocompromised', 0.2290969833717368)\n",
      "\n",
      "Topic 8:\n",
      "('influenza', 0.763210297689973)\n",
      "('pandemic', 0.3776111271427795)\n",
      "('effectiveness', 0.19465603179677388)\n",
      "('transmission', 0.19075852483461594)\n",
      "('health', 0.1578627441744231)\n",
      "('research', 0.153083612235494)\n",
      "('tropical', 0.14861794173181253)\n",
      "('community', 0.1463740041579031)\n",
      "('population', 0.1389170575167668)\n",
      "('obese', 0.11664507535001639)\n",
      "\n",
      "Topic 9:\n",
      "('SP', 0.6864689014010235)\n",
      "('IAV', 0.4966283619021658)\n",
      "('LL', 0.22754748085881618)\n",
      "('37', 0.21053194796693234)\n",
      "('defensins', 0.19776975428603605)\n",
      "('binding', 0.19597166660342558)\n",
      "('HA', 0.17970223242733033)\n",
      "('viral', 0.17693902246646717)\n",
      "('strains', 0.15322620346010488)\n",
      "('activity', 0.1498684556563209)\n"
     ]
    }
   ],
   "source": [
    "print_topics(nmf_model, nmf_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list_topics(nmf_model, nmf_vectorizer, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coherence Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for coherence - needs each document as a list of tokens\n",
    "\n",
    "lim_docs_c = [docs[i] for i in docs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-1e1d719f9746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create Dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlim_docs_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_extremes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_below\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_above\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, keep_n=5000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "# Create corpus and dictionary for the limited corpus: dictionary (id2word), corpus\n",
    "    \n",
    "# Create Dictionary\n",
    "id2word = gensim.corpora.Dictionary(lim_docs_c)\n",
    "id2word.filter_extremes(no_below=3, no_above=0.9)#, keep_n=5000)\n",
    "\n",
    "# Create Corpus (Term Document Frequency)\n",
    "\n",
    "#Creates a count for each unique word appearing in the document, where the word_id is substituted for the word\n",
    "corpus = [id2word.doc2bow(doc) for doc in lim_docs_c]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=id2word, texts=lim_docs_c, \n",
    "                            coherence='c_v', processes=1) #window_size=500 ) \n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_dict = id2word.token2id\n",
    "terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at sci kit learn tfidf how tokens are formed...alpha numeric only..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.get(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sci-kit learn is keeping the most n frequent tokens by term frequency across corpus, \n",
    "# id2word is \"Keep only the first keep_n most frequent tokens.\" not exactly sure if this is in corpus?\n",
    "# if there is a tie break for same frequency it appears that scikit learn may be abc order?  and id2word is order\n",
    "# in corpus?\n",
    "\n",
    "# Fix: use more words for keep_n in id2word.filter_extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Dirichlet Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "hdp_model = HdpModel(corpus=corpus, id2word=id2word, random_state=1)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Time = \", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = hdp_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TT.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the topic words from the model\n",
    "topics = []\n",
    "for topic_id, topic in hdp_model.show_topics(num_topics=150, formatted=False):\n",
    "    topic = [word for word, _ in topic]\n",
    "    topics.append(topic)\n",
    "topics[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
