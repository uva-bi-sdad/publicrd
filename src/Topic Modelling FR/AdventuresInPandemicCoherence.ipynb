{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval - Pandemic Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from gensim.models.coherencemodel import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this script, I attempted to take the subset of documents produced by IR.ipynb (copied into here), and tune the number of topics and best fit model using coherence\n",
    "#However, for some reason, coherence couldn't be calculated--this script is still broken as a result and needs to be updated to fix coherence calculations.\n",
    "\n",
    "#Please also see the query terms, which are more expanded than \"pandemic\", and could improve this approach moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle \n",
    "os.chdir('/home/sc2pg/src/prnd/publicrd/data/prd/RND Topic Modelling') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs_per_method=600\n",
    "file_path=('lda_data_stanford_lemma.sav') #Entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function slightly modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CLEANED AND PROCESSED DATA PULL\n",
    "\n",
    "# import NSF data\n",
    "#f = open('/project/biocomplexity/sdad/projects_data/ncses/prd/RND Topic Modelling/agency_data.sav', 'rb')\n",
    "#f = open('nsf_stanford_lemma.sav', 'rb')\n",
    "\n",
    "# import entire dataset\n",
    "f = open(file_path, 'rb')\n",
    "\n",
    "[corpus, id2word, docs] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# corpus - word frequency in docs\n",
    "# id2word - dictionary\n",
    "# docs - lemmatized abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=docs.reset_index(drop=True)\n",
    "docs = docs.loc[docs.apply(lambda x: len(x)>0)] #No duplicates removed here\n",
    "corpus=[x for i,x in enumerate(corpus) if i in docs.index]\n",
    "docs=docs.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543414"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543414"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needed for doc-term matrix creation is one string per document (not a list of strings).  This is \n",
    "# already the format of df[\"ABSTRACT\"] so nothing to do here\n",
    "\n",
    "\n",
    "text = []\n",
    "i=0\n",
    "for doc in docs:\n",
    "    text.append(\" \".join(doc))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed for all approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query vector \n",
    "\n",
    "def create_query(words, terms):\n",
    "    \n",
    "    # words: search query words\n",
    "    # terms: terms in corpus\n",
    "    \n",
    "    q = np.zeros(len(terms))  # number of terms\n",
    "\n",
    "    idx = []\n",
    "    for word in query_words:\n",
    "        idx.append(terms.index(word))\n",
    "\n",
    "    q[idx] = 1\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_topics(model, vectorizer, top_n=10):\n",
    "\n",
    "    #input. top_n: how many words to list per topic.  If -1, then list all words.\n",
    "       \n",
    "    topic_words = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        #print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        if top_n == -1:   \n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1]])\n",
    "        else:\n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_abstracts(df, scores, top_n):\n",
    "    \n",
    "    '''\n",
    "    df: dataframe that contains ABSTRACT column\n",
    "    scores: scores of abstracts\n",
    "    top_n: return the top_n abstracts given by idx\n",
    "    '''\n",
    "    # sort scores in descending order\n",
    "    scores_sorted_idx = np.argsort(scores)[::-1]\n",
    "    \n",
    "    ix = scores_sorted_idx[:top_n]\n",
    "    print(ix[0:10])\n",
    "    \n",
    "    return ix, docs[ix]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_df(abstracts, scores):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"abstracts\"] = abstracts\n",
    "    df[\"scores\"] = scores\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact word matches - Frequency Count Document-Term Matrix\n",
    "\n",
    "This will return all abstracts in the corpus with exact word matches to the query.  A query is just a list of words to search for.\n",
    "\n",
    "Results will be return in sorted order of how high the query scores with each abstract. A high score means more occurences of the query words in the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrix based on count frequencies\n",
    "\n",
    "# create document-term matrix\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.4, min_df=3, lowercase=False, max_features=int(len(docs)/2))\n",
    "doc_term_matrix = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE QUERY WORDS HERE\n",
    "\n",
    "query_words=[x for x in terms if 'pandemic' in x or 'epidemic' in x]\n",
    "query_words.extend(['contagion','contagious','sars','h1n1', 'outbreak','zika'])#, 'epidemic','contagion','contagious']#'h1n1','epidemic','sars']\n",
    "q = create_query(query_words, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the score for each document against the query. Docs with more occurences of the query words \n",
    "# will score higher\n",
    "\n",
    "f_scores = doc_term_matrix.dot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12065"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(f_scores >0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38., 28., 27., 26., 25., 24., 23., 21., 21., 20.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "f_scores_sorted = np.sort(f_scores)[::-1]\n",
    "f_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233719 143617  91351 233180 289274 232152 190307 233418 300559 342174]\n"
     ]
    }
   ],
   "source": [
    "f_idx, f_top_abstracts = return_top_abstracts(docs, f_scores, top_docs_per_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = create_result_df(text, f_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find doc-term matrix using TF-IDF weighting\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_terms = tf_idf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE QUERY WORDS HERE\n",
    "\n",
    "#query_words = ['pandemic']\n",
    "\n",
    "q = create_query(query_words, tf_idf_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the score for each document against the query. Docs with more occurences of the query words \n",
    "# will score higher\n",
    "\n",
    "tf_idf_scores = tf_idf.dot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12065"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tf_idf_scores >0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.73887383, 1.56009364, 1.54074833, 1.45882028, 1.32375162,\n",
       "       1.15300596, 1.12886319, 1.12811907, 1.06449067, 1.05236444])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "tf_idf_scores_sorted = np.sort(tf_idf_scores)[::-1]\n",
    "tf_idf_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146156 289274 233719 146682 342174 431512  84846 233180 233418 294876]\n"
     ]
    }
   ],
   "source": [
    "tfidf_idx, tfidf_top_abstracts = return_top_abstracts(docs, tf_idf_scores, top_docs_per_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146156    [influenza, virus, iav, significant, human, pa...\n",
       "289274    [influenza_pandemics, foremost, international,...\n",
       "233719    [influenza, virus, significant, human, pathoge...\n",
       "146682    [influenza, virus, iav, significant, human, pa...\n",
       "342174    [influenza_pandemics, foremost, international,...\n",
       "                                ...                        \n",
       "344618    [dengue, world, important, arbovirus, cause, e...\n",
       "72983     [severe, acute, respiratory, syndrome_sars, re...\n",
       "215604    [rapid, accurate, identification, highly, infe...\n",
       "71238     [core, part, program, project, sars_coronaviru...\n",
       "399885    [recent, event, pandemic_influenza, h1n1_pdm, ...\n",
       "Name: final_tokens, Length: 600, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df = create_result_df(text, tf_idf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing (LSI) Approach\n",
    "\n",
    "Uses the TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Truncated SVD of the TF-IDF matrix\n",
    "\n",
    "lsa = TruncatedSVD(n_components=500, random_state=1)\n",
    "USigma = lsa.fit_transform(tf_idf)\n",
    "Vtrans = lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE QUERY WORDS HERE\n",
    "\n",
    "#query_words = ['pandemic']\n",
    "\n",
    "q = create_query(query_words, tf_idf_terms)\n",
    "\n",
    "# transform query to be in same space as documents\n",
    "q = q.reshape(1,-1)\n",
    "qhat = lsa.transform(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500)\n",
      "(543414, 500)\n",
      "(500, 1098073)\n"
     ]
    }
   ],
   "source": [
    "print(qhat.shape)\n",
    "print(USigma.shape)\n",
    "print(Vtrans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_scores = pairwise_distances(qhat, USigma, metric='cosine', n_jobs=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 543414)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03471822, 1.01045798, 0.99789794, ..., 1.01919306, 1.01570976,\n",
       "        1.00180207]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lsa_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543414"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lsa_scores[0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.03471822, 1.01045798, 0.99789794, ..., 1.01919306, 1.01570976,\n",
       "       1.00180207])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.12595817, 1.12595817, 1.12595817, 1.12281549, 1.12281065,\n",
       "       1.11019888, 1.11003927, 1.10904275, 1.10541619, 1.1043847 ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "lsa_scores_sorted = np.sort(lsa_scores[0])[::-1]\n",
    "lsa_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182549 220675  23881  74450 136650 182663 221223  18399  74273 136151]\n"
     ]
    }
   ],
   "source": [
    "lsa_idx, lsa_top_abstracts = return_top_abstracts(docs, lsa_scores[0], top_docs_per_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182549    [efficacy, antiretroviral, therapy, art, hiv, ...\n",
       "220675    [efficacy, antiretroviral, therapy, art, hiv, ...\n",
       "23881     [efficacy, antiretroviral, therapy, art, hiv, ...\n",
       "74450     [objective, efficacy, antiretroviral, therapy,...\n",
       "136650    [efficacy, antiretroviral, therapy, arv, hiv, ...\n",
       "                                ...                        \n",
       "272961    [recent, study, demonstrate, nuclear, factor, ...\n",
       "482804    [gut, brain, connect, gut, brain, axis, gba, n...\n",
       "485033    [long_term, goal, research, understand, molecu...\n",
       "138427    [background, blockade, regulatory, pathway, me...\n",
       "413130    [nef, protein, hiv, essential, infectivity, vi...\n",
       "Name: final_tokens, Length: 600, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['efficacy',\n",
       " 'antiretroviral',\n",
       " 'therapy',\n",
       " 'art',\n",
       " 'hiv',\n",
       " 'infect',\n",
       " 'individual',\n",
       " 'determine',\n",
       " 'restoration',\n",
       " 'peripheral',\n",
       " 'blood',\n",
       " 'cd4',\n",
       " 'cell',\n",
       " 'number',\n",
       " 'viral',\n",
       " 'suppression',\n",
       " 'peripheral',\n",
       " 'blood',\n",
       " 'represent',\n",
       " 'total',\n",
       " 'lymphocyte',\n",
       " 'body',\n",
       " 'contrast',\n",
       " 'gut',\n",
       " 'associate',\n",
       " 'lymphoid',\n",
       " 'tissue',\n",
       " 'galt',\n",
       " 'harbor',\n",
       " 'lymphocyte',\n",
       " 'body',\n",
       " 'previous',\n",
       " 'study',\n",
       " 'show',\n",
       " 'severe',\n",
       " 'cd4',\n",
       " 'cell',\n",
       " 'depletion',\n",
       " 'occur',\n",
       " 'galt',\n",
       " 'primary',\n",
       " 'hiv',\n",
       " 'infection',\n",
       " 'cd4',\n",
       " 'cell',\n",
       " 'restoration',\n",
       " 'galt',\n",
       " 'modest',\n",
       " 'slow',\n",
       " 'compare',\n",
       " 'peripheral',\n",
       " 'blood',\n",
       " 'art',\n",
       " 'change',\n",
       " 'galt',\n",
       " 'adequately',\n",
       " 'reflect',\n",
       " 'peripheral',\n",
       " 'blood',\n",
       " 'analysis',\n",
       " 'kinetic',\n",
       " 'mechanism',\n",
       " 'cd4',\n",
       " 'cell',\n",
       " 'restoration',\n",
       " 'function',\n",
       " 'galt',\n",
       " 'follow',\n",
       " 'art',\n",
       " 'fully',\n",
       " 'determine',\n",
       " 'simian_immunodeficiency_virus',\n",
       " 'siv',\n",
       " 'infect',\n",
       " 'rhesus_macaque',\n",
       " 'provide',\n",
       " 'excellent',\n",
       " 'animal',\n",
       " 'model',\n",
       " 'study',\n",
       " 'gut',\n",
       " 'mucosal',\n",
       " 'immune',\n",
       " 'system',\n",
       " 'comparison',\n",
       " 'peripheral',\n",
       " 'blood',\n",
       " 'compartment',\n",
       " 'objective',\n",
       " 'research',\n",
       " 'proposal',\n",
       " 'examine',\n",
       " 'suppression',\n",
       " 'viral',\n",
       " 'replication',\n",
       " 'kinetic',\n",
       " 'mechanism',\n",
       " 'restoration',\n",
       " 'gut',\n",
       " 'mucosal',\n",
       " 'immune',\n",
       " 'system',\n",
       " 'function',\n",
       " 'comparison',\n",
       " 'mucosal',\n",
       " 'peripheral',\n",
       " 'lymph_node',\n",
       " 'peripheral',\n",
       " 'blood',\n",
       " 'rhesus_macaque',\n",
       " 'start',\n",
       " 'art',\n",
       " 'combination',\n",
       " 'pmpa_ftc',\n",
       " 'primary',\n",
       " 'chronic',\n",
       " 'siv',\n",
       " 'infection',\n",
       " 'hypothesis',\n",
       " 'slow',\n",
       " 'restoration',\n",
       " 'cd4',\n",
       " 'cell',\n",
       " 'galt',\n",
       " 'therapy',\n",
       " 'attribute',\n",
       " 'disruption',\n",
       " 'functional',\n",
       " 'organization',\n",
       " 'gut',\n",
       " 'mucosal',\n",
       " 'tissue',\n",
       " 'occur',\n",
       " 'early',\n",
       " 'siv',\n",
       " 'infection',\n",
       " 'adequately',\n",
       " 'support',\n",
       " 'survival',\n",
       " 'maintenance',\n",
       " 'cd4',\n",
       " 'cell',\n",
       " 'home',\n",
       " 'gut_mucosa',\n",
       " 'longitudinal',\n",
       " 'evaluation',\n",
       " 'siv',\n",
       " 'model',\n",
       " 'lead',\n",
       " 'characterization',\n",
       " 'mechanism',\n",
       " 'relationship',\n",
       " 'cd4',\n",
       " 'cell',\n",
       " 'restoration',\n",
       " 'galt',\n",
       " 'peripheral',\n",
       " 'blood',\n",
       " 'lymph_node',\n",
       " 'compartment',\n",
       " 'peripheral',\n",
       " 'drain',\n",
       " 'mucosal',\n",
       " 'site',\n",
       " 'aim',\n",
       " 'rhesus_macaque',\n",
       " 'initiate',\n",
       " 'art',\n",
       " 'primary',\n",
       " 'chronic',\n",
       " 'siv',\n",
       " 'infection',\n",
       " 'determine',\n",
       " 'suppression',\n",
       " 'siv',\n",
       " 'replication',\n",
       " 'genomic',\n",
       " 'diversity',\n",
       " 'kinetic',\n",
       " 'cd4',\n",
       " 'cell',\n",
       " 'restoration',\n",
       " 'galt',\n",
       " 'comparison',\n",
       " 'peripheral',\n",
       " 'blood',\n",
       " 'lymph_node',\n",
       " 'determine',\n",
       " 'homing',\n",
       " 'survival',\n",
       " 'cd4',\n",
       " 'cell',\n",
       " 'galt',\n",
       " 'microenvironment',\n",
       " 'investigate',\n",
       " 'molecular',\n",
       " 'process',\n",
       " 'involve',\n",
       " 'restoration',\n",
       " 'gut',\n",
       " 'mucosal',\n",
       " 'immune',\n",
       " 'system',\n",
       " 'proposal',\n",
       " 'capitalize',\n",
       " 'experience',\n",
       " 'enteropathogenic',\n",
       " 'study',\n",
       " 'siv',\n",
       " 'model',\n",
       " 'expertise',\n",
       " 'multi_color_flow_cytometry',\n",
       " 'vivo',\n",
       " 'molecular',\n",
       " 'imaging',\n",
       " 'autologous',\n",
       " 'cell',\n",
       " 'transfer',\n",
       " 'gene',\n",
       " 'expression',\n",
       " 'methodology',\n",
       " 'propose',\n",
       " 'study',\n",
       " 'promise',\n",
       " 'provide',\n",
       " 'valuable',\n",
       " 'insight',\n",
       " 'impact',\n",
       " 'impaired',\n",
       " 'gut',\n",
       " 'microenvironment',\n",
       " 'viral',\n",
       " 'suppression',\n",
       " 'restoration',\n",
       " 'gut',\n",
       " 'mucosal',\n",
       " 'immune',\n",
       " 'system',\n",
       " 'compare',\n",
       " 'mucosal',\n",
       " 'peripheral',\n",
       " 'lymph_node',\n",
       " 'compartment',\n",
       " 'molecular',\n",
       " 'basis',\n",
       " 'pathophysiologic',\n",
       " 'process',\n",
       " 'galt']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_top_abstracts.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_df = create_result_df(docs, lsa_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstracts</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[project, explore, game, base, metaphor, enhan...</td>\n",
       "      <td>1.034718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[institution, science, museum, pi, steve, proj...</td>\n",
       "      <td>1.010458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[program, small, group, conversation, citizen,...</td>\n",
       "      <td>0.997898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[partnership, american, chemical, society, acs...</td>\n",
       "      <td>0.990249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[amphibian, population, world, experience, dec...</td>\n",
       "      <td>0.834095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543409</th>\n",
       "      <td>[establish, administration_children_families, ...</td>\n",
       "      <td>1.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543410</th>\n",
       "      <td>[mix, method, study, seek, deepen, understandi...</td>\n",
       "      <td>1.005692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543411</th>\n",
       "      <td>[purpose, project, examine, long_term, effect,...</td>\n",
       "      <td>1.019193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543412</th>\n",
       "      <td>[child, care, development, block, grant, ccdbg...</td>\n",
       "      <td>1.015710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543413</th>\n",
       "      <td>[goal, study, understand, use, positive, behav...</td>\n",
       "      <td>1.001802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543414 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstracts    scores\n",
       "0       [project, explore, game, base, metaphor, enhan...  1.034718\n",
       "1       [institution, science, museum, pi, steve, proj...  1.010458\n",
       "2       [program, small, group, conversation, citizen,...  0.997898\n",
       "3       [partnership, american, chemical, society, acs...  0.990249\n",
       "4       [amphibian, population, world, experience, dec...  0.834095\n",
       "...                                                   ...       ...\n",
       "543409  [establish, administration_children_families, ...  1.001070\n",
       "543410  [mix, method, study, seek, deepen, understandi...  1.005692\n",
       "543411  [purpose, project, examine, long_term, effect,...  1.019193\n",
       "543412  [child, care, development, block, grant, ccdbg...  1.015710\n",
       "543413  [goal, study, understand, use, positive, behav...  1.001802\n",
       "\n",
       "[543414 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with relevant pandemic abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1401,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_idx = np.concatenate([f_idx, tfidf_idx, lsa_idx])\n",
    "docs_idx = np.unique(docs_idx)\n",
    "docs_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_topic=docs[docs_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From here, who knows why it doesn't work\n",
    "id2wordx = gensim.corpora.Dictionary(docs_topic)\n",
    "\n",
    "keep_only_most_common=int(len(docs_topic)/2) #LDA works best with less features than documents\n",
    "#Filter words to only those found in at least a set number of documents (min_appearances)\n",
    "id2wordx.filter_extremes(no_below=3, no_above=0.4, keep_n=keep_only_most_common)\n",
    "corpusx = [id2word.doc2bow(doc) for doc in docs_topic]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_text=[]\n",
    "\n",
    "\n",
    "for doc in docs_topic:\n",
    "    mini_text.append(\" \".join(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_corpus=[doc for i,doc in enumerate(corpus) if i in docs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(801, 801)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_topic), len(mini_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_vectorizer = TfidfVectorizer(max_df=0.4, min_df=3, lowercase=True, max_features=int(len(docs_topic)/2))\n",
    "nmf_tf_idf = nmf_vectorizer.fit_transform(mini_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only tokens found in the dictionary? Maybe that will help\n",
    "sanitized_docs=[]\n",
    "for doc in docs_topic:\n",
    "    san_doc=[]\n",
    "    sanitized_docs.append(\" \".join([token for token in doc if token in list(id2wordx.token2id.keys())]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling with NMF\n",
    "\n",
    "#nmf_model = NMF(n_components=20, random_state=1)\n",
    "#W = nmf_model.fit_transform(nmf_tf_idf)\n",
    "#H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "\n",
    "def nmf_metrics(doc_term_matrix, n_topics, vectorizer, corpus, id2word, docs, rand_start):\n",
    "    coherence_values = []\n",
    "    \n",
    "    i = rand_start\n",
    "    for num_topics in n_topics:\n",
    "\n",
    "        # create model\n",
    "        nmf_model = NMF(n_components=num_topics, random_state = i)\n",
    "        nmf_model.fit_transform(doc_term_matrix)\n",
    "        \n",
    "        # create list of topics\n",
    "        topics = list_topics(nmf_model, vectorizer, top_n=10)\n",
    "        \n",
    "        # calculate coherence\n",
    "        cm = CoherenceModel(topics=topics, \n",
    "                            corpus=corpus,\n",
    "                            dictionary=id2word,\n",
    "                            texts=docs, \n",
    "                            coherence='c_v', \n",
    "                            processes=10) #window_size=500 ) \n",
    "        coherence_values.append(cm.get_coherence())\n",
    "        \n",
    "        # output completion message\n",
    "        i = i+1\n",
    "        print('Number of topics =', num_topics, \"complete.\")\n",
    "\n",
    "    return coherence_values\n",
    "\n",
    "# Function to format topics as a \"list of list of strings\".\n",
    "# Needed for topic coherence function in Gensim\n",
    "\n",
    "# function modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(801, 801, 801)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_text), len(mini_corpus), len(docs_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269109"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "15\n",
      "20\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'zika'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m_ensure_elements_are_ids\u001b[0;34m(self, topic)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'development'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c052466692e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs_topic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                 processes=10) #window_size=500 ) \n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcoherence_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, topics, texts, corpus, dictionary, window_size, keyed_vectors, coherence, topn, processes)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mtopics\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mnew_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                 \u001b[0mtopic_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_elements_are_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m                 \u001b[0mnew_topics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m_ensure_elements_are_ids\u001b[0;34m(self, topic)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'zika'"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "coherence_values=[]\n",
    "\n",
    "for num_topics in range(5,56,5):\n",
    "    doc_term_matrix_x=nmf_vectorizer.fit_transform(mini_text)\n",
    "    nmf_model_x = NMF(n_components=num_topics, random_state = 0)\n",
    "    nmf_model_x.fit_transform(doc_term_matrix_x)\n",
    "    # create list of topics\n",
    "    topics_x = list_topics(nmf_model_x, nmf_vectorizer, top_n=10)\n",
    "    print(num_topics)\n",
    "    cm = CoherenceModel(topics=topics_x, \n",
    "                                corpus=mini_corpus,\n",
    "                                dictionary=id2word,\n",
    "                                texts=docs_topic, \n",
    "                                coherence='c_v', \n",
    "                                processes=10) #window_size=500 ) \n",
    "    coherence_values.append(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Number of topics = 5 complete.\n",
      "Number of topics = 10 complete.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'zika'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m_ensure_elements_are_ids\u001b[0;34m(self, topic)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'development'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8cecec73701d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# run models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     c = nmf_metrics(doc_term_matrix=doc_term_matrix_x, n_topics=n_topics, vectorizer=nmf_vectorizer, \n\u001b[0;32m---> 15\u001b[0;31m                          corpus=mini_corpus, id2word=id2word, docs=docs_topic, rand_start = i*len(n_topics))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-ae66ced84fd6>\u001b[0m in \u001b[0;36mnmf_metrics\u001b[0;34m(doc_term_matrix, n_topics, vectorizer, corpus, id2word, docs, rand_start)\u001b[0m\n\u001b[1;32m     20\u001b[0m                             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                             \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                             processes=10) #window_size=500 ) \n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mcoherence_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, topics, texts, corpus, dictionary, window_size, keyed_vectors, coherence, topn, processes)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mtopics\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mnew_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                 \u001b[0mtopic_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_elements_are_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m                 \u001b[0mnew_topics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m_ensure_elements_are_ids\u001b[0;34m(self, topic)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# might be a list of token ids already, but let's verify all in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'zika'"
     ]
    }
   ],
   "source": [
    "# code copied from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "# minor alterations made\n",
    "\n",
    "n_topics = range(5,56,5)\n",
    "num_runs = 10\n",
    "\n",
    "col_names = [f\"iteration {i}\" for i in range(num_runs)]\n",
    "nmf_c = pd.DataFrame(index = n_topics, columns = col_names)\n",
    "for i in range(num_runs):\n",
    "    \n",
    "    print(f\"Iteration {i}\")\n",
    "    \n",
    "    # run models\n",
    "    c = nmf_metrics(doc_term_matrix=doc_term_matrix_x, n_topics=n_topics, vectorizer=nmf_vectorizer, \n",
    "                         corpus=mini_corpus, id2word=id2word, docs=docs_topic, rand_start = i*len(n_topics))\n",
    "    \n",
    "    # save results\n",
    "    nmf_c[f\"iteration {i}\"] = c\n",
    "    nmf_c.to_pickle(\"./pandemic_nmf_c_intermittent.pkl\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_topics(nmf_model, nmf_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.token2id['zika'] #That token is definitely in the dictionary!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
