{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for use with Topic Models\n",
    "\n",
    "The text has already been cleaned.  This script will preprocess it - tokenize, remove stop words, add bigrams and trigrams, lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import TextCleaning\n",
    "import LDAvariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/sc2pg/src/prnd/publicrd/data/prd/RND Topic Modelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved df.  df['working_abstract'] contains clean text.\n",
    "df = pd.read_pickle(\"./clean_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa='working_abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)\n",
    "df.rename(columns={'index':'original index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So this should go in the first line of the preprocessing script. These are two functions that remove junk words but should be run on the lowercase, tokenized text. Since we're not necessarily removing numbers, I included an extra parameter in remove_first_x_tokens called max_tokens_to_skip. That says search for this series in tokens within max_tokens_to_skip from the first token.\n",
    "##############\n",
    "#one could do this by running custom regular exprsesions on original abstract, to account for variations in capitalization\n",
    "#Or you can run it on the abstracts following the lowercase function but before any other type of preproccesing, including splitting tokens and removing too short BECAUSE\n",
    "#1. If you run this after removing too short words (e.g. of) you would never match items like  'national institutes of health'\n",
    "#2. If you tokenize, you have to either check for sequenes of tokens (which is very bad, given the number of unique sequqneces for names)\n",
    "#Or you remove any subtokens, which is bad for a phrase like \"National institutes of health\", since \"health\" is a token we want to retain IN GENERAL\n",
    "#But, this needs to be done on tokens, not entire string, because imagine replacing all instances of Dr. \"Brown\", which would turn 'brownian motion' or 'brown fat' into 'ian motion' or 'fat'\n",
    "#So the best way to run this is to apply your own lowercase, remove custom stop words, and then continue reguolar preprocessing with tokenizing, lemmatizing, etc.\n",
    "#############################################\n",
    "def remove_institution(record):\n",
    "    \"\"\"removes all instances of exact institution name from lowercase abstract string\"\"\"\n",
    "    org=record['ORGANIZATION_NAME']\n",
    "    if pd.notnull(org):\n",
    "        return record['working_abstract'].replace(org.lower(),'')\n",
    "    else:\n",
    "        return record['working_abstract']\n",
    "    \n",
    "def remove_custom_words(record):\n",
    "    \"\"\"Designates stopwords for a particular abstract that contain embedded info e.g. author names and removes them from a lowercase abstract\"\"\"\n",
    "    fields_to_replace=[]\n",
    "    #Main PI\n",
    "    #Adds all words in the pis names, excluding initials (hence why the commas and periods must be replaced)\n",
    "    if pd.notnull(record['CONTACT_PI_PROJECT_LEADER']):\n",
    "        fields_to_replace.extend([x.lower() for x in record['CONTACT_PI_PROJECT_LEADER'].replace(',','').replace('.','').replace('-',' ').split() if len(x)>1])\n",
    "    #Additional PIs\n",
    "    #For each pi, which are split by semicolons, and format is last,first;  #Sometimes a middle initial\n",
    "    if pd.notnull(record['OTHER_PIS']):\n",
    "        for i in record['OTHER_PIS'].split(';'):\n",
    "            i=i.strip() #Remove whitespace\n",
    "            i=i.replace('.','')#Periods for initials\n",
    "            i=i.replace(',','')#Commas between last, first\n",
    "            i=i.replace('-',' ')#Remove hyphen in hypenated names to make separate words once tokens.\n",
    "            fields_to_replace.extend([x.lower().strip() for x in i.split() if len(x)>1])\n",
    "    return [x for x in record['tokened_abstracts'] if not x in fields_to_replace]\n",
    "\n",
    "def remove_first_x_tokens(tokened_abstract,bad_start_phrases,max_tokens_to_skip=3):\n",
    "    assert [type(phrase)==list for phrase in bad_start_phrases] #Make sure not just a string\n",
    "    assert [type(tokened_abstract)==list]\n",
    "    \"\"\"removes each bad_start_phrase occuring within max_tokens_to_skip of the front--phrases must be lowered.\n",
    "    be careful calling this, as order matters! It always starts looking at the first token, which will change between runs.\n",
    "    both tokened_abstract and each phrase in bad_start_phrases must be a list, not just a string\n",
    "    eg the phrase 'overall project summary' and 'technical abstract' should be input as a list of lists: [ ['overall','project','summary'],['technical','abstract']] \"\"\"\n",
    "    for token_sequence in bad_start_phrases:\n",
    "        #Look for a match within up to 3 tokens from the start. The reasoning here is some abstract start with numbers indicating sections\n",
    "        #EG 8., 8.a, 8.1.1.--from EDA of first tokens\n",
    "        for idx in range(0,max_tokens_to_skip):\n",
    "            if tokened_abstract[idx:len(token_sequence)+idx]==token_sequence:\n",
    "                tokened_abstract=tokened_abstract[len(token_sequence)+idx:]\n",
    "                break\n",
    "    return tokened_abstract\n",
    "\n",
    "start_phrases_to_remove=[['section'],['abstract'],['contact','pd','pi'],['technical'],['nontechnical'],['non','technical'],\n",
    "                         ['project','summary','abstract'], ['overall','project','summary'],['project','abstract'],\n",
    "                        ['project','narrative'],['abstract'],['summary'],['description','provided','by','the','applicant'],\n",
    "                         ['description','provided','by','applicant'],['description','provided','by','candidate'],\n",
    "                         ['provided','by','investigator'], ['provided','by','the','investigator'],['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[wa]=df[wa].apply(str.lower)\n",
    "df[wa]=df.apply(remove_institution,axis=1) #case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tokenize abstracts 445.0988566875458 seconds\n"
     ]
    }
   ],
   "source": [
    "tokened_abstracts = TextCleaning.tokenize(df['working_abstract'])\n",
    "df['tokened_abstracts'] = tokened_abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pis=df.apply(remove_custom_words,axis=1).apply(remove_first_x_tokens,args=[start_phrases_to_remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stopwords():\n",
    "    \n",
    "    # stop words include the general English list and those specific to the corpus that do not aid in meaning\n",
    "    \n",
    "    stopWords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # format stop words the same way we formatted our corpus, ie. without apostrophes.  \n",
    "    stop_wds = stopWords.copy()\n",
    "    for word in stopWords:\n",
    "        if \"\\'\" in word:\n",
    "            stop_wds.discard(word)\n",
    "            stop_wds.add(word.replace(\"\\'\",\"\"))\n",
    "    \n",
    "    # more stop words that do not add meaning to topics\n",
    "    additional_stopwords=['another','well','addition', 'thus',\n",
    "                      'specifically', 'similar','including',\n",
    "                       'via','within', 'thus', 'particular', 'furthermore','include','also',\n",
    "                      'includes','however','whether','due', 'may','overall', 'whether','could',\n",
    "                      'many','finally', 'several', 'specific', 'additional', 'therefore', 'either', 'various',\n",
    "                       'within', 'among', 'would'] \n",
    "        \n",
    "    sw = stop_wds.union(additional_stopwords)\n",
    "    \n",
    "    return sw\n",
    "    \n",
    "\n",
    "stopWords = create_stopwords()\n",
    "tokened_docs_nostop = TextCleaning.remove_stopwords(no_pis, stopWords)\n",
    "\n",
    "df['tokened_docs_nostop'] = tokened_docs_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sc2pg/.conda/envs/env_full/lib/python3.5/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "bigram = gensim.models.Phrases(df['tokened_docs_nostop'], min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_docs=df['tokened_docs_nostop'].apply(lambda x: bigram[x])\n",
    "trigram = gensim.models.Phrases(bigram_docs, threshold=100)  \n",
    "tri_docs =bigram_docs.apply(lambda x: trigram[x])\n",
    "df['tns_bi_tri_docs'] = tri_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [institution, science, museum, steve, project,...\n",
       "2    [programs, small, group, conversations, citize...\n",
       "3    [partnership, american, chemical, society, acs...\n",
       "4    [amphibian, populations, around_world, experie...\n",
       "5    [center, molecular, interfacing, cmi, enable, ...\n",
       "6    [dru, integrated, optimization, evacuation_she...\n",
       "7    [foc, international, collaborative, project, w...\n",
       "8    [goal, project, reconstruct, low, frequency, b...\n",
       "9    [mapping, characterization, analysis, channel,...\n",
       "Name: tns_bi_tri_docs, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tns_bi_tri_docs'][1:10]#[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to lemmatize: 19.618050813674927 seconds\n"
     ]
    }
   ],
   "source": [
    "lemma_docs = TextCleaning.lemmatize(df['tns_bi_tri_docs'].iloc[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f6345788429b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemma_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextCleaning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tns_bi_tri_docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma_abstracts'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/qumulo/qhome/sc2pg/src/prnd/publicrd/src/Topic Modelling FR/TextCleaning.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_sentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mtokens_kept\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_lemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m#Removes tokens that aren't nouns, verbs, adverbs, adjectives or foreign words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/qumulo/qhome/sc2pg/src/prnd/publicrd/src/Topic Modelling FR/TextCleaning.py\u001b[0m in \u001b[0;36mapply_lemmatizer\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NNS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NNP'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NNPS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'POS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'RB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RBR'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RBS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#This doesn't actually work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env_full/lib/python3.5/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env_full/lib/python3.5/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1825\u001b[0m                     if form.endswith(old)]\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1827\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1828\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m             \u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lemma_docs = TextCleaning.lemmatize(df['tns_bi_tri_docs'])\n",
    "df['lemma_abstracts'] = lemma_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed text\n",
    "\n",
    "df.to_pickle(\"./processed_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the earth is unique among known planets in having liquid water on its surface, a crucial reason for the existence of life.  explaining how this came to be, and predicting conditions friendly to life elsewhere requires understanding the protoplanetary disk from which earth and the other planets formed, a complex, interacting system of rocks, dust, gas, plasma, and magnetic fields in orbit around the young sun.  many models by different scientific communities have addressed separate aspects of this problem, but none has integrated all the different physical, chemical, and mineralogical processes into a single, general, continuously improvable, three-dimensional, computational model.our imminent entry into the petascale computing era makes such a model practical for the first time.  this project will be the first serious attempt to develop a complete, multi-physics model based on a global simulation of a protoplanetary disk.  we will include three major physical processes.  first is the turbulence driven by magnetic fields coupled to the partially ionized gas, which determines how fast the disk accretes onto the star and how well dust can stick together to form the building blocks of planets.   second is the radiative cooling of the disk, which determines the temperature of the disk, and is determined by how dusty the disk is.  that, in turn, is controlled partly by the temperature.  third, the gas chemistry and the temperature, as well as the dust properties, determine the ionization of the gas, which in turn determines how turbulent the disk is. all of these processes will be included in a common computational framework.  this framework will rely on a novel numerical algorithm for computing magnetized gas flows using simulated particles moving with the gas. this algorithm combines advantages and evades limitations of both traditional grid-based simulation codes and of more widely used particle methods such as smoothed particle hydrodynamics. our team includes the inventor of this method.with a new, integrated model, we will be able to better understand earth's position and properties, the properties of other solar system objects, and observations of extrasolar planetary systems.the funding for this project will support the interdisciplinary training of a graduate student in the areas of meteoritics and astrophysics treated here.  a collaboration between the host institution and two european institutions will be supported, with graduate students traveling in both directions for collaboration and training. three undergraduate students will also participate in this research.the research undertaken here will inform the exhibitions and education work of the pi and co-pi ebel.  currently, their projects include a new hayden planetarium space show (estimated international viewership 7 million over 5 years, including 0.5 million nyc school children), a photo exhibit on the cassini-huygens mission to saturn, as well as extensive work with the professional development and community education departments of the museum, and mentoring summer reu students and interns.\n",
      "nan\n",
      "a cataclysmic variable is a compact binary (with a period less than twelve hours) in which the primary (a white dwarf) accretes matter and angular momentum from the secondary star (a main sequence star) filling its roche-lobe. in non-magnetic systems, the matter is transferred, at continuous or sporadic rates, by means of an accretion disk around the white dwarf. ongoing accretion at a low rate (quiescence) is interrupted every few weeks to months by intense accretion (outburst) of days to weeks, and every few thousand years by a thermonuclear explosion (a classical nova event). thus the white dwarfs in these systems are probes of cataclysmic evolution and accretion physics because they bear the thermal, chemical and rotational imprint of their long-term accretion and explosive thermonuclear history. here, professor sion, accompanied by collaborators and students, will examine white dwarfs in cataclysmic binaries and make measurements of their surface temperatures, rotation rates and chemistry of their accreted atmospheres, using multi-component synthetic spectral fitting. this will allow them to probe the age, evolutionary history, time-averaged accretion rate, white dwarf mass and core temperature, and the mass transfer driving mechanisms in these systems. in particular, measurements of photospheric chemical abundances will test accretion and diffusion theories, while overabundances of thermonuclear-processed elements could indicate pre-historical nova explosions or processed core material from an originally more massive secondary. the distribution of white dwarf rotation rates above and below the period gap, which will be explored here, is key to understanding the physics of angular momentum transfer during accretion. through this project they will also enlarge the sample of white dwarfs with known surface temperatures across all cataclysmic variable subtypes. they will carry out evolutionary accretion simulations with time-variable accretion and thus provide a new and independent means of determining white dwarf masses, constraining white dwarf rotation velocities, and determining the thermal impact that prolonged accretion heating has on the white dwarfs. an understanding of the consequences of accretion in cataclysmic variables is the first step in a global understanding of accretion-related phenomena throughout the universe (e.g. around neutron stars and black holes) which cannot be easily observed. also the ejected material into the interstellar medium from novae explosions, and hence the composition of the outer envelopes of white dwarfs in cataclysmic variables, tells us about which elements (metals) are expected to be recycled in the interstellar medium for the next generation of stars. in addition, some cataclysmic variable systems are possible progenitors for type ia supernovae and changes in our understanding of these accreting white dwarfs translates into change in the possible size of the population from which type ia supernovae arise. throughout this project, undergraduate students will play key roles in the analysis and synthetic spectral modeling of the data. they will also participate in the dissemination of the results via presentation of poster papers at professional meetings and co-authoring refereed publications.\n",
      "nan\n",
      "accretionary prisms form as sediments are collected (i.e., accreted) and complexly deformed at the down-going edge of a subducting oceanic plate. accretionary prisms provide evidence of convergence in the geologic record and are a significant mechanism for the growth of continental crust. some accretionary prisms in subduction zones experience long periods of erosion, rather than accretion, which has led the pis to propose investigating the triggers for accretion vs. erosion. the chugach terrane in alaska has long been considered as a type example of an accretionary prism. the pis will relate rock-type variations in the accretionary prism to major tectonic events in north america. structural geology and detrital zircon ages will be used to determine if these variations represent distinct accretionary episodes or a continuum of accretion.  they will study the flysch to constrain its accretionary history using regional geology as a proxy for sediment supply. this project will elucidate processes of subduction erosion that impact general models of geochemical cycling in the earths interior.  geologists have long realized that many of the features of the earth, as well as the causes of natural hazards, are related to the processes by which the large tectonic plates move into the interior of the earth during subduction.  subduction leads to volcanism, such as that seen in the cascades, and subduction caused the largest earthquakes ever recorded, such as in alaska in 1964. studying the sediment accreted above the subducting plate will help us understand this subduction process.  in addition, accretionary prisms have the potential to host mineral deposits and could provide a source of methane that would enhance global warming. this project will also help us to understand the history of alaska and provide for the education of undergraduate and graduate students at minority-serving institutions in the southwest u.s.\n",
      "nan\n",
      " are the birthplaces of both high-mass stars and star clusters. dr. james jackson (boston university) and his team will analyze a large suite of observations at centimeter, millimeter, sub-millimeter, and infrared wavelengths, supplemented by new observations at the very large telescope, the australian telescope national facility 22-m telescope, plateau de bure, and institut de radio astronomie millimétrique 30-m telescopes to explore the poorly understood processes at play in high-mass protostellar evolution.  the team will complete six key science projects in this funding period: (1) measuring the spectral energy distributions, and hence, bolometric luminosities, of ~100 high-mass protostars, (2) searching for high-mass  pre-stellar  cores, (3) testing the suggestion of  competitive accretion  by searching for circumstellar disks surrounding high mass stars, (4) testing the idea that young o stars actually enter the main-sequence as b stars and then continue to accrete material on their way to becoming main-sequence o stars, (5) measuring the clump mass function in high-mass star forming clouds, and (6) studying the chemical evolution of high-mass protostellar cores.  when this project is complete, the team will have made a significant contribution to the understanding of the poorly-known processes at play in the earliest stages of high-mass star formation and will have also tested two key theoretical ideas:  competitive accretion  and the idea that o stars first enter the main-sequence as accreting b stars.dr. jackson's group has led the way in identifying and characterizing  (irdcs).  their characteristic sizes (~few parsecs) and masses (~few 1,000 solar-masses) resemble those of cluster-forming molecular clumps such as those in orion and ophichus. their cold temperatures, however, suggest that they are in an earlier evolutionary state. imaging of the mm/sub-mm dust continuum reveals that irdcs contain compact cores, whose sizes (<0.5 parsecs) and masses (~100 solar masses) match those of  hot cores  associated with high-mass protostars. an examination of these cores reveals that about 1/3 contain unambiguous evidence of star formation. the large bolometric luminosities of the embedded protostars within several irdc cores show that they are currently forming high-mass stars. interferometric observations reveal that these high-mass stars are accompanied by a number of lower mass protostars. all of the initial evidence, therefore, suggests that irdcs host the earliest stages of the formation of high-mass stars and star clusters.  in addition to the research that will be carried out under this award, this proposal will fund the training and education of two female graduate students and one female post-doctoral student.  moreover, the boston university astronomy department has entered into a collaborative agreement with fisk university, a historically black college, and vanderbilt university to participate in the fisk-vanderbilt masters-ph.d. bridge program which allows fisk students to pursue their master's degrees at fisk with guaranteed admission into ph.d. programs at vanderbilt and boston university.\n",
      "nan\n",
      "this project is an observational program to determine the abundances of select white dwarfs as a method of measuring the bulk compositions of extrasolar minor planets. it is motivated by a recent analysis of keck i spectra of the white dwarf gd 362 whose photospheric abundances are naturally understood as resulting from the destruction of a minor planet similar in composition to the earth/moon system which is depleted in volatiles like carbon and sodium by more than a factor of 10. this suggests that white dwarfs with an infrared excess, such as gd 362, have accreted debris from asteroids that strayed within the star's tidal radius and that the atmospheres of these white dwarfs thus provide a powerful and currently unique opportunity to measure the bulk compositions of extrasolar minor planets.here, professor jura will use the keck i telescope and hires spectrograph to obtain spectra of additional white dwarfs with an infrared excess (presumably the result of a dust disk inside the tidal radius) and determine the abundances of externally contaminated elements (including volatile versus refractory elements). white dwarfs without infrared excesses will also be observed to evaluate the role/possibility of interstellar accretion. the resulting data may well enable a more comprehensive understanding of the existence and frequency of extrasolar earth-like planets. this project will directly support science education at ucla and the education/training of a graduate student whose thesis will be based on these data.\n",
      "nan\n",
      "dr. steven majewski (university of virginia) and his team will undertake a suite of five major experiments that will both expand the census of known milky way halo tidal streams and exploit halo streams for the understanding of galaxy evolution, structure formation and dark matter (dm) on galaxy-scales.  these experiments are (1) a study of the chemistry of halo tidal streams using echelle spectroscopy of stream stars at both optical and near-infrared wavelengths to verify the discovery of a metallicity gradient in the leading arm of the sagittarius stream, (2) a study of the 3-d motions of stream stars contained in a ground-based proper motion survey of 54 near-equatorial kapteyn selected area fields to derive stream orbits and constrain the shape and strength of the milky way potential, (3) undertake a pilot study with the green bank telescope of a sample of m-type giant stars in tidal streams to look for bright water masers that can be used for precision astrometry of tidal streams using the very large baseline array,  (4) a spectroscopic follow-up to the photometric grid giant star survey, which has already identified a number of previously known and newly discovered halo substructures across both hemispheres of the sky, and (5) more fully characterize the spatial and radial velocity distributions of g and k giant stars in what look to be  classical  stellar halos discovered around the large and small magellanic clouds by this research group. the stellar component of our galactic halo provides an important test-bed for theories of galaxy formation. the present  concordance , hierarchical dm cosmologies predict the presence of stellar streams as tidal remnants of accreted sub-halos. finding and characterizing these streams thus provides crucial information to constrain the nature of the cold dm-predicted merging history. these streams are also highly sensitive dynamical probes constraining the shape, strength and lumpiness of the halo of the milky way, and provide key insights into the chemical evolution of galaxies. a key aspect of this research is that it has been fueled primarily by student leadership and contributions at both the undergraduate and graduate level.  each experiment is being led or will be led by a different graduate student as part of their ph.d. dissertation.\n",
      "nan\n",
      "the plant disease, tanspot, caused by the pathogenic fungus, pyrenophora tritici-repentis, is a destructive and economically significant disease of wheat, which exhibits characteristics that make it ideally suited for investigating properties of pathogen virulence and plant disease susceptibility. for example, the fungus produces multiple protein toxins each of which is required for its ability to cause disease depending on the genetic makeup of the host. understanding how these toxins function should provide substantial insight into how pathogens cause disease and how plants become vulnerable to pathogens. this project will evaluate one of these toxins called ptr toxa and investigates how and where this protein interacts at the cellular level.  the experimental protocol is highly integrative and incorporates a diversity of approaches including biochemical, cytological, molecular, and genetical analyses designed to identify interacting plant partners. such partners include host proteins that mediate its uptake, facilitate intracellular transport, and comprise the toxin's site-of-action.  successful completion of the objectives promises to be transformative not only for its contribution to understanding the basic biology of pathogen virulence and mechanisms that govern host disease susceptibility, but also for understanding processes fundamentally important to plant biology including, receptor biology and function, intracellular protein transport and chloroplast structure and function.  the project will support training and outreach activities in several ways including: the implementation of a workshop in molecular plant-microbe interactions at heritage university, a nondenominational private, accreted institution of higher education sited on the yakama indian nation reservation in toppenish washington; conducting an eleven-week internship program at  hosting select heritage university students; and direct training experiences in an area with broad implications to the plant sciences for a postdoctoral associate or a research assistant and graduate and undergraduate students.\n",
      "nan\n",
      "are hard x-ray symbiotics progenitors of type ia supernovae? symbiotic stars are interacting binaries in which a white dwarf (wd) accretes from the wind of a red giant. their x-ray emission is typically very soft. recently, however, 4 symbiotics have been\n",
      "nan\n",
      "prior to the recent application of stable isotope based gc/ms methodology, little was known about in vivo essential fatty acid metabolism in animals or humans.  essential fatty acid metabolism was studies in human adults, both male and female, and those who smoked as well as non-smokers.  this was a stable isotope study of in vivo metabolism of deuterated-la and deuterated-lna conversion after a single oral dose of these precursors.  our results indicated that female smokers had a two-fold increase in the percent of plasma dose and a higher fractional conversion rate for 22:5n-3 conversion to 22:6n-3 compared with non-smokers.  male smokers had elevated total plasma n-3 fatty acids, a more rapid turn over of d5-18:3n-3, a disappaerance rate of d5-20:5n-3 that was both delayed and slower, and a greater percentage of d5-20:5n-3 was directed into 22:5n-3 relative to non-smokers.  generally, smoking increased the bioavailablity of n-3 fatty acids from plasma, accelerated fractional conversion rates, and increased the percent formation for some long chain n-3 fatty acids. in rats, it was observed that addition of preformed dha to the diet leads to a decreased accumulation of label from 18-c precursors into dha and dpan6 in several organs even though there was a significant increase in tissue dha.  female rats accumulated more dha and dpan6 but less aa than males when fed a controlled diet containing 3 wt% alpha-linolenic acid.  an n-3 fatty acid deficient diet led to a marked decline in labeling of liver 22:4n6 and 22:5n6 from the 18:2n6 precursor.  a closely related research project concerns the origins of nervous system and other organ dha. possible sources are from dietary preformed dha, from metabolism of the precursor, lna, or from body stores of dha. a novel technique has been developed that allows for the quantitative assessment of the amount of dha accreted from lna metabolism under various dietary conditions. for this study, it is necessary to control the diet from near birth up to a period where significant brain development has occurred. this has been accomplished thru the use of newly developed artifiicial rearing techniques using an artificial rat milk that was nearly devoid of n-3 fatty acids. the n-3 fatty acids are then added as deuterated-lna and containing varying levels of dha. in one major experiment, rat pups were fed diets with 0 or 2% dha between days 8-29 of life. during this period, it could be calculated that 40% of the newly formed brain dha in the animals fed d5-lna as their only source of n-3 fatty acids were derived from preformed dha and not from lna metabolism. this was surprising as there was no dha in the diet; thus, all preformed dha deposited in the brain must have been derived from other organs via the blood stream. when dha was added to the diet, there was a pronounced decrease in the rate of lna metabolism to dha, possibly due to a form of end-product inhibition, and 88% of brain dha was derived from the preformed dietary dha. the biochemical mechanisms underlying these metabolic effects of dietary dha are being investigated.  a decline in labeled dha was also observed in liver, heart, muscle, kidney and testes but no such changes were observed in adipose tissues.  there was also a higher level of brain dha in the rats given preformed dha indicating that metabolism could not provide an adequate source of brain dha.  another finding of consequence for infants fed formulas without dha was that several organs including the heart, lungs, kidney and spleen had a net loss of dha content during a period of intense body growth when no preformed dha was present in the diet.  an attempt was made to determine what the underlying mechanisms for dha transport into brain and other organs. lipoproteins were purified and labeled with radiotracers and modified with a tracer levels of phospholipids acylated with dha, aa or oleic acid (oa). the modified lipoproteins were intravenously injected in mice. the plasma and tissue distribution of the radiotracers were investigated as a function of time and the lipoproteins composition. we found that higher proportion of dha in ldl results in an enhanced uptake of these lipoproteins by brain and heart.  a similar enrichment of ldl in aa or oa did not result in any changes compared to control unaltered ldl. tissue uptake of hdl did not depend on its fatty acid composition.  we next compared the distribution in plasma pools and tissue uptake of 14c-dha and 3h-(oa) intravenously injected in mice. we found that dha is rapidly taken up by liver, selectively acylated into triglycerides and released back into the circulation in vldl. most of the dha from vldl and ldl appeared to be rapidly taken up by extrahepatic organs. this pattern seems to be unique for dha, because no significant amount of non-essential oleic acid, traced in a similar way, was found in tg and vldl fractions.  in summary, these results point to the important role of vldl and ldl in transport of dha to extrahepatic tissues, and to the involvement of liver in the initial selectivity for dha transport.a novel application of pet imaging for the study of c11-dha incorporation into brain has been initiated.  brain and heart images from 19 healthy volunteers and 17 alcoholics have now been obtained.  extensive characterization of the fatty acid input function in plasma has been made in real time for the 11-c-dha. our findings thusfar are that the j(in) and k* values for male and female healthy volunteers are similar except for the k* values in the thalamus and the gray matter/white matter ratio. there is a suggestion from initial studies that alcoholics may have a lower incorporation of dha in many areas of cortex than control subjects, but more subjects will be needed.\n",
      "nan\n",
      "proxy reconstructions of global temperatures over the past 1000 years provide a context for understanding recent warming trends and allow better estimates of the contribution of anthropogenic (greenhouse gas) forcing to global climate change. data compiled mostly from high latitude tree ring records suggest that the late 20th century was the warmest period of the last millennium. nevertheless, the uncertainties associated with these estimates are significant, especially during the period known as the medieval warming which ended ~700 years ago. temperature changes at the earth?s surface follow most closely those of the global tropics, which are 75% ocean; yet few datasets that span the last 1000 years exist from the low latitude oceans. in order to improve global temperature reconstructions and to reduce uncertainties, well-replicated records of sea surface temperature (sst) from the low-latitude oceans are urgently needed.  massive long-lived corals, which accrete calcium carbonate skeleton in annual bands like growth rings on a tree, have the potential to provide this information. the pis have recently applied a method of reconstructing annual sst that takes advantage of the demonstrated relationship between the skeletal growth of corals and water temperature. in multiple records generated from 3 coral species, coral growth captures between 50-60% of the sst variability in the instrumental record, on interannual and longer timescales. applying the method to a ~440-year long slow-growing coral collected from the bahamas suggests that ssts were within error of modern ~ ad1550, that little ice age ssts were about 1ºc cooler than today and that there is a strong anthropogenic signal in the ssts of the last 50 years. the pis will build on this initial work, focusing on refining the method of extracting sst from coral growth records and applying it to generate multi-century long proxy sst records for the low-latitude atlantic and pacific oceans. data acquisition by 3-d cat scanning of intact coral cores is relatively rapid and inexpensive, enabling generation of many records of varying lengths from multiple colonies at each site, and, using techniques applied in dendrochronology, provide enough data to provide realistic error estimates on reconstructed ssts. funding will support 3 pis, a post-doctoral research scientist, a phd student, and research experience for an undergraduate.\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sc2pg/.conda/envs/env_full/lib/python3.5/site-packages/pandas/core/indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "mini=df.loc[df['ABSTRACT'].apply(lambda x: 'accrete' in x),[wa,'lemma_abstracts']]\n",
    "for i in range(10):\n",
    "    print(mini.iloc[i][wa])\n",
    "    print(mini.iloc[i]['lemma_abstracts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_sets=[set(doc) for doc in lemma_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-50bc7b12b32b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbig_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmany_sets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbig_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbig_set\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "big_set=set([])\n",
    "for i in many_sets:\n",
    "    big_set=big_set |i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed text\n",
    "\n",
    "df = pd.read_pickle(\"./processed_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('FRAbstractsProcessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original index</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>FY</th>\n",
       "      <th>FIRST_CHAR</th>\n",
       "      <th>LAST_CHAR</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>AGENCY</th>\n",
       "      <th>IC_CENTER</th>\n",
       "      <th>PROJECT_NUMBER</th>\n",
       "      <th>...</th>\n",
       "      <th>ORGANIZATION_NAME</th>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <th>FY_TOTAL_COST</th>\n",
       "      <th>working_abstract</th>\n",
       "      <th>nchar</th>\n",
       "      <th>Start Char</th>\n",
       "      <th>tokened_abstracts</th>\n",
       "      <th>tokened_docs_nostop</th>\n",
       "      <th>tns_bi_tri_docs</th>\n",
       "      <th>lemma_abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>89996</td>\n",
       "      <td>This is a project to explore Game-based, Metap...</td>\n",
       "      <td>2008</td>\n",
       "      <td>This is a project to explore Game-based, Metap...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0814512</td>\n",
       "      <td>...</td>\n",
       "      <td>WHEELING JESUIT UNIVERSITY</td>\n",
       "      <td>47.076</td>\n",
       "      <td>1999467.0</td>\n",
       "      <td>This is a project to explore Game-based, Metap...</td>\n",
       "      <td>2057</td>\n",
       "      <td>T</td>\n",
       "      <td>[this, project, explore, game, based, metaphor...</td>\n",
       "      <td>[explore, game, based, metaphor, enhanced, gam...</td>\n",
       "      <td>[explore, game, based, metaphor, enhanced, gam...</td>\n",
       "      <td>[explore, game, base, metaphor, enhanced, game...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>89997</td>\n",
       "      <td>Institution: Franklin Institute Science Museum...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Institution: Franklin Institute Science Museum...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0741659</td>\n",
       "      <td>...</td>\n",
       "      <td>FRANKLIN INSTITUTE</td>\n",
       "      <td>47.076</td>\n",
       "      <td>1799699.0</td>\n",
       "      <td>Institution:  Science Museum PI: Snyder, Steve...</td>\n",
       "      <td>2053</td>\n",
       "      <td>I</td>\n",
       "      <td>[institution, science, museum, snyder, steve, ...</td>\n",
       "      <td>[science, museum, snyder, steve, drl, summary,...</td>\n",
       "      <td>[science, museum, snyder, steve, drl, summary,...</td>\n",
       "      <td>[science, museum, snyder, steve, drl, summary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>89998</td>\n",
       "      <td>Through programs (including small group conver...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Through programs (including small group conver...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0813522</td>\n",
       "      <td>...</td>\n",
       "      <td>SCIENCE MUSEUM OF MINNESOTA</td>\n",
       "      <td>47.076</td>\n",
       "      <td>1505858.0</td>\n",
       "      <td>Through programs (including small group conver...</td>\n",
       "      <td>1154</td>\n",
       "      <td>T</td>\n",
       "      <td>[through, programs, including, small, group, c...</td>\n",
       "      <td>[programs, small, group, conversations, citize...</td>\n",
       "      <td>[programs, small, group, conversations, citize...</td>\n",
       "      <td>[program, small, group, conversation, citizen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>89999</td>\n",
       "      <td>In partnership with the American Chemical Soci...</td>\n",
       "      <td>2008</td>\n",
       "      <td>In partnership with the American Chemical Soci...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0838627</td>\n",
       "      <td>...</td>\n",
       "      <td>INTERNATIONAL UNION OF PURE AND APPLIED CHEMISTRY</td>\n",
       "      <td>47.049</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>In partnership with the American Chemical Soci...</td>\n",
       "      <td>875</td>\n",
       "      <td>I</td>\n",
       "      <td>[partnership, with, the, american, chemical, s...</td>\n",
       "      <td>[partnership, american, chemical, society, acs...</td>\n",
       "      <td>[partnership, american, chemical, society, acs...</td>\n",
       "      <td>[partnership, american, chemical, society, ac,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>90000</td>\n",
       "      <td>Amphibian populations around the world are exp...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Amphibian populations around the world are exp...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0815315</td>\n",
       "      <td>...</td>\n",
       "      <td>CORNELL UNIVERSITY ITHACA</td>\n",
       "      <td>47.074</td>\n",
       "      <td>370996.0</td>\n",
       "      <td>Amphibian populations around the world are exp...</td>\n",
       "      <td>1322</td>\n",
       "      <td>A</td>\n",
       "      <td>[amphibian, populations, around, the, world, a...</td>\n",
       "      <td>[amphibian, populations, around, world, experi...</td>\n",
       "      <td>[amphibian, populations, around_world, experie...</td>\n",
       "      <td>[amphibian, population, around_world, experien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   original index  PROJECT_ID  \\\n",
       "0               0       89996   \n",
       "1               1       89997   \n",
       "2               2       89998   \n",
       "3               3       89999   \n",
       "4               4       90000   \n",
       "\n",
       "                                            ABSTRACT    FY  \\\n",
       "0  This is a project to explore Game-based, Metap...  2008   \n",
       "1  Institution: Franklin Institute Science Museum...  2008   \n",
       "2  Through programs (including small group conver...  2008   \n",
       "3  In partnership with the American Chemical Soci...  2008   \n",
       "4  Amphibian populations around the world are exp...  2008   \n",
       "\n",
       "                                          FIRST_CHAR LAST_CHAR DEPARTMENT  \\\n",
       "0  This is a project to explore Game-based, Metap...         .        NSF   \n",
       "1  Institution: Franklin Institute Science Museum...         .        NSF   \n",
       "2  Through programs (including small group conver...         .        NSF   \n",
       "3  In partnership with the American Chemical Soci...         .        NSF   \n",
       "4  Amphibian populations around the world are exp...         .        NSF   \n",
       "\n",
       "  AGENCY IC_CENTER PROJECT_NUMBER  ...  \\\n",
       "0    NSF       NaN        0814512  ...   \n",
       "1    NSF       NaN        0741659  ...   \n",
       "2    NSF       NaN        0813522  ...   \n",
       "3    NSF       NaN        0838627  ...   \n",
       "4    NSF       NaN        0815315  ...   \n",
       "\n",
       "                                   ORGANIZATION_NAME CFDA_CODE FY_TOTAL_COST  \\\n",
       "0                         WHEELING JESUIT UNIVERSITY    47.076     1999467.0   \n",
       "1                                 FRANKLIN INSTITUTE    47.076     1799699.0   \n",
       "2                        SCIENCE MUSEUM OF MINNESOTA    47.076     1505858.0   \n",
       "3  INTERNATIONAL UNION OF PURE AND APPLIED CHEMISTRY    47.049       51000.0   \n",
       "4                          CORNELL UNIVERSITY ITHACA    47.074      370996.0   \n",
       "\n",
       "                                    working_abstract nchar Start Char  \\\n",
       "0  This is a project to explore Game-based, Metap...  2057          T   \n",
       "1  Institution:  Science Museum PI: Snyder, Steve...  2053          I   \n",
       "2  Through programs (including small group conver...  1154          T   \n",
       "3  In partnership with the American Chemical Soci...   875          I   \n",
       "4  Amphibian populations around the world are exp...  1322          A   \n",
       "\n",
       "                                   tokened_abstracts  \\\n",
       "0  [this, project, explore, game, based, metaphor...   \n",
       "1  [institution, science, museum, snyder, steve, ...   \n",
       "2  [through, programs, including, small, group, c...   \n",
       "3  [partnership, with, the, american, chemical, s...   \n",
       "4  [amphibian, populations, around, the, world, a...   \n",
       "\n",
       "                                 tokened_docs_nostop  \\\n",
       "0  [explore, game, based, metaphor, enhanced, gam...   \n",
       "1  [science, museum, snyder, steve, drl, summary,...   \n",
       "2  [programs, small, group, conversations, citize...   \n",
       "3  [partnership, american, chemical, society, acs...   \n",
       "4  [amphibian, populations, around, world, experi...   \n",
       "\n",
       "                                     tns_bi_tri_docs  \\\n",
       "0  [explore, game, based, metaphor, enhanced, gam...   \n",
       "1  [science, museum, snyder, steve, drl, summary,...   \n",
       "2  [programs, small, group, conversations, citize...   \n",
       "3  [partnership, american, chemical, society, acs...   \n",
       "4  [amphibian, populations, around_world, experie...   \n",
       "\n",
       "                                     lemma_abstracts  \n",
       "0  [explore, game, base, metaphor, enhanced, game...  \n",
       "1  [science, museum, snyder, steve, drl, summary,...  \n",
       "2  [program, small, group, conversation, citizen,...  \n",
       "3  [partnership, american, chemical, society, ac,...  \n",
       "4  [amphibian, population, around_world, experien...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only what is needed for LDA - docs, corpus, and dictionary. When loading the entire dataframe, I have run \n",
    "# out of memory to run the model\n",
    "\n",
    "# from Sam's code:\n",
    "#    corpus = corpus, dictionary = id2word, texts = docs\n",
    "\n",
    "docs = df['lemma_abstracts']\n",
    "id2word, corpus = LDAvariables.createLDAvars(docs)\n",
    "pickle.dump([corpus, id2word, docs], open('lda_data.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-7fda2c96edac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/env_full/lib/python3.5/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env_full/lib/python3.5/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en', '__pycache__', '__init__.py']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/sc2pg/.local/lib/python3.7/site-packages/spacy/data/')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (2.2.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (1.18.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from spacy) (4.43.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: setuptools in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from spacy) (46.0.0.post20200309)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: setuptools in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0.post20200309)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.43.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /sfs/qumulo/qhome/sc2pg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /sfs/applications/202003/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/sc2pg/.local/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/home/sc2pg/.local/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy && python -m spacy download en\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-env_full]",
   "language": "python",
   "name": "conda-env-.conda-env_full-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
