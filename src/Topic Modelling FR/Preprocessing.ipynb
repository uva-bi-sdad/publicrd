{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for use with Topic Models\n",
    "\n",
    "The text has already been cleaned.  This script will preprocess it - tokenize, remove stop words, add bigrams and trigrams, lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import TextCleaning\n",
    "import LDAvariables\n",
    "import stanza\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to your data folder appropriately\n",
    "os.chdir('/home/sc2pg/src/prnd/publicrd/data/prd/RND Topic Modelling') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved df.  df['working_abstract'] contains clean text.\n",
    "df = pd.read_pickle(\"./clean_dataset.pkl\")\n",
    "df.reset_index(inplace = True)\n",
    "df.rename(columns={'index':'original index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-13 11:53:23 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-05-13 11:53:23 INFO: Use device: cpu\n",
      "2020-05-13 11:53:23 INFO: Loading: tokenize\n",
      "2020-05-13 11:53:23 INFO: Loading: pos\n",
      "2020-05-13 11:53:24 INFO: Loading: lemma\n",
      "2020-05-13 11:53:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "wa='working_abstract'\n",
    "\n",
    "#Create a pipeline for lemmatizing working abstract. This pipeline will tokenize, determine pos, and then lemmatize the token appropriately.\n",
    "nlp = stanza.Pipeline(lang='en',processors='tokenize,pos,lemma',tokenize_batch_size=500,lemma_batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "#Helper Functions\n",
    "#############################################\n",
    "\n",
    "#This function was used previously, but we were unaware it had already been included in 'Cleaning', and is redundant.        \n",
    "#def remove_institution(record):\n",
    "#    \"\"\"removes all instances of exact institution name from lowercase abstract string\"\"\"\n",
    "#    org=record['ORGANIZATION_NAME']\n",
    "#    if pd.notnull(org):\n",
    "#        return record['working_abstract'].replace(org.lower(),'')\n",
    "#    else:\n",
    "#        return record['working_abstract']\n",
    "    \n",
    "def remove_custom_words(record,col_to_clean):\n",
    "    \"\"\"Designates stopwords for a particular abstract that contain embedded info e.g. author names and removes them from a lowercase abstract\"\"\"\n",
    "    fields_to_replace=[]\n",
    "    if type(record[col_to_clean])!=list:\n",
    "        return np.nan\n",
    "    else:\n",
    "        #Main PI\n",
    "        #Adds all words in the pis names, excluding initials (hence why the commas and periods must be replaced)\n",
    "        if pd.notnull(record['CONTACT_PI_PROJECT_LEADER']):\n",
    "            fields_to_replace.extend([x.lower() for x in record['CONTACT_PI_PROJECT_LEADER'].replace(',','').replace('.','').replace('-',' ').split() if len(x)>1])\n",
    "        #Additional PIs\n",
    "        #For each pi, which are split by semicolons, and format is last,first;  #Sometimes a middle initial\n",
    "        if pd.notnull(record['OTHER_PIS']):\n",
    "            for i in record['OTHER_PIS'].split(';'):\n",
    "                i=i.strip() #Remove whitespace\n",
    "                i=i.replace('.','')#Periods for initials\n",
    "                i=i.replace(',','')#Commas between last, first\n",
    "                i=i.replace('-',' ')#Remove hyphen in hypenated names to make separate words once tokens.\n",
    "                fields_to_replace.extend([x.lower().strip() for x in i.split() if len(x)>1])\n",
    "        return [x.lower() for x in record[col_to_clean] if not x.lower() in fields_to_replace]\n",
    "\n",
    "def remove_first_x_tokens(tokened_abstract,bad_start_phrases,max_tokens_to_skip=3):\n",
    "    \"\"\"removes each bad_start_phrase occuring within max_tokens_to_skip of the front--phrases must be lowered.\n",
    "    be careful calling this, as order matters! It always starts looking at the first token, which will change between runs.\n",
    "    both tokened_abstract and each phrase in bad_start_phrases must be a list, not just a string\n",
    "    eg the phrase 'overall project summary' and 'technical abstract' should be input as a list of lists: [ ['overall','project','summary'],['technical','abstract']] \"\"\"\n",
    "    assert [type(phrase)==list for phrase in bad_start_phrases] #Make sure not just a string\n",
    "    assert [type(tokened_abstract)==list]\n",
    "    if type(tokened_abstract)!=list:\n",
    "        return np.nan\n",
    "    else:\n",
    "        for token_sequence in bad_start_phrases:\n",
    "            #Look for a match within up to 3 tokens from the start. The reasoning here is some abstract start with numbers indicating sections\n",
    "            #EG 8., 8.a, 8.1.1.--from EDA of first tokens\n",
    "            for idx in range(0,max_tokens_to_skip):\n",
    "                if tokened_abstract[idx:len(token_sequence)+idx]==token_sequence:\n",
    "                    tokened_abstract=tokened_abstract[len(token_sequence)+idx:]\n",
    "                    break\n",
    "        return tokened_abstract\n",
    "\n",
    "#Original list used to remove--must be updated now that lemmatization occurs before\n",
    "#start_phrases_to_remove=[['section'],['abstract'],['contact','pd','pi'],['technical'],['nontechnical'],['non','technical'],\n",
    "#                         ['project','summary','abstract'], ['overall','project','summary'],['project','abstract'],\n",
    "#                        ['project','narrative'],['abstract'],['summary'],['description','provided','by','the','applicant'],\n",
    "#                         ['description','provided','by','applicant'],['description','provided','by','candidate'],\n",
    "#                         ['provided','by','investigator'], ['provided','by','the','investigator'],['description']]\n",
    "\n",
    "start_phrases_to_remove=[['section'],['abstract'],['contact','pd','pi'],['nontechnical'],['non','technical'], ['non-technical'],['technical'],\n",
    "                         ['project','summary','abstract'], ['overall','project','summary'],['project','abstract'],\n",
    "                        ['project','narrative'],['abstract'],['summary'],['description','provide','applicant'],\n",
    "                         ['description','provide','applicant'],['description','provide','candidate'],\n",
    "                         ['provide','investigator'],['description']]\n",
    "\n",
    "\"\"\"\n",
    "#Prior function which used the spacy module--but spacy is not 'research-grade'\n",
    "def lemmatize_spacy(doc,punctuation_or_token='token'):\n",
    "\"\"\"\n",
    "\"\"\"use spacy to lemmatize a document. token takes a list of strings and is then turned into a string once again. Punctuation takes one string with punctuation and parses by sentence\"\"\"\n",
    "\"\"\"\n",
    "    assert punctuation_or_token in ['token','punctuation']\n",
    "    if punctuation_or_token=='token':\n",
    "        sentence=sp(' '.join(doc))\n",
    "    elif punctuation_or_token=='punctuation':\n",
    "        sentence=sp(doc)\n",
    "    new_tokens=[]\n",
    "    for word in sentence:\n",
    "        if word.pos_ in ['NOUN','VERB','ADJ','ADV']:\n",
    "            new_tokens.append(word.lemma_)\n",
    "        elif word.pos_ in ['PROPN','NUM','X','INTJ']:\n",
    "            new_tokens.append(word.text)\n",
    "    return new_tokens\n",
    "\"\"\"    \n",
    "def lemmatize_stanford(doc,pretokened=False,keep_numbers=True):\n",
    "    \"\"\"if pretokened, dont use this function, as it hasnt been adapted for it\"\"\"\n",
    "    ##to compare lemmatization functions, try test cases 463, 40, and 2247 (iloc)\n",
    "    assert not pretokened #If these are already tokened per another pipeline, this function won't work correctly\n",
    "    new_tokens=[]\n",
    "    if doc==' ': #Quirk that somehow two empty abstracts were not caught\n",
    "        return np.nan #Produces null abstracts that can mess up your code if you're not careful\n",
    "    else: \n",
    "        processed=nlp(doc)\n",
    "        for sent in processed.sentences:\n",
    "            for word in sent.words:\n",
    "                #If its a regular noun, verb, adj, or adverb, keep lemmatized form\n",
    "                if word.pos in ['NOUN','VERB','ADJ','ADV']:\n",
    "                    new_tokens.append(word.lemma)\n",
    "                #If you decided to retain numbers, their lemma is kept here. Note that number catching isnt perfect by this lemmatizing.\n",
    "                elif word.pos=='NUM' and keep_numbers:\n",
    "                    new_tokens.append(word.lemma)\n",
    "                #Exact phrases are kept here with no attempt at lemmatization: e.g. mars does not become mars, and hopefully scientific words e.g. chemicals will be tagged as propn, x, or intj if needed\n",
    "                elif word.pos in ['PROPN','X','INTJ']: \n",
    "                    new_tokens.append(word.text)\n",
    "                #Note that no other tokens are kept\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "def create_stopwords():\n",
    "    \"\"\" creates list of stopwords. stop words include the general English list and any additional we see sneaking through\n",
    "    #We no longer remove words specific to the corpus that do not aid in meaning like science/research\"\"\"\n",
    "    \n",
    "    stopWords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # format stop words the same way we formatted our corpus, ie. without apostrophes.  \n",
    "    stop_wds = stopWords.copy()\n",
    "    for word in stopWords:\n",
    "        if \"\\'\" in word:\n",
    "            stop_wds.discard(word)\n",
    "            stop_wds.add(word.replace(\"\\'\",\"\"))\n",
    "    \n",
    "    # more stop words that do not add meaning to topics\n",
    "    additional_stopwords=['another','well','addition', 'thus',\n",
    "                      'specifically', 'similar','including',\n",
    "                       'via','within', 'thus', 'particular', 'furthermore','include','also',\n",
    "                      'includes','however','whether','due', 'may','overall', 'whether','could',\n",
    "                      'many','finally', 'several', 'specific', 'additional', 'therefore', 'either', 'various',\n",
    "                       'within', 'among', 'would'] \n",
    "        \n",
    "    sw = stop_wds.union(additional_stopwords)\n",
    "    \n",
    "    return sw\n",
    "\n",
    "def remove_stopwords(doc, stop_words):\n",
    "    \"\"\"remove stopwords\"\"\"\n",
    "    #If no acceptable tokens, this is np.nan\n",
    "    if type(doc)!=list:\n",
    "        return np.nan\n",
    "    \n",
    "    return [word for word in doc if word not in stop_words] \n",
    "\n",
    "def apply_n_grams(abstract,function):\n",
    "    \"\"\"apply an n-gram--could be bi or tri gram--to abstract\"\"\"\n",
    "    #Non lists--aka no acceptable tokens\n",
    "    if type(abstract)==list:\n",
    "        return function[abstract]\n",
    "    else:\n",
    "        return abstract\n",
    "\n",
    "\n",
    "    import re\n",
    "\n",
    "def clean_up_tokens(doc):\n",
    "    \"\"\"determines for each doc which tokens to clean up formatting further in keep_token, and decides which of these cleaned up tokens will be kept\"\"\"\n",
    "    kept_tokens=[]\n",
    "    #Ignores documents that are abstracts with no valid tokens\n",
    "    if type(doc)!=list:\n",
    "        return np.nan\n",
    "    else:\n",
    "        for token in doc:\n",
    "            keep,altered_token=keep_token(token)\n",
    "            if keep:\n",
    "                kept_tokens.append(altered_token)\n",
    "        return kept_tokens\n",
    "        \n",
    "def keep_token(token):\n",
    "    \"\"\"strips hyphens, replaces internal hyphens with _, turns non-alphanumeric tokens into alphanumerics', strips leading _ if produced by alphamumeric,\n",
    "    then removes those updated tokens that: are numeric but not length 4, are tokens related to college names (see below) or are less than length 2.\"\"\"\n",
    "    token=token.strip('- ') #Removes leading and trailing hyphens\n",
    "    token=token.replace('-','_')\n",
    "    if not str.isalnum(token):\n",
    "        token=re.sub(r'\\W+', '', token)\n",
    "    token=token.strip('_')\n",
    "    #Names of universities\n",
    "    if 'university' in token or 'college' in token or 'universities' in token:\n",
    "        return (not token in college_tokens, token)\n",
    "    if str.isnumeric(token):\n",
    "        #keep years\n",
    "        return (len(token)==4,token)\n",
    "    else:\n",
    "        #Keep anything that is alphanumeric or alpha if its over length 2--allows mixed types e.g. h1n1\n",
    "        return (len(token)>=2,token)\n",
    "\n",
    "\n",
    "\n",
    "#Any specific university word is removed--schools within college/university, college names, etc. that cannot apply to multiple schools\n",
    "#This list was generated from all tokens that contained the string 'college','university',or 'universities'. The commented out parts of the list are terms\n",
    "#That could be considered \"generic\" ie apply to more than one school\n",
    "college_tokens=[#'college',\n",
    "'aga_khan_university',\n",
    "'ahmadu_bello_university',\n",
    "'alabama_aamp_university',\n",
    "'albert_einstein_college',\n",
    "'alcorn_state_university',\n",
    "'american_college_obstetricians',\n",
    "'american_college_surgeons',\n",
    "'americancollege',\n",
    "#'amongcollege',\n",
    "#'anduniversity',\n",
    "'anne_molloy_trinity_college',\n",
    "#'atuniversity',\n",
    "'auburn_university',\n",
    "'auburn_university_alabama',\n",
    "'auburn_university_au',\n",
    "'auburn_university_auburn',\n",
    "'auburn_university_montgomery',\n",
    "'auburn_university_tuskegee_university',\n",
    "'augustana_college',\n",
    "#'auniversity',\n",
    "'babes_bolyai_university',\n",
    "'barnard_college',\n",
    "'baruch_college',\n",
    "'bates_college',\n",
    "'baylor_college',\n",
    "'baylor_college_dentistry',\n",
    "'baylor_college_medicine',\n",
    "'baylor_college_ofmedicine',\n",
    "'baylor_collegeof',\n",
    "'baylorcollege_medicine',\n",
    "'baylorcollege_medicine_bcm',\n",
    "'ben_gurion_university',\n",
    "'benedict_college',\n",
    "'benedict_college_historically_black',\n",
    "'berea_college',\n",
    "'binghamton_university',\n",
    "#'black_colleges',\n",
    "'board_trinity_college',\n",
    "'bostonuniversity',\n",
    "'bowdoin_college',\n",
    "'brownuniversity',\n",
    "'bryn_mawr_college',\n",
    "'bucknell_university',\n",
    "#'cape_universities',\n",
    "'cardiff_university',\n",
    "'carleton_college',\n",
    "'carnegie_mellon_university',\n",
    "'carver_college',\n",
    "'carver_college_medicine',\n",
    "'case_western_reserveuniversity',\n",
    "'case_westernreserve_university',\n",
    "'catholic_university',\n",
    "'cerritos_college',\n",
    "'charles_drew_university',\n",
    "'chulalongkorn_university',\n",
    "'chulalongkorn_university_bangkok_thailand',\n",
    "'claflin_university',\n",
    "'claremont_colleges',\n",
    "'clark_atlanta_university',\n",
    "'colby_college',\n",
    "'colby_sawyer_college',\n",
    "#'college',\n",
    "#'college',\n",
    "#'college_american_pathologists',\n",
    "#'college_arts',\n",
    "#'college_arts_sciences',\n",
    "'college_brockport',\n",
    "'college_dentistry_nyucd',\n",
    "'college_dentistry_ufcd',\n",
    "#'college_goer',\n",
    "#'college_graduates',\n",
    "#'college_letters',\n",
    "#'college_letters_arts_sciences',\n",
    "#'college_letters_sciences',\n",
    "'college_lewiston',\n",
    "#'college_liberal_arts',\n",
    "'college_london',\n",
    "'college_medicine_aecom',\n",
    "'college_medicine_uccom',\n",
    "'college_menominee_nation',\n",
    "#'college_optometry',\n",
    "#'college_osteopathic_medicine',\n",
    "'college_park_umcp',\n",
    "'college_park_umd',\n",
    "#'college_physicians_surgeons',\n",
    "#'college_rheumatology_acr',\n",
    "'college_south_hadley',\n",
    "#'college_sports_medicine',\n",
    "'college_st_scholastica',\n",
    "'college_staten_island',\n",
    "#'college_students_basics',\n",
    "#'college_veterinary_medicine',\n",
    "#'college_veterinary_pathologists',\n",
    "#'college_veterinarymedicine',\n",
    "'college_wcmc',\n",
    "'college_wcmc_rockefeller_university',\n",
    "'college_william_mary',\n",
    "'college_wisconsin_mcw',\n",
    "'college_wooster',\n",
    "#'collegeand',\n",
    "#'collegeof',\n",
    "#'collegeof_medicine',\n",
    "#'colleges',\n",
    "#'colleges_arts_sciences',\n",
    "#'colleges_chicago',\n",
    "#'colleges_dentistry',\n",
    "#'colleges_dentistry_medicine',\n",
    "#'colleges_optometry',\n",
    "'colleges_rcc_umb', #Iffy--not sure what this is\n",
    "#'colleges_schools',\n",
    "#'colleges_universities',\n",
    "#'colleges_universities_hacu',\n",
    "#'collegesand',\n",
    "#'collegestudent',\n",
    "'columbia_university',\n",
    "'columbiauniversity',\n",
    "'comanche_nation_college',\n",
    "#'communitycollege',\n",
    "'creighton_university',\n",
    "'cross_university',\n",
    "'cross__university',\n",
    "'cuny_hunter_college',\n",
    "'del_mar_college',\n",
    "'depaul_university',\n",
    "'dine_college',\n",
    "'din_college',\n",
    "'diné_college',\n",
    "'diplomate_american_college',\n",
    "'doane_college',\n",
    "'doron_levy_university_maryland',\n",
    "'dukeuniversity',\n",
    "'eckerd_college',\n",
    "'emoryuniversity',\n",
    "'famu_fsu_college',\n",
    "'fort_lewis_college',\n",
    "'franklin_marshall_college',\n",
    "'fudan_university',\n",
    "'fudan_university_shanghai',\n",
    "'fudan_university_shanghai_china',\n",
    "'gallaudet_university',\n",
    "'george_mason_university',\n",
    "'george_washington_university',\n",
    "'georgetown_howard_universities',\n",
    "'georgetown_university',\n",
    "'georgia_regents_university',\n",
    "'gettysburg_college',\n",
    "#'grant_universities_aplu',\n",
    "'gu_howard_university',\n",
    "'hackensack_university',\n",
    "'hampton_university',\n",
    "'hanyang_university',\n",
    "'hartnell_college',\n",
    "'harvarduniversity',\n",
    "'harvey_mudd_college',\n",
    "#'historically_black_college',\n",
    "#'historically_black_colleges',\n",
    "#'historically_black_colleges_universities',\n",
    "'hokkaido_university',\n",
    "'hold_bates_college',\n",
    "'hold_colby_sawyer_college',\n",
    "'hold_stonehill_college_easton',\n",
    "'honors_college',\n",
    "'houston_baylor_college',\n",
    "'hunter_college',\n",
    "'imperial_college',\n",
    "'imperial_college_london',\n",
    "'imperial_college_london_uk',\n",
    "#'incollege',\n",
    "'indiana_university',\n",
    "'indianauniversity',\n",
    "#'inspect_certified_college',\n",
    "#'inter_college',\n",
    "#'inter_university',\n",
    "#'inter_university_consortium_political',\n",
    "#'interuniversity',\n",
    "#'interuniversity_consortium_political',\n",
    "#'intra_university',\n",
    "'james_cook_university',\n",
    "'james_madison_university',\n",
    "'jeffersonuniversity',\n",
    "'john_jay_college',\n",
    "'johns_hopkinsuniversity',\n",
    "'kennesaw_state_university',\n",
    "'king_college_london',\n",
    "'kwame_nkrumah_university',\n",
    "'kyoto_university',\n",
    "'kyushu_university',\n",
    "'langston_university',\n",
    "'lehman_college',\n",
    "'lehman_college_city',\n",
    "'lehman_college_cuny',\n",
    "'lemoyne_owen_college',\n",
    "'lewis_clark_college',\n",
    "#'liberal_art_college',\n",
    "'louisiana_universities_marine',\n",
    "'loyola_marymount_university',\n",
    "'loyola_university',\n",
    "'loyola_university_chicago',\n",
    "'macalester_college',\n",
    "'makerere_university',\n",
    "'makerere_university_kampala_uganda',\n",
    "'makerere_university_uganda',\n",
    "'makerereuniversity',\n",
    "'marquette_university',\n",
    "'marquette_university_milwaukee',\n",
    "'mbarara_university',\n",
    "'mcgill_university',\n",
    "'mcmaster_university',\n",
    "'medgar_evers_college',\n",
    "'medical_colleges_aamc',\n",
    "'medicalcollege',\n",
    "'medicaluniversity_south_carolina',\n",
    "'medicine_yeshiva_university',\n",
    "'meharrymedical_college',\n",
    "'mellon_university',\n",
    "'mellonuniversity',\n",
    "'mexico_highlands_university',\n",
    "'miami_dade_college',\n",
    "'middlebury_college',\n",
    "'millsaps_college',\n",
    "'monash_university',\n",
    "'monash_university_australia',\n",
    "#'montana_tribal_college',\n",
    "#'montana_tribal_colleges',\n",
    "'montclair_state_university',\n",
    "'morehouse_college',\n",
    "'morehouse_college_spelman_college',\n",
    "'mount_holyoke_college',\n",
    "'msm_tuskegee_university',\n",
    "'mt_marty_college',\n",
    "'muhimbili_university',\n",
    "#'multi_university',\n",
    "#'muniversity',\n",
    "'nakoda_college',\n",
    "'nanyang_technological_university',\n",
    "'nazarene_university',\n",
    "'nazareth_college',\n",
    "#'non_college',\n",
    "#'non_university',\n",
    "'northern_arizona_university',\n",
    "'northern_kentucky_university',\n",
    "'northshore_university',\n",
    "'northshore_university_healthsystem',\n",
    "'northwest_nazarene_university',\n",
    "'northwestern_university',\n",
    "'norwich_university',\n",
    "#'ofuniversity',\n",
    "'oglala_lakota_college',\n",
    "'ohio_stateuniversity',\n",
    "'old_dominion_university',\n",
    "'olin_college',\n",
    "#'otheruniversity',\n",
    "#'participatinguniversity',\n",
    "'pasadena_city_college',\n",
    "'peking_university',\n",
    "'peking_university_beijing_china',\n",
    "'pennsylvania_college_optometry',\n",
    "#'phduniversity',\n",
    "#'polytechnic_university',\n",
    "#'post__college',\n",
    "'prairie_view_university',\n",
    "#'pre_college',\n",
    "#'pre_university',\n",
    "#'pre__college',\n",
    "#'precollege',\n",
    "'queens_college',\n",
    "'regents_university',\n",
    "'researchuniversity',\n",
    "'rockefeller_university',\n",
    "'rockefeller_university_memorial_sloan',\n",
    "'rockefeller_university_ru',\n",
    "'rockefeller_university_weill_cornell',\n",
    "'rockefelleruniversity',\n",
    "'royal_college_surgeons',\n",
    "'rutgers_university',\n",
    "'rutgersuniversity',\n",
    "'saddleback_college',\n",
    "'saginaw_chippewa_tribal_college',\n",
    "'saint_michael_college',\n",
    "'salish_kootenai_college',\n",
    "'salve_regina_university',\n",
    "'sawyer_college',\n",
    "#'scienceuniversity', #This is likely ohsu, as bellow, but for parsimony, this is kept\n",
    "'scienceuniversity_ohsu',\n",
    "'serc_carleton_college',\n",
    "'shams_university',\n",
    "'shams_university_cairo_egypt',\n",
    "'shanghai_jiaotong_university',\n",
    "'simon_fraser_university',\n",
    "'sinte_gleska_university',\n",
    "'sisseton_wahpeton_college',\n",
    "'sitting_bull_college',\n",
    "'skc_tribal_college',\n",
    "'sokoine_university',\n",
    "'south_africa_university_witwatersrand',\n",
    "'southern_illinois_university_carbondale',\n",
    "'southern_illinois_university_edwardsville',\n",
    "'southern_methodist_university',\n",
    "'spelman_college',\n",
    "'st_edward_university',\n",
    "'st_mary_college',\n",
    "'st_olaf_college',\n",
    "'st_philip_college',\n",
    "'stanforduniversity',\n",
    "'state_university_dominguez', #Specific university\n",
    "#'stateuniversity', #This could be any state\n",
    "'stellenbosch_university',\n",
    "'stellenbosch_university_south_africa',\n",
    "'stonehill_college',\n",
    "'stonehill_college_easton_massachusetts',\n",
    "'stony_brook_university',\n",
    "'swarthmore_college',\n",
    "'tarrant_county_college',\n",
    "'tel_aviv_university',\n",
    "'templeuniversity',\n",
    "'texas_a_university',\n",
    "'texas_southmost_college',\n",
    "'texas_university_kingsville',\n",
    "#'thecollege',\n",
    "#'theuniversity',\n",
    "'theuniversity_california_san',\n",
    "'theuniversity_colorado',\n",
    "'theuniversity_maryland',\n",
    "'theuniversity_michigan',\n",
    "'theuniversity_minnesota',\n",
    "'theuniversity_north_carolina',\n",
    "'theuniversity_pennsylvania',\n",
    "'theuniversity_pittsburgh',\n",
    "'tougaloo_college',\n",
    "#'touniversity',\n",
    "#'triangle_universities_nuclear', #this is a government research center\n",
    "#'tribal_college',\n",
    "'tribal_college_haskell_indian', #specific university\n",
    "#'tribal_colleges',\n",
    "#'tribal_colleges_universities',\n",
    "#'tribal_colleges_universities_tcus',\n",
    "'trinity_college',\n",
    "'trinity_college_arts_sciences',\n",
    "'trinity_college_dublin',\n",
    "'tsinghua_university',\n",
    "'tsinghua_university_beijing',\n",
    "'tsinghua_university_beijing_china',\n",
    "'tsinghua_university_china',\n",
    "#'tsinghua_university_prof_roberto',\n",
    "'tulaneuniversity',\n",
    "'tuskegee_universities',\n",
    "'tuskegee_university',\n",
    "'tuskegee_university_hbcu',\n",
    "'uams_colleges',\n",
    "'ucsf_makerere_university',\n",
    "'umbc_university_maryland',\n",
    "'uniformed_services_university',\n",
    "'united_negro_college',\n",
    "#'universities',\n",
    "#'universities_aau', #this is an association of universities, not a university\n",
    "#'universities_hbcu',\n",
    "'universities_kansas_ku',\n",
    "#'universitiesand',\n",
    "#'universitiesin',\n",
    "#'university',\n",
    "#'university',\n",
    "'university_alabama_birmingham',\n",
    "'university_alabama_huntsville',\n",
    "'university_alabama_tuscaloosa',\n",
    "'university_alabama_ua',\n",
    "'university_alaska_anchorage',\n",
    "'university_alaska_fairbanks',\n",
    "'university_albany_suny',\n",
    "'university_arizona_ua',\n",
    "'university_arkansas_fayetteville',\n",
    "'university_arkansas_pine',\n",
    "'university_arkansas_ua',\n",
    "'university_buffalo_suny',\n",
    "'university_buffalo_ub',\n",
    "'university_california_berkeley',\n",
    "'university_california_davis',\n",
    "'university_california_irvine',\n",
    "'university_california_los',\n",
    "'university_california_merced',\n",
    "'university_california_riverside',\n",
    "'university_california_san',\n",
    "'university_california_sanfrancisco',\n",
    "'university_california_santa',\n",
    "'university_cincinnati_cincinnati',\n",
    "'university_college_dublin',\n",
    "'university_college_london',\n",
    "'university_colorado_anschutz',\n",
    "'university_colorado_boulder',\n",
    "'university_colorado_denver',\n",
    "'university_connecticut_uconn',\n",
    "'university_feinberg_school',\n",
    "'university_florida_gainesville',\n",
    "'university_florida_uf',\n",
    "'university_fullerton_csuf',\n",
    "'university_georgia_athens',\n",
    "'university_georgia_uga',\n",
    "'university_hawaii_hilo',\n",
    "'university_hawaii_manoa',\n",
    "'university_hawaii_uh',\n",
    "'university_hospitals_cleveland',\n",
    "'university_houston_downtown',\n",
    "'university_houston_uh',\n",
    "'university_illinois_chicago',\n",
    "'university_illinois_urbana',\n",
    "'university_indianapolis_iupui',\n",
    "'university_kansas_ku',\n",
    "'university_kansas_lawrence',\n",
    "'university_kingsville',\n",
    "'university_langone_medical',\n",
    "'university_louisiana_lafayette',\n",
    "'university_louisiana_monroe',\n",
    "'university_maryland',\n",
    "'university_maryland_baltimore',\n",
    "'university_maryland_baltimore_county',\n",
    "'university_maryland_baltimore_umb',\n",
    "'university_maryland_eastern_shore',\n",
    "'university_maryland_greenebaum',\n",
    "'university_maryland_marlene_stewart', \n",
    "'university_maryland_umd',\n",
    "'university_massachusetts_amherst',\n",
    "'university_massachusetts_dartmouth',\n",
    "'university_massachusetts_lowell',\n",
    "'university_massachusetts_umass',\n",
    "'university_miami_miller',\n",
    "'university_miami_um',\n",
    "'university_michigan_ann',\n",
    "'university_michigan_dearborn',\n",
    "'university_michigan_um',\n",
    "'university_minnesota',\n",
    "'university_minnesota_duluth',\n",
    "'university_minnesota_masonic',\n",
    "'university_minnesota_minneapolis',\n",
    "'university_minnesota_twin',\n",
    "'university_minnesota_umn',\n",
    "'university_missouri__columbia',\n",
    "'university_missouri_columbia',\n",
    "'university_missouri_kansas',\n",
    "'university_missouri_mu',\n",
    "'university_missouri_rolla',\n",
    "'university_missouri_st',\n",
    "'university_nebraska_lincoln',\n",
    "'university_nebraska_omaha',\n",
    "'university_nevada_las',\n",
    "'university_nevada_reno',\n",
    "'university_northcarolina_chapel',\n",
    "'university_northridge_csun',\n",
    "'university_ofalabama',\n",
    "'university_ofcalifornia',\n",
    "'university_ofcolorado',\n",
    "'university_ofmichigan',\n",
    "'university_ofminnesota',\n",
    "'university_ofpennsylvania',\n",
    "'university_ofrochester',\n",
    "'university_oftexas',\n",
    "'university_ofwashington',\n",
    "'university_ofwashington_uw',\n",
    "'university_ofwisconsin',\n",
    "'university_ofwisconsin_madison',\n",
    "'university_oklahoma_norman',\n",
    "'university_oklahoma_ou',\n",
    "'university_pennsylvania_upenn',\n",
    "'university_pittsburgh_pitt',\n",
    "'university_singapore_nus',\n",
    "'university_singapore_singapore',\n",
    "'university_tennessee_chattanooga',\n",
    "'university_tennessee_knoxville',\n",
    "'university_tennessee_memphis',\n",
    "'university_texas_arlington',\n",
    "'university_texas_austin',\n",
    "'university_texas_brownsville',\n",
    "'university_texas_dallas',\n",
    "'university_texas_el',\n",
    "'university_texas_pan',\n",
    "'university_texas_rio',\n",
    "'university_texas_southwestern',\n",
    "'university_texas_tyler',\n",
    "'university_toronto_toronto',\n",
    "'university_venda',\n",
    "'university_vermont_burlington',\n",
    "'university_vermont_uvm',\n",
    "'university_virginia_charlottesville',\n",
    "'university_virginia_uva',\n",
    "'university_washington_seattle',\n",
    "'university_washington_uw',\n",
    "'university_waterloo',\n",
    "'university_west_indies',\n",
    "'university_wisconsin_carbone',\n",
    "'university_wisconsin_eau',\n",
    "'university_wisconsin_madison',\n",
    "'university_wisconsin_milwaukee',\n",
    "'university_wisconsin_oshkosh',\n",
    "'university_wisconsin_platteville',\n",
    "'university_wisconsin_stout',\n",
    "'university_witwatersrand',\n",
    "'university_witwatersrand_south_africa',\n",
    "'university_witwatersrand_wits',\n",
    "#'universityabstract',\n",
    "#'universityand',\n",
    "#'universitycareer',\n",
    "#'universityco',\n",
    "#'universityhospitals',\n",
    "#'universityin',\n",
    "#'universityintellectual',\n",
    "#'universitymedical',\n",
    "#'universityof',\n",
    "'universityof_california_san',\n",
    "'universityof_chicago',\n",
    "'universityof_colorado',\n",
    "'universityof_kentucky',\n",
    "'universityof_michigan',\n",
    "'universityof_minnesota',\n",
    "'universityof_pennsylvania',\n",
    "'universityof_pittsburgh',\n",
    "'universityof_washington',\n",
    "#'universityproposal',\n",
    "#'universityresources',\n",
    "#'universitys',\n",
    "#'universityschool_medicine',\n",
    "#'universitytitle',\n",
    "'urmc_college_arts',\n",
    "'vanderbiltuniversity',\n",
    "'virginia_commonwealth_university',\n",
    "'wake_forest_university',\n",
    "'washingtonuniversity',\n",
    "'wayne_stateuniversity',\n",
    "'weinberg_college_arts',\n",
    "'wellesley_college',\n",
    "'wesley_college',\n",
    "'western_ontario_mcmaster_universities',\n",
    "'western_ontario_mcmasters_universities',\n",
    "'westminster_college',\n",
    "#'withuniversity',\n",
    "'xiamen_university',\n",
    "'xiamen_university_china',\n",
    "'yaleuniversity',\n",
    "'yeshiva_university',\n",
    "'yonsei_university',\n",
    "'yonsei_university_seoul_south',\n",
    "'yorkuniversity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: There is an important change in order from past runs of the code here. Now, lemmatization and tokenization occurs within the same step\n",
    "#Removal of pi names, 'description provided by the applicant' and the creation of n-grams occurs AFTER lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#Lemmatize\n",
    "####################\n",
    "#In an ideal world, this would be run once. However, it takes ages, so instead, we ran it in parallel, and batches them together\n",
    "#tokened_lemma_docs=df[wa].apply(lambda x: lemmatize_stanford(x,keep_numbers=False))\n",
    "\n",
    "#Bring all the batches together\n",
    "tokened_lemma_docs=pd.concat([pd.read_pickle(\"./lemma_docs/lemma_docs_\"+str(idx)+\".pkl\") for idx in range(20)],ignore_index=True)\n",
    "df['lemma_docs_with_stop']=tokened_lemma_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#Additional cleaning post lemmatizing:\n",
    "#abstracts with no tokens become np.nan\n",
    "#remove_custom_words: Abstracts have the names of the pis removed (remove_custom_words) and lowercased\n",
    "#remove_first_x_tokens: Remove starting phrases (and any tokens proceeding them up to \"x\") like 'description', 'provided', 'by', 'applicant'\n",
    "#remove_stopwords: Removes standard and user-entered custom stopwords. Note that this is not the text cleaning version to account for emptylists\n",
    "###############\n",
    "\n",
    "#Fill nulls to account for 2 blanks\n",
    "df['lemma_docs_with_stop'].fillna(value=np.nan,inplace=True)\n",
    "\n",
    "#Remove tokens of PI names from abstract, as well as lowercasing them\n",
    "no_pis=df.apply(lambda x: remove_custom_words(x,'lemma_docs_with_stop'),axis=1)\n",
    "\n",
    "#Remove starting phrases (and any tokens proceeding them up to \"x\") like 'description', 'provided', 'by', 'applicant'\n",
    "no_pis=no_pis.apply(remove_first_x_tokens,args=[start_phrases_to_remove])\n",
    "\n",
    "#Remove stopwords--nltk and those added on in 'additional_stopwords' function\n",
    "stopWords = create_stopwords()\n",
    "#tokened_docs_nostop = TextCleaning.remove_stopwords(no_pis, stopWords) #Old code--does not account for non list documents\n",
    "tokened_docs_nostop = no_pis.apply(remove_stopwords,args=[stopWords])\n",
    "df['tokened_docs_nostop'] = tokened_docs_nostop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#Create bigrams and trigrams\n",
    "###################\n",
    "#Note that this is not trained on null abstracts\n",
    "bigram = gensim.models.Phrases(df['tokened_docs_nostop'].dropna(), min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "#This function will return a bigram if\n",
    "bigram_docs=df['tokened_docs_nostop'].apply(lambda x: apply_n_grams(x,bigram))\n",
    "trigram = gensim.models.Phrases(bigram_docs.dropna(), threshold=100)  \n",
    "tri_docs =bigram_docs.apply(lambda x: apply_n_grams(x,trigram))\n",
    "df['tns_bi_tri_docs'] = tri_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_tokens']=df['tns_bi_tri_docs'].apply(clean_up_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed text\n",
    "df.to_pickle(\"./processed_dataset_stanford_lemma.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract IDs with no lemmas in them\n",
      "{74268, 79910}\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "#Create datasets for analysis\n",
    "############\n",
    "\n",
    "#All data\n",
    "docs = df['final_tokens'].dropna() #<--If you're having issues merging with a prior dataset, note that this will NOT be the same length as the df overall, because of nulls, so be careful appending\n",
    "id2word, corpus = LDAvariables.createLDAvars(docs)\n",
    "pickle.dump([corpus, id2word, docs], open('lda_data_stanford_lemma.sav','wb'))\n",
    "\n",
    "#The two IDs for abstracts that were a space\n",
    "print('Abstract IDs with no lemmas in them')\n",
    "print(set(range(len(docs)))-set(docs.index))\n",
    "\n",
    "#Just nsf\n",
    "nsf_docs=df.groupby('AGENCY').get_group('NSF')['final_tokens'].dropna()\n",
    "id2word, corpus = LDAvariables.createLDAvars(nsf_docs)\n",
    "pickle.dump([corpus, id2word, nsf_docs], open('nsf_stanford_lemma.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old code method:\n",
    "\n",
    "#Data Cleaning\n",
    "#df[wa]=df[wa].apply(str.lower)\n",
    "#df[wa]=df.apply(remove_institution,axis=1) #case sensitive\n",
    "\n",
    "#Tokenizing\n",
    "#tokened_abstracts = TextCleaning.tokenize(df['working_abstract'])\n",
    "#df['tokened_abstracts'] = tokened_abstracts\n",
    "\n",
    "#Removal of stopwords, starting phrases, and PIs\n",
    "#no_pis=df.apply(remove_custom_words,axis=1).apply(remove_first_x_tokens,args=[start_phrases_to_remove])\n",
    "#stopWords = create_stopwords()\n",
    "#tokened_docs_nostop = TextCleaning.remove_stopwords(no_pis, stopWords)\n",
    "#df['tokened_docs_nostop'] = tokened_docs_nostop\n",
    "\n",
    "#n-grams\n",
    "#bigram = gensim.models.Phrases(df['tokened_docs_nostop'], min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "#bigram_docs=df['tokened_docs_nostop'].apply(lambda x: bigram[x])\n",
    "#trigram = gensim.models.Phrases(bigram_docs, threshold=100)  \n",
    "#tri_docs =bigram_docs.apply(lambda x: trigram[x])\n",
    "#df['tns_bi_tri_docs'] = tri_docs\n",
    "\n",
    "#Lemmatizing\n",
    "#lemma_docs = TextCleaning.lemmatize(df['tns_bi_tri_docs'])\n",
    "#df['lemma_abstracts'] = lemma_docs\n",
    "\n",
    "# save processed text\n",
    "\n",
    "#df.to_pickle(\"./processed_dataset.pkl\")\n",
    "#df.to_csv('FRAbstractsProcessed.csv')\n",
    "\n",
    "# Save only what is needed for LDA - docs, corpus, and dictionary. When loading the entire dataframe, I have run \n",
    "# out of memory to run the model\n",
    "\n",
    "# from Sam's code:\n",
    "#    corpus = corpus, dictionary = id2word, texts = docs\n",
    "\n",
    "#docs = df['lemma_abstracts']\n",
    "#id2word, corpus = LDAvariables.createLDAvars(docs)\n",
    "#pickle.dump([corpus, id2word, docs], open('lda_data.sav','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
