{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Number of Topics for each model - LDA, NMF, LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NSF data\n",
    "#f = open('../../data/prd/RND Topic Modelling/nsf_stanford_lemma.sav', 'rb')\n",
    "\n",
    "# import entire dataset\n",
    "f = open('../../data/prd/RND Topic Modelling/lda_data_stanford_lemma.sav', 'rb')\n",
    "\n",
    "[corpus, id2word, docs] = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# corpus - word frequency in docs\n",
    "# id2word - dictionary\n",
    "# docs - lemmatized abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needed for LDA, NMF and LSA (all from Scikit-Learn) is one string per document (not a list of strings)\n",
    "\n",
    "text = []\n",
    "for doc in docs:\n",
    "    text.append(\" \".join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions needed for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function slightly modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format topics as a \"list of list of strings\".\n",
    "# Needed for topic coherence function in Gensim\n",
    "\n",
    "# function modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def list_topics(model, vectorizer, top_n=10):\n",
    "\n",
    "    #input. top_n: how many words to list per topic.  If -1, then list all words.\n",
    "       \n",
    "    topic_words = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        #print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        if top_n == -1:   \n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1]])\n",
    "        else:\n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document-term matrix\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.4, min_df=3, lowercase=False, max_features=int(len(docs)/2))\n",
    "doc_term_matrix = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "\n",
    "def lda_metrics(doc_term_matrix, n_topics, vectorizer, corpus, id2word, docs, rand_start):\n",
    "    \"\"\"\n",
    "    Compute perplexity and c_v topic coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    doc_term_matrix\n",
    "    n_topics : list of number of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    coherence_values : c_v topic coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    perplexity_values = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    i = rand_start\n",
    "    for num_topics in n_topics:\n",
    "        \n",
    "        # create model\n",
    "        t1 = time.time()\n",
    "        lda_model = LatentDirichletAllocation(n_components=num_topics, doc_topic_prior = 1/num_topics, \n",
    "                                              topic_word_prior=0.1, n_jobs=39, random_state = i)\n",
    "        lda_model.fit_transform(doc_term_matrix)\n",
    "        t2 = time.time()\n",
    "        print(f\"  Model time: {t2-t1}\")\n",
    "        \n",
    "        # compute perplexity\n",
    "        perplexity_values.append(lda_model.bound_)\n",
    "        \n",
    "        # create list of topics\n",
    "        topics = list_topics(lda_model, vectorizer, top_n=10)\n",
    "        \n",
    "        # calculate coherence\n",
    "        t1 = time.time()\n",
    "        cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=id2word, texts=docs, \n",
    "                            coherence='c_v', processes=10) #window_size=500 ) \n",
    "        coherence_values.append(cm.get_coherence())\n",
    "        t2 = time.time()\n",
    "        print(f\"  Coherence time: {t2-t1}\")\n",
    "        \n",
    "        # output completion message\n",
    "        i = i+1\n",
    "        print('Number of topics =', num_topics, \"complete.\")\n",
    "\n",
    "    return perplexity_values, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  Model time: 333.9065086841583\n",
      "  Coherence time: 144.03825736045837\n",
      "Number of topics = 16 complete.\n"
     ]
    }
   ],
   "source": [
    "# code copied from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "# minor alterations made\n",
    "\n",
    "n_topics = [16] #range(4,101,4) \n",
    "num_runs = 1\n",
    "\n",
    "batch= 9\n",
    "\n",
    "col_names = [f\"iteration {i+batch}\" for i in range(num_runs)]\n",
    "lda_p = pd.DataFrame(index = n_topics, columns = col_names)\n",
    "lda_c = pd.DataFrame(index = n_topics, columns = col_names)\n",
    "\n",
    "for i in range(num_runs):\n",
    "    \n",
    "    print(f\"Iteration {i}\")\n",
    "    \n",
    "    # run models\n",
    "    [p, c] = lda_metrics(doc_term_matrix=doc_term_matrix, n_topics=n_topics, vectorizer=vectorizer, \n",
    "                         corpus=corpus, id2word=id2word, docs=docs, rand_start = 228) #(i+batch)*len(n_topics))\n",
    "    \n",
    "    # save results\n",
    "    lda_p[f\"iteration {i+batch}\"] = p\n",
    "    lda_c[f\"iteration {i+batch}\"] = c\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2752.512653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration 9\n",
       "16  2752.512653"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.574763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration 9\n",
       "16     0.574763"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "\n",
    "#lda_p.to_pickle(\"./nsf_lda_p.pkl\")\n",
    "#lda_c.to_pickle(\"./nsf_lda_c.pkl\")\n",
    "\n",
    "lda_p.to_pickle(\"./lda_p.pkl\")\n",
    "lda_c.to_pickle(\"./lda_c.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for NMF and LSA\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.4, min_df=3, lowercase=False, max_features=int(len(docs)/2))\n",
    "tf_idf = tfidf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "\n",
    "def nmf_metrics(doc_term_matrix, n_topics, vectorizer, corpus, id2word, docs, rand_start):\n",
    "    \"\"\"\n",
    "    Compute c_v topic coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tf_idf\n",
    "    n_topics : list of number of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    coherence_values : c_v topic coherence values corresponding to the NMF model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    coherence_values = []\n",
    "    \n",
    "    i = rand_start\n",
    "    for num_topics in n_topics:\n",
    "\n",
    "        # create model\n",
    "        t1 = time.time()\n",
    "        nmf_model = NMF(n_components=num_topics, random_state = i)\n",
    "        nmf_model.fit_transform(doc_term_matrix)\n",
    "        t2 = time.time()\n",
    "        print(f\"  Model time: {t2-t1}\")\n",
    "        \n",
    "        # create list of topics\n",
    "        topics = list_topics(nmf_model, vectorizer, top_n=10)\n",
    "        \n",
    "        # calculate coherence\n",
    "        t1 = time.time()\n",
    "        cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=id2word, texts=docs, \n",
    "                            coherence='c_v', processes=10) #window_size=500 ) \n",
    "        coherence_values.append(cm.get_coherence())\n",
    "        t2 = time.time()\n",
    "        print(f\"  Coherence time: {t2-t1}\")\n",
    "        \n",
    "        # output completion message\n",
    "        i = i+1\n",
    "        print('Number of topics =', num_topics, \"complete.\")\n",
    "\n",
    "    return coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nmf_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-153a2d39bc3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# run models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     c = nmf_metrics(doc_term_matrix=tf_idf, n_topics=n_topics, vectorizer=tfidf_vectorizer, \n\u001b[0m\u001b[1;32m     18\u001b[0m                          corpus=corpus, id2word=id2word, docs=docs, rand_start = (i+batch)*len(n_topics))\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nmf_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# code copied from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "# minor alterations made\n",
    "\n",
    "n_topics = [16] #range(4,101,4) \n",
    "num_runs = 1\n",
    "\n",
    "batch = 2\n",
    "\n",
    "col_names = [f\"iteration {i+batch}\" for i in range(num_runs)]\n",
    "nmf_c = pd.DataFrame(index = n_topics, columns = col_names)\n",
    "\n",
    "for i in range(num_runs):\n",
    "    \n",
    "    print(f\"Iteration {i}\")\n",
    "    \n",
    "    # run models\n",
    "    c = nmf_metrics(doc_term_matrix=tf_idf, n_topics=n_topics, vectorizer=tfidf_vectorizer, \n",
    "                         corpus=corpus, id2word=id2word, docs=docs, rand_start = (i+batch)*len(n_topics))\n",
    "    \n",
    "    # save results\n",
    "    nmf_c[f\"iteration {i+batch}\"] = c\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "\n",
    "#nmf_c.to_pickle(\"./nsf_nmf_c67.pkl\")\n",
    "\n",
    "nmf_c.to_pickle(\"./nmf_c.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA\n",
    "\n",
    "We use the same tf_idf created for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "\n",
    "def lsa_metrics(doc_term_matrix, n_topics, vectorizer, corpus, id2word, docs, rand_start):\n",
    "    \"\"\"\n",
    "    Compute c_v topic coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tf_idf\n",
    "    n_topics : list of number of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    coherence_values : c_v topic coherence values corresponding to the LSA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    coherence_values = []\n",
    "    \n",
    "    i = rand_start\n",
    "    for num_topics in n_topics:\n",
    "\n",
    "        # create model\n",
    "        t1 = time.time()\n",
    "        lsa_model = TruncatedSVD(n_components=num_topics, random_state = i)\n",
    "        lsa_model.fit_transform(doc_term_matrix)\n",
    "        t2 = time.time()\n",
    "        print(f\"  Model time: {t2-t1}\")\n",
    "        \n",
    "        # create list of topics\n",
    "        topics = list_topics(lsa_model, vectorizer, top_n=10)\n",
    "        \n",
    "        # calculate coherence\n",
    "        t1 = time.time()\n",
    "        cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=id2word, texts=docs, \n",
    "                            coherence='c_v', processes=10) #window_size=500 ) \n",
    "        coherence_values.append(cm.get_coherence())\n",
    "        t2 = time.time()\n",
    "        print(f\"  Coherence time: {t2-t1}\")\n",
    "        \n",
    "        # output completion message\n",
    "        i = i+1\n",
    "        print('Number of topics =', num_topics, \"complete.\")\n",
    "\n",
    "    return coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code copied from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "# minor alterations made\n",
    "\n",
    "n_topics = range(4,101,4) \n",
    "num_runs = 3\n",
    "\n",
    "batch = 4\n",
    "\n",
    "col_names = [f\"iteration {i+batch}\" for i in range(num_runs)]\n",
    "lsa_c = pd.DataFrame(index = n_topics, columns = col_names)\n",
    "\n",
    "for i in range(num_runs):\n",
    "    \n",
    "    print(f\"Iteration {i}\")\n",
    "    \n",
    "    # run models\n",
    "    c = lsa_metrics(doc_term_matrix=tf_idf, n_topics=n_topics, vectorizer=tfidf_vectorizer, \n",
    "                         corpus=corpus, id2word=id2word, docs=docs, rand_start = (i+batch)*len(n_topics))\n",
    "    \n",
    "    # save results\n",
    "    lsa_c[f\"iteration {i+batch}\"] = c\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "\n",
    "#lsa_c.to_pickle(\"./nsf_lsa_c.pkl\")\n",
    "\n",
    "lsa_c.to_pickle(\"./lsa_c456.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# plot results\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(n_topics, lda_c)\n",
    "plt.title(\"NSF data\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"c_v Topic Coherence\")\n",
    "#plt.legend((\"coherence\"), loc='best')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(n_topics, lda_p)\n",
    "plt.title(\"NSF data\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "#plt.legend((\"perplexity\"), loc='best')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = pd.read_pickle(\"./lda_data_ALL/all_lda_c0.pkl\")\n",
    "c1 = pd.read_pickle(\"./lda_data_ALL/all_lda_c1-3.pkl\")\n",
    "c2 = pd.read_pickle(\"./lda_data_ALL/all_lda_c4-6.pkl\")\n",
    "c3 = pd.read_pickle(\"./lda_data_ALL/all_lda_c7-9.pkl\")\n",
    "\n",
    "p0 = pd.read_pickle(\"./lda_data_ALL/all_lda_p0.pkl\")\n",
    "p1 = pd.read_pickle(\"./lda_data_ALL/all_lda_p1-3.pkl\")\n",
    "p2 = pd.read_pickle(\"./lda_data_ALL/all_lda_p4-6.pkl\")\n",
    "p3 = pd.read_pickle(\"./lda_data_ALL/all_lda_p7-9.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns if necessary\n",
    "\n",
    "df1 = df1.rename(columns={\"iteration 0\": \"iteration 1\"})\n",
    "df23 = df23.rename(columns={\"iteration 0\": \"iteration 2\", \"iteration 1\": \"iteration 3\"})\n",
    "df45 = df45.rename(columns={\"iteration 0\": \"iteration 4\", \"iteration 1\": \"iteration 5\"})\n",
    "df67 = df67.rename(columns={\"iteration 0\": \"iteration 6\", \"iteration 1\": \"iteration 7\"})\n",
    "df89 = df89.rename(columns={\"iteration 0\": \"iteration 8\", \"iteration 1\": \"iteration 9\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_coherence = pd.concat([c0, c1, c2, c3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration 0</th>\n",
       "      <th>iteration 1</th>\n",
       "      <th>iteration 2</th>\n",
       "      <th>iteration 3</th>\n",
       "      <th>iteration 4</th>\n",
       "      <th>iteration 5</th>\n",
       "      <th>iteration 6</th>\n",
       "      <th>iteration 7</th>\n",
       "      <th>iteration 8</th>\n",
       "      <th>iteration 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.525502</td>\n",
       "      <td>0.516735</td>\n",
       "      <td>0.484837</td>\n",
       "      <td>0.521476</td>\n",
       "      <td>0.471487</td>\n",
       "      <td>0.490747</td>\n",
       "      <td>0.515226</td>\n",
       "      <td>0.516282</td>\n",
       "      <td>0.525464</td>\n",
       "      <td>0.517342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.572111</td>\n",
       "      <td>0.526925</td>\n",
       "      <td>0.558833</td>\n",
       "      <td>0.522835</td>\n",
       "      <td>0.550526</td>\n",
       "      <td>0.559538</td>\n",
       "      <td>0.572227</td>\n",
       "      <td>0.560100</td>\n",
       "      <td>0.555319</td>\n",
       "      <td>0.576492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.584261</td>\n",
       "      <td>0.578889</td>\n",
       "      <td>0.574052</td>\n",
       "      <td>0.573472</td>\n",
       "      <td>0.565584</td>\n",
       "      <td>0.525675</td>\n",
       "      <td>0.564939</td>\n",
       "      <td>0.585593</td>\n",
       "      <td>0.554570</td>\n",
       "      <td>0.567584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.589525</td>\n",
       "      <td>0.580833</td>\n",
       "      <td>0.589836</td>\n",
       "      <td>0.586698</td>\n",
       "      <td>0.567065</td>\n",
       "      <td>0.581224</td>\n",
       "      <td>0.562805</td>\n",
       "      <td>0.592358</td>\n",
       "      <td>0.564412</td>\n",
       "      <td>0.574763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.567955</td>\n",
       "      <td>0.576958</td>\n",
       "      <td>0.565704</td>\n",
       "      <td>0.576695</td>\n",
       "      <td>0.583724</td>\n",
       "      <td>0.584184</td>\n",
       "      <td>0.590186</td>\n",
       "      <td>0.580329</td>\n",
       "      <td>0.592885</td>\n",
       "      <td>0.561338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.605736</td>\n",
       "      <td>0.590307</td>\n",
       "      <td>0.585401</td>\n",
       "      <td>0.569520</td>\n",
       "      <td>0.599309</td>\n",
       "      <td>0.589748</td>\n",
       "      <td>0.577234</td>\n",
       "      <td>0.592363</td>\n",
       "      <td>0.583009</td>\n",
       "      <td>0.571619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.583631</td>\n",
       "      <td>0.619945</td>\n",
       "      <td>0.601776</td>\n",
       "      <td>0.584610</td>\n",
       "      <td>0.581604</td>\n",
       "      <td>0.589834</td>\n",
       "      <td>0.610876</td>\n",
       "      <td>0.585780</td>\n",
       "      <td>0.583893</td>\n",
       "      <td>0.591035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.597807</td>\n",
       "      <td>0.597357</td>\n",
       "      <td>0.607031</td>\n",
       "      <td>0.572299</td>\n",
       "      <td>0.597684</td>\n",
       "      <td>0.609442</td>\n",
       "      <td>0.596639</td>\n",
       "      <td>0.591546</td>\n",
       "      <td>0.591964</td>\n",
       "      <td>0.600725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.588713</td>\n",
       "      <td>0.599887</td>\n",
       "      <td>0.619905</td>\n",
       "      <td>0.612457</td>\n",
       "      <td>0.585150</td>\n",
       "      <td>0.607189</td>\n",
       "      <td>0.593386</td>\n",
       "      <td>0.583210</td>\n",
       "      <td>0.591024</td>\n",
       "      <td>0.603566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.583883</td>\n",
       "      <td>0.595965</td>\n",
       "      <td>0.588807</td>\n",
       "      <td>0.611395</td>\n",
       "      <td>0.604895</td>\n",
       "      <td>0.587521</td>\n",
       "      <td>0.609423</td>\n",
       "      <td>0.600010</td>\n",
       "      <td>0.595055</td>\n",
       "      <td>0.596664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.583203</td>\n",
       "      <td>0.609837</td>\n",
       "      <td>0.591277</td>\n",
       "      <td>0.602801</td>\n",
       "      <td>0.597840</td>\n",
       "      <td>0.606106</td>\n",
       "      <td>0.598593</td>\n",
       "      <td>0.600496</td>\n",
       "      <td>0.611381</td>\n",
       "      <td>0.593899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.598437</td>\n",
       "      <td>0.605189</td>\n",
       "      <td>0.604116</td>\n",
       "      <td>0.606391</td>\n",
       "      <td>0.595891</td>\n",
       "      <td>0.612725</td>\n",
       "      <td>0.606754</td>\n",
       "      <td>0.590995</td>\n",
       "      <td>0.601105</td>\n",
       "      <td>0.605767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.585431</td>\n",
       "      <td>0.596097</td>\n",
       "      <td>0.615519</td>\n",
       "      <td>0.597791</td>\n",
       "      <td>0.612372</td>\n",
       "      <td>0.619315</td>\n",
       "      <td>0.616807</td>\n",
       "      <td>0.613717</td>\n",
       "      <td>0.603707</td>\n",
       "      <td>0.600467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.608546</td>\n",
       "      <td>0.596016</td>\n",
       "      <td>0.594673</td>\n",
       "      <td>0.602443</td>\n",
       "      <td>0.624119</td>\n",
       "      <td>0.605865</td>\n",
       "      <td>0.625887</td>\n",
       "      <td>0.611672</td>\n",
       "      <td>0.599099</td>\n",
       "      <td>0.611618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.600563</td>\n",
       "      <td>0.605060</td>\n",
       "      <td>0.618478</td>\n",
       "      <td>0.591022</td>\n",
       "      <td>0.622470</td>\n",
       "      <td>0.598218</td>\n",
       "      <td>0.595132</td>\n",
       "      <td>0.612509</td>\n",
       "      <td>0.581411</td>\n",
       "      <td>0.611685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.603247</td>\n",
       "      <td>0.592120</td>\n",
       "      <td>0.606273</td>\n",
       "      <td>0.609009</td>\n",
       "      <td>0.607385</td>\n",
       "      <td>0.604021</td>\n",
       "      <td>0.593931</td>\n",
       "      <td>0.602785</td>\n",
       "      <td>0.610115</td>\n",
       "      <td>0.604417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.612892</td>\n",
       "      <td>0.594460</td>\n",
       "      <td>0.600130</td>\n",
       "      <td>0.616021</td>\n",
       "      <td>0.598973</td>\n",
       "      <td>0.603681</td>\n",
       "      <td>0.600897</td>\n",
       "      <td>0.606863</td>\n",
       "      <td>0.600036</td>\n",
       "      <td>0.612840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.611240</td>\n",
       "      <td>0.608049</td>\n",
       "      <td>0.606911</td>\n",
       "      <td>0.605576</td>\n",
       "      <td>0.601210</td>\n",
       "      <td>0.615917</td>\n",
       "      <td>0.611088</td>\n",
       "      <td>0.606524</td>\n",
       "      <td>0.616476</td>\n",
       "      <td>0.600480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.599671</td>\n",
       "      <td>0.606797</td>\n",
       "      <td>0.614172</td>\n",
       "      <td>0.607662</td>\n",
       "      <td>0.601383</td>\n",
       "      <td>0.595469</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.616015</td>\n",
       "      <td>0.615623</td>\n",
       "      <td>0.604235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.600116</td>\n",
       "      <td>0.603059</td>\n",
       "      <td>0.607931</td>\n",
       "      <td>0.602285</td>\n",
       "      <td>0.614100</td>\n",
       "      <td>0.602793</td>\n",
       "      <td>0.598919</td>\n",
       "      <td>0.610229</td>\n",
       "      <td>0.597131</td>\n",
       "      <td>0.590176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.603233</td>\n",
       "      <td>0.613943</td>\n",
       "      <td>0.598489</td>\n",
       "      <td>0.605542</td>\n",
       "      <td>0.586950</td>\n",
       "      <td>0.589807</td>\n",
       "      <td>0.598933</td>\n",
       "      <td>0.616861</td>\n",
       "      <td>0.594254</td>\n",
       "      <td>0.614605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.587839</td>\n",
       "      <td>0.596914</td>\n",
       "      <td>0.592618</td>\n",
       "      <td>0.597022</td>\n",
       "      <td>0.600933</td>\n",
       "      <td>0.587384</td>\n",
       "      <td>0.592832</td>\n",
       "      <td>0.601560</td>\n",
       "      <td>0.604190</td>\n",
       "      <td>0.603393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.606262</td>\n",
       "      <td>0.614024</td>\n",
       "      <td>0.619615</td>\n",
       "      <td>0.592613</td>\n",
       "      <td>0.591702</td>\n",
       "      <td>0.614149</td>\n",
       "      <td>0.593392</td>\n",
       "      <td>0.597280</td>\n",
       "      <td>0.606532</td>\n",
       "      <td>0.596328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.584125</td>\n",
       "      <td>0.605841</td>\n",
       "      <td>0.610531</td>\n",
       "      <td>0.610260</td>\n",
       "      <td>0.590029</td>\n",
       "      <td>0.611201</td>\n",
       "      <td>0.586190</td>\n",
       "      <td>0.596381</td>\n",
       "      <td>0.593886</td>\n",
       "      <td>0.606861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.609527</td>\n",
       "      <td>0.599862</td>\n",
       "      <td>0.594248</td>\n",
       "      <td>0.605509</td>\n",
       "      <td>0.596557</td>\n",
       "      <td>0.589186</td>\n",
       "      <td>0.575508</td>\n",
       "      <td>0.618336</td>\n",
       "      <td>0.597569</td>\n",
       "      <td>0.593328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iteration 0  iteration 1  iteration 2  iteration 3  iteration 4  \\\n",
       "4       0.525502     0.516735     0.484837     0.521476     0.471487   \n",
       "8       0.572111     0.526925     0.558833     0.522835     0.550526   \n",
       "12      0.584261     0.578889     0.574052     0.573472     0.565584   \n",
       "16      0.589525     0.580833     0.589836     0.586698     0.567065   \n",
       "20      0.567955     0.576958     0.565704     0.576695     0.583724   \n",
       "24      0.605736     0.590307     0.585401     0.569520     0.599309   \n",
       "28      0.583631     0.619945     0.601776     0.584610     0.581604   \n",
       "32      0.597807     0.597357     0.607031     0.572299     0.597684   \n",
       "36      0.588713     0.599887     0.619905     0.612457     0.585150   \n",
       "40      0.583883     0.595965     0.588807     0.611395     0.604895   \n",
       "44      0.583203     0.609837     0.591277     0.602801     0.597840   \n",
       "48      0.598437     0.605189     0.604116     0.606391     0.595891   \n",
       "52      0.585431     0.596097     0.615519     0.597791     0.612372   \n",
       "56      0.608546     0.596016     0.594673     0.602443     0.624119   \n",
       "60      0.600563     0.605060     0.618478     0.591022     0.622470   \n",
       "64      0.603247     0.592120     0.606273     0.609009     0.607385   \n",
       "68      0.612892     0.594460     0.600130     0.616021     0.598973   \n",
       "72      0.611240     0.608049     0.606911     0.605576     0.601210   \n",
       "76      0.599671     0.606797     0.614172     0.607662     0.601383   \n",
       "80      0.600116     0.603059     0.607931     0.602285     0.614100   \n",
       "84      0.603233     0.613943     0.598489     0.605542     0.586950   \n",
       "88      0.587839     0.596914     0.592618     0.597022     0.600933   \n",
       "92      0.606262     0.614024     0.619615     0.592613     0.591702   \n",
       "96      0.584125     0.605841     0.610531     0.610260     0.590029   \n",
       "100     0.609527     0.599862     0.594248     0.605509     0.596557   \n",
       "\n",
       "     iteration 5  iteration 6  iteration 7  iteration 8  iteration 9  \n",
       "4       0.490747     0.515226     0.516282     0.525464     0.517342  \n",
       "8       0.559538     0.572227     0.560100     0.555319     0.576492  \n",
       "12      0.525675     0.564939     0.585593     0.554570     0.567584  \n",
       "16      0.581224     0.562805     0.592358     0.564412     0.574763  \n",
       "20      0.584184     0.590186     0.580329     0.592885     0.561338  \n",
       "24      0.589748     0.577234     0.592363     0.583009     0.571619  \n",
       "28      0.589834     0.610876     0.585780     0.583893     0.591035  \n",
       "32      0.609442     0.596639     0.591546     0.591964     0.600725  \n",
       "36      0.607189     0.593386     0.583210     0.591024     0.603566  \n",
       "40      0.587521     0.609423     0.600010     0.595055     0.596664  \n",
       "44      0.606106     0.598593     0.600496     0.611381     0.593899  \n",
       "48      0.612725     0.606754     0.590995     0.601105     0.605767  \n",
       "52      0.619315     0.616807     0.613717     0.603707     0.600467  \n",
       "56      0.605865     0.625887     0.611672     0.599099     0.611618  \n",
       "60      0.598218     0.595132     0.612509     0.581411     0.611685  \n",
       "64      0.604021     0.593931     0.602785     0.610115     0.604417  \n",
       "68      0.603681     0.600897     0.606863     0.600036     0.612840  \n",
       "72      0.615917     0.611088     0.606524     0.616476     0.600480  \n",
       "76      0.595469     0.612700     0.616015     0.615623     0.604235  \n",
       "80      0.602793     0.598919     0.610229     0.597131     0.590176  \n",
       "84      0.589807     0.598933     0.616861     0.594254     0.614605  \n",
       "88      0.587384     0.592832     0.601560     0.604190     0.603393  \n",
       "92      0.614149     0.593392     0.597280     0.606532     0.596328  \n",
       "96      0.611201     0.586190     0.596381     0.593886     0.606861  \n",
       "100     0.589186     0.575508     0.618336     0.597569     0.593328  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_coherence.to_pickle(\"./lda_all_coherence.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_perplexity = pd.concat([p0, p1, p2, p3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration 0</th>\n",
       "      <th>iteration 1</th>\n",
       "      <th>iteration 2</th>\n",
       "      <th>iteration 3</th>\n",
       "      <th>iteration 4</th>\n",
       "      <th>iteration 5</th>\n",
       "      <th>iteration 6</th>\n",
       "      <th>iteration 7</th>\n",
       "      <th>iteration 8</th>\n",
       "      <th>iteration 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3411.850617</td>\n",
       "      <td>3470.474742</td>\n",
       "      <td>3417.276040</td>\n",
       "      <td>3498.457741</td>\n",
       "      <td>3458.967470</td>\n",
       "      <td>3440.116389</td>\n",
       "      <td>3456.834542</td>\n",
       "      <td>3456.981969</td>\n",
       "      <td>3411.192599</td>\n",
       "      <td>3452.274672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3086.985890</td>\n",
       "      <td>3095.768809</td>\n",
       "      <td>3066.296019</td>\n",
       "      <td>3125.895783</td>\n",
       "      <td>3115.637292</td>\n",
       "      <td>3039.088288</td>\n",
       "      <td>3074.477114</td>\n",
       "      <td>3105.269439</td>\n",
       "      <td>3030.043877</td>\n",
       "      <td>3021.558313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2863.172934</td>\n",
       "      <td>2822.892377</td>\n",
       "      <td>2844.603640</td>\n",
       "      <td>2884.786139</td>\n",
       "      <td>2894.029123</td>\n",
       "      <td>2881.276110</td>\n",
       "      <td>2884.880764</td>\n",
       "      <td>2868.998633</td>\n",
       "      <td>2885.188989</td>\n",
       "      <td>2888.700197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2761.281565</td>\n",
       "      <td>2789.988357</td>\n",
       "      <td>2700.742524</td>\n",
       "      <td>2776.553340</td>\n",
       "      <td>2743.972965</td>\n",
       "      <td>2761.983738</td>\n",
       "      <td>2764.946260</td>\n",
       "      <td>2751.663137</td>\n",
       "      <td>2761.217982</td>\n",
       "      <td>2752.512653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2698.868888</td>\n",
       "      <td>2686.914208</td>\n",
       "      <td>2696.475790</td>\n",
       "      <td>2636.333342</td>\n",
       "      <td>2641.749492</td>\n",
       "      <td>2682.264809</td>\n",
       "      <td>2634.079236</td>\n",
       "      <td>2684.279605</td>\n",
       "      <td>2655.402770</td>\n",
       "      <td>2724.368298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2615.713814</td>\n",
       "      <td>2598.526983</td>\n",
       "      <td>2619.954561</td>\n",
       "      <td>2604.806371</td>\n",
       "      <td>2577.051923</td>\n",
       "      <td>2590.714647</td>\n",
       "      <td>2589.597093</td>\n",
       "      <td>2604.317709</td>\n",
       "      <td>2598.437246</td>\n",
       "      <td>2614.512329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2573.387007</td>\n",
       "      <td>2556.035496</td>\n",
       "      <td>2534.411520</td>\n",
       "      <td>2560.242781</td>\n",
       "      <td>2565.825100</td>\n",
       "      <td>2574.442458</td>\n",
       "      <td>2548.842372</td>\n",
       "      <td>2572.175109</td>\n",
       "      <td>2574.055280</td>\n",
       "      <td>2577.835906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2508.584065</td>\n",
       "      <td>2546.299230</td>\n",
       "      <td>2504.199384</td>\n",
       "      <td>2539.052117</td>\n",
       "      <td>2541.041433</td>\n",
       "      <td>2493.438208</td>\n",
       "      <td>2543.361929</td>\n",
       "      <td>2508.029562</td>\n",
       "      <td>2511.296978</td>\n",
       "      <td>2530.016826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2501.122269</td>\n",
       "      <td>2476.239002</td>\n",
       "      <td>2463.482486</td>\n",
       "      <td>2494.876162</td>\n",
       "      <td>2499.015256</td>\n",
       "      <td>2485.582394</td>\n",
       "      <td>2480.352125</td>\n",
       "      <td>2500.854008</td>\n",
       "      <td>2485.841674</td>\n",
       "      <td>2484.224644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2465.073743</td>\n",
       "      <td>2467.997133</td>\n",
       "      <td>2476.197199</td>\n",
       "      <td>2484.433925</td>\n",
       "      <td>2460.838535</td>\n",
       "      <td>2458.959215</td>\n",
       "      <td>2485.054891</td>\n",
       "      <td>2470.834881</td>\n",
       "      <td>2458.305425</td>\n",
       "      <td>2468.237953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2454.868309</td>\n",
       "      <td>2445.940244</td>\n",
       "      <td>2446.670606</td>\n",
       "      <td>2452.805607</td>\n",
       "      <td>2451.647079</td>\n",
       "      <td>2434.222982</td>\n",
       "      <td>2442.282191</td>\n",
       "      <td>2431.341082</td>\n",
       "      <td>2453.735028</td>\n",
       "      <td>2452.624810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2435.819184</td>\n",
       "      <td>2423.165764</td>\n",
       "      <td>2432.596458</td>\n",
       "      <td>2426.913774</td>\n",
       "      <td>2434.825741</td>\n",
       "      <td>2440.321686</td>\n",
       "      <td>2415.917638</td>\n",
       "      <td>2436.357600</td>\n",
       "      <td>2445.259908</td>\n",
       "      <td>2416.535610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2430.274280</td>\n",
       "      <td>2425.658951</td>\n",
       "      <td>2411.579215</td>\n",
       "      <td>2417.098446</td>\n",
       "      <td>2430.793665</td>\n",
       "      <td>2412.858099</td>\n",
       "      <td>2399.908553</td>\n",
       "      <td>2406.484309</td>\n",
       "      <td>2406.764419</td>\n",
       "      <td>2408.111764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2371.061969</td>\n",
       "      <td>2385.807445</td>\n",
       "      <td>2408.914253</td>\n",
       "      <td>2387.657019</td>\n",
       "      <td>2374.860190</td>\n",
       "      <td>2425.950861</td>\n",
       "      <td>2378.145166</td>\n",
       "      <td>2401.764493</td>\n",
       "      <td>2416.324578</td>\n",
       "      <td>2392.164985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2403.991993</td>\n",
       "      <td>2392.454642</td>\n",
       "      <td>2395.454664</td>\n",
       "      <td>2394.272600</td>\n",
       "      <td>2386.045764</td>\n",
       "      <td>2383.703808</td>\n",
       "      <td>2371.145917</td>\n",
       "      <td>2389.070230</td>\n",
       "      <td>2404.571064</td>\n",
       "      <td>2392.730392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2389.064645</td>\n",
       "      <td>2394.949847</td>\n",
       "      <td>2369.342396</td>\n",
       "      <td>2385.294324</td>\n",
       "      <td>2367.053923</td>\n",
       "      <td>2371.832844</td>\n",
       "      <td>2378.839474</td>\n",
       "      <td>2394.104011</td>\n",
       "      <td>2373.255168</td>\n",
       "      <td>2369.354334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2363.895251</td>\n",
       "      <td>2385.098810</td>\n",
       "      <td>2361.626386</td>\n",
       "      <td>2360.422682</td>\n",
       "      <td>2367.223782</td>\n",
       "      <td>2374.215018</td>\n",
       "      <td>2375.086219</td>\n",
       "      <td>2366.865050</td>\n",
       "      <td>2377.179932</td>\n",
       "      <td>2376.624343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2362.774527</td>\n",
       "      <td>2367.692082</td>\n",
       "      <td>2381.436146</td>\n",
       "      <td>2363.107235</td>\n",
       "      <td>2368.049256</td>\n",
       "      <td>2374.707162</td>\n",
       "      <td>2363.941646</td>\n",
       "      <td>2358.455029</td>\n",
       "      <td>2352.195647</td>\n",
       "      <td>2378.718914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2336.566684</td>\n",
       "      <td>2342.184507</td>\n",
       "      <td>2347.749141</td>\n",
       "      <td>2366.923209</td>\n",
       "      <td>2378.773145</td>\n",
       "      <td>2367.154327</td>\n",
       "      <td>2359.976685</td>\n",
       "      <td>2357.515854</td>\n",
       "      <td>2360.651616</td>\n",
       "      <td>2358.555118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2375.739353</td>\n",
       "      <td>2371.317474</td>\n",
       "      <td>2354.788096</td>\n",
       "      <td>2353.560958</td>\n",
       "      <td>2359.079180</td>\n",
       "      <td>2355.653299</td>\n",
       "      <td>2351.253714</td>\n",
       "      <td>2348.799470</td>\n",
       "      <td>2371.408677</td>\n",
       "      <td>2365.155405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2349.563347</td>\n",
       "      <td>2365.036766</td>\n",
       "      <td>2351.420938</td>\n",
       "      <td>2348.425939</td>\n",
       "      <td>2375.809015</td>\n",
       "      <td>2361.213481</td>\n",
       "      <td>2363.709287</td>\n",
       "      <td>2348.311898</td>\n",
       "      <td>2351.406663</td>\n",
       "      <td>2365.936282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2354.627638</td>\n",
       "      <td>2342.971462</td>\n",
       "      <td>2352.037101</td>\n",
       "      <td>2366.717258</td>\n",
       "      <td>2341.369523</td>\n",
       "      <td>2345.041186</td>\n",
       "      <td>2353.773179</td>\n",
       "      <td>2369.952361</td>\n",
       "      <td>2337.748849</td>\n",
       "      <td>2356.435743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2352.923972</td>\n",
       "      <td>2354.041092</td>\n",
       "      <td>2334.806983</td>\n",
       "      <td>2342.891620</td>\n",
       "      <td>2344.262248</td>\n",
       "      <td>2346.895382</td>\n",
       "      <td>2351.119065</td>\n",
       "      <td>2346.248461</td>\n",
       "      <td>2361.462637</td>\n",
       "      <td>2363.654566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2341.476898</td>\n",
       "      <td>2339.124967</td>\n",
       "      <td>2350.546461</td>\n",
       "      <td>2332.656638</td>\n",
       "      <td>2346.912338</td>\n",
       "      <td>2356.119275</td>\n",
       "      <td>2356.105957</td>\n",
       "      <td>2339.343961</td>\n",
       "      <td>2359.439180</td>\n",
       "      <td>2326.234119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2349.036705</td>\n",
       "      <td>2336.096538</td>\n",
       "      <td>2336.790921</td>\n",
       "      <td>2347.673871</td>\n",
       "      <td>2370.183324</td>\n",
       "      <td>2345.263906</td>\n",
       "      <td>2360.202038</td>\n",
       "      <td>2333.404739</td>\n",
       "      <td>2330.539086</td>\n",
       "      <td>2348.213553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iteration 0  iteration 1  iteration 2  iteration 3  iteration 4  \\\n",
       "4    3411.850617  3470.474742  3417.276040  3498.457741  3458.967470   \n",
       "8    3086.985890  3095.768809  3066.296019  3125.895783  3115.637292   \n",
       "12   2863.172934  2822.892377  2844.603640  2884.786139  2894.029123   \n",
       "16   2761.281565  2789.988357  2700.742524  2776.553340  2743.972965   \n",
       "20   2698.868888  2686.914208  2696.475790  2636.333342  2641.749492   \n",
       "24   2615.713814  2598.526983  2619.954561  2604.806371  2577.051923   \n",
       "28   2573.387007  2556.035496  2534.411520  2560.242781  2565.825100   \n",
       "32   2508.584065  2546.299230  2504.199384  2539.052117  2541.041433   \n",
       "36   2501.122269  2476.239002  2463.482486  2494.876162  2499.015256   \n",
       "40   2465.073743  2467.997133  2476.197199  2484.433925  2460.838535   \n",
       "44   2454.868309  2445.940244  2446.670606  2452.805607  2451.647079   \n",
       "48   2435.819184  2423.165764  2432.596458  2426.913774  2434.825741   \n",
       "52   2430.274280  2425.658951  2411.579215  2417.098446  2430.793665   \n",
       "56   2371.061969  2385.807445  2408.914253  2387.657019  2374.860190   \n",
       "60   2403.991993  2392.454642  2395.454664  2394.272600  2386.045764   \n",
       "64   2389.064645  2394.949847  2369.342396  2385.294324  2367.053923   \n",
       "68   2363.895251  2385.098810  2361.626386  2360.422682  2367.223782   \n",
       "72   2362.774527  2367.692082  2381.436146  2363.107235  2368.049256   \n",
       "76   2336.566684  2342.184507  2347.749141  2366.923209  2378.773145   \n",
       "80   2375.739353  2371.317474  2354.788096  2353.560958  2359.079180   \n",
       "84   2349.563347  2365.036766  2351.420938  2348.425939  2375.809015   \n",
       "88   2354.627638  2342.971462  2352.037101  2366.717258  2341.369523   \n",
       "92   2352.923972  2354.041092  2334.806983  2342.891620  2344.262248   \n",
       "96   2341.476898  2339.124967  2350.546461  2332.656638  2346.912338   \n",
       "100  2349.036705  2336.096538  2336.790921  2347.673871  2370.183324   \n",
       "\n",
       "     iteration 5  iteration 6  iteration 7  iteration 8  iteration 9  \n",
       "4    3440.116389  3456.834542  3456.981969  3411.192599  3452.274672  \n",
       "8    3039.088288  3074.477114  3105.269439  3030.043877  3021.558313  \n",
       "12   2881.276110  2884.880764  2868.998633  2885.188989  2888.700197  \n",
       "16   2761.983738  2764.946260  2751.663137  2761.217982  2752.512653  \n",
       "20   2682.264809  2634.079236  2684.279605  2655.402770  2724.368298  \n",
       "24   2590.714647  2589.597093  2604.317709  2598.437246  2614.512329  \n",
       "28   2574.442458  2548.842372  2572.175109  2574.055280  2577.835906  \n",
       "32   2493.438208  2543.361929  2508.029562  2511.296978  2530.016826  \n",
       "36   2485.582394  2480.352125  2500.854008  2485.841674  2484.224644  \n",
       "40   2458.959215  2485.054891  2470.834881  2458.305425  2468.237953  \n",
       "44   2434.222982  2442.282191  2431.341082  2453.735028  2452.624810  \n",
       "48   2440.321686  2415.917638  2436.357600  2445.259908  2416.535610  \n",
       "52   2412.858099  2399.908553  2406.484309  2406.764419  2408.111764  \n",
       "56   2425.950861  2378.145166  2401.764493  2416.324578  2392.164985  \n",
       "60   2383.703808  2371.145917  2389.070230  2404.571064  2392.730392  \n",
       "64   2371.832844  2378.839474  2394.104011  2373.255168  2369.354334  \n",
       "68   2374.215018  2375.086219  2366.865050  2377.179932  2376.624343  \n",
       "72   2374.707162  2363.941646  2358.455029  2352.195647  2378.718914  \n",
       "76   2367.154327  2359.976685  2357.515854  2360.651616  2358.555118  \n",
       "80   2355.653299  2351.253714  2348.799470  2371.408677  2365.155405  \n",
       "84   2361.213481  2363.709287  2348.311898  2351.406663  2365.936282  \n",
       "88   2345.041186  2353.773179  2369.952361  2337.748849  2356.435743  \n",
       "92   2346.895382  2351.119065  2346.248461  2361.462637  2363.654566  \n",
       "96   2356.119275  2356.105957  2339.343961  2359.439180  2326.234119  \n",
       "100  2345.263906  2360.202038  2333.404739  2330.539086  2348.213553  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_perplexity.to_pickle(\"./lda_all_perplexity.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
