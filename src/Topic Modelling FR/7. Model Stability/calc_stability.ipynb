{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/software/standard/compiler/gcc/7.1.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import logging as log\n",
    "import numpy as np\n",
    "import term_similarity\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full corpus\n",
    "#df = pd.read_pickle(\"../dspg20RnD/data/final/final_dataset_7-20.pkl\")\n",
    "\n",
    "# pandemics corpus\n",
    "#df = pd.read_pickle(\"../dspg20RnD/data/final/dashboard_data/pandemic_corpus.pkl\")\n",
    "#df.reset_index(inplace = True)\n",
    "\n",
    "# coronavirus corpus\n",
    "df = pd.read_pickle(\"../dspg20RnD/data/final/dashboard_data/corona_corpus.pkl\")\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needed for LDA, NMF (all from Scikit-Learn) is one string per document (not a list of strings)\n",
    "text = []\n",
    "docs = df[\"final_frqwds_removed\"]\n",
    "\n",
    "for abstract in docs:\n",
    "    text.append(\" \".join(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format topics as a \"list of list of strings\".\n",
    "# Needed for topic coherence function in Gensim\n",
    "\n",
    "# function modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def list_topics(model, vectorizer, top_n):\n",
    "\n",
    "    #input. top_n: how many words to list per topic.  If -1, then list all words.\n",
    "       \n",
    "    topic_words = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "            \n",
    "        if top_n == -1:   \n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1]])\n",
    "        else:\n",
    "            topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "    \n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NMF\n",
    "# Optimal Model: full dataset, 75 topics, random_state = 14\n",
    "# Pandemic Model: pandemic dataset, 30, random_state = 1\n",
    "# Coronavirus Model: coronavirus dataset, 30, random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for full dataset\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.6, min_df=20, lowercase=False, max_features=int(len(docs)/2))\n",
    "#tf_idf = tfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "# use for pandemic or coronavirus dataset\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df = 1.0, min_df = 3, lowercase = False)\n",
    "tf_idf = tfidf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stability_measures(num_topics, text_mat, vectorizer, num_run, n_word):\n",
    "    \n",
    "    all_descriptor_set = []\n",
    "    topic_words = []\n",
    "    all_partition = []\n",
    "    \n",
    "    for run in range(num_run):\n",
    "\n",
    "        nmf_model = NMF(n_components = num_topics,\n",
    "                        random_state = run + 1)\n",
    "        doc_topic = nmf_model.fit_transform(text_mat)\n",
    "\n",
    "        doc_topic_partition = pd.DataFrame(doc_topic).idxmax(axis = 1).values.tolist()\n",
    "        all_partition.append(doc_topic_partition)\n",
    "\n",
    "        topics = list_topics(nmf_model, vectorizer, top_n = n_word)\n",
    "        topic_words.append(topics)\n",
    "\n",
    "        all_topic_words = [item for sublist in topics for item in sublist]\n",
    "        all_descriptor_set.append(all_topic_words)\n",
    "        \n",
    "    ### descriptor set difference ###\n",
    "\n",
    "    # get the set of all terms used in the top terms for specified model\n",
    "    all_model_terms = []\n",
    "    for descrip_set in all_descriptor_set:\n",
    "        model_terms = set()\n",
    "        for term in descrip_set:\n",
    "            model_terms.add(term)\n",
    "        all_model_terms.append(model_terms)\n",
    "\n",
    "    # perform pairwise comparisons to get DSD\n",
    "    all_dsd = []\n",
    "    for i in range(num_run):\n",
    "        for j in range(i+1,num_run):\n",
    "            diff = len(all_model_terms[i].symmetric_difference(all_model_terms[j]))\n",
    "            ndiff = float(diff)/(num_topics*n_word)\n",
    "            all_dsd.append(ndiff)\n",
    "\n",
    "    avg_dsd = sum(all_dsd)/(len(all_dsd))\n",
    "    \n",
    "    ### term stability ###\n",
    "\n",
    "    metric = term_similarity.JaccardBinary()\n",
    "    matcher = term_similarity.RankingSetAgreement(metric)\n",
    "\n",
    "    # calculate TS score on each pair of models\n",
    "    all_ts = []\n",
    "    for i in range(num_run):\n",
    "        for j in range(i+1,num_run):\n",
    "            score = matcher.similarity(topic_words[i], topic_words[j])\n",
    "            all_ts.append(score)\n",
    "\n",
    "    avg_ts = sum(all_ts)/(len(all_ts))\n",
    "    \n",
    "    ### partition stability ###\n",
    "\n",
    "    # calculate NMI on each pair of model partitions\n",
    "    all_nmi = []\n",
    "    for i in range(num_run):\n",
    "        for j in range(i+1,num_run):\n",
    "            score = normalized_mutual_info_score(all_partition[i], all_partition[j])\n",
    "            all_nmi.append(score)\n",
    "\n",
    "    p_nmi = sum(all_nmi)/(len(all_nmi))\n",
    "    \n",
    "    return avg_dsd, avg_ts, p_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 30\n",
    "num_run = 20\n",
    "top_n_lst = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "dsd_res = []\n",
    "ts_res = []\n",
    "nmi_res = []\n",
    "\n",
    "for n_word in top_n_lst:\n",
    "    \n",
    "    avg_dsd, avg_ts, p_nmi = calc_stability_measures(num_topics, tf_idf, tfidf_vectorizer, \n",
    "                                                     num_run, n_word)\n",
    "    \n",
    "    dsd_res.append({'Top terms': n_word,\n",
    "                    'ADSD': avg_dsd})\n",
    "    \n",
    "    ts_res.append({'Top terms': n_word,\n",
    "                   'ATS': avg_ts})\n",
    "\n",
    "    nmi_res.append({'Top terms': n_word,\n",
    "                    'PNMI': p_nmi})\n",
    "\n",
    "dsd_df = pd.DataFrame(dsd_res)\n",
    "ts_df = pd.DataFrame(ts_res)\n",
    "nmi_df = pd.DataFrame(nmi_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(dsd_df['Top terms'], dsd_df['ADSD'])\n",
    "#plt.title(\"ADSD\")\n",
    "plt.xlabel(\"Number of top terms for each topic\")\n",
    "plt.ylabel(\"ADSD\")\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,0.5])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(ts_df['Top terms'], ts_df['ATS'])\n",
    "#plt.title(\"Dataset 2008-2019\")\n",
    "plt.xlabel(\"Number of top terms for each topic\")\n",
    "plt.ylabel(\"ATS\")\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.5,1.0])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(nmi_df['Top terms'], nmi_df['PNMI'])\n",
    "#plt.title(\"Dataset 2008-2019\")\n",
    "plt.xlabel(\"Number of top terms for each topic\")\n",
    "plt.ylabel(\"PNMI\")\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.5,1.0])\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/coronavirus_stability_measures.png\", dpi = 800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meas_df = dsd_df.merge(ts_df, \n",
    "                           left_on = 'Top terms', \n",
    "                           right_on = 'Top terms').merge(nmi_df, \n",
    "                                                         left_on = 'Top terms', \n",
    "                                                         right_on = 'Top terms')\n",
    "all_meas_df.to_csv('results/coronavirus_stability_measures.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
