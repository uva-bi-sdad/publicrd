{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic nmf year = 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import pickle\n",
    "import time\n",
    "import joblib\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import islice\n",
    "from scipy.linalg import block_diag\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.matutils import hellinger\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "# Set all functions for the dynamic topics modellings\n",
    "\n",
    "# function to create a new dictionary and corpus\n",
    "def createLDAvars(docs):\n",
    "\n",
    "    # Create the variables needed for LDA from df[final_frqwds_removed]: dictionary (id2word), corpus\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "    #Filter words to only those found in at least a set number of documents (min_appearances)\n",
    "    id2word.filter_extremes(no_below=20, no_above=0.6)\n",
    "    \n",
    "    # filter out stop words - \"use\" already filtered out by previous line\n",
    "    id2word.filter_tokens(bad_ids=[id2word.token2id['research'], id2word.token2id['project']])\n",
    "\n",
    "    # Create Corpus (Term Document Frequency)\n",
    "\n",
    "    #Creates a count for each unique word appearing in the document, where the word_id is substituted for the word\n",
    "    # corpus not need for c_v coherence\n",
    "    corpus = [id2word.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    return id2word, corpus\n",
    "\n",
    "\n",
    "# function to pre-process the data: compute tfidf\n",
    "def preprocess(df, stopwords):\n",
    "    # Append all the final tokens\n",
    "    text = []\n",
    "    docs = df['list_final_tokens']\n",
    "    \n",
    "    for abstract in docs:\n",
    "        text.append(' '.join(abstract))\n",
    "        \n",
    "    # Create the term-document matrix\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=0, lowercase=False, stop_words=stop_wds)\n",
    "    tf_idf = tfidf_vectorizer.fit_transform(text)\n",
    "            \n",
    "    return (tf_idf, tfidf_vectorizer)\n",
    "\n",
    "\n",
    "# function to list topic (modified function from https://nlpforhackers.io/topic-modeling/)\n",
    "def list_topics(topic_term_dist, vectorizer, top_n=10):\n",
    "\n",
    "    #input. top_n: how many words to list per topic.  If -1, then list all words.  \n",
    "    topic_words = []\n",
    "    \n",
    "    for idx, topic in enumerate(topic_term_dist):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "            \n",
    "        if top_n == -1: \n",
    "            # check if the vectorized has an attribute get_features_names. if not vectorized contains terms hasattr('abc', 'lower')\n",
    "            if hasattr(vectorizer, 'get_feature_names'):\n",
    "                topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1]])\n",
    "            else:\n",
    "                topic_words.append([vectorizer[i] for i in topic.argsort()[::-1]])\n",
    "        else:\n",
    "            if hasattr(vectorizer, 'get_feature_names'):\n",
    "                topic_words.append([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "            else:\n",
    "                topic_words.append([vectorizer[i] for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "    return topic_words\n",
    "\n",
    "\n",
    "# function to solve the nmf (modified from https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/)\n",
    "def nmf_models(doc_term_matrix, n_topics, vectorizer, rand_start):\n",
    "    \"\"\"\n",
    "    Compute NMF model, save topics list for coherence calc\n",
    "    Parameters:\n",
    "    ----------\n",
    "    doc_term_matrix: document-terms matrix\n",
    "    n_topics: list of topics number\n",
    "    vectorizer: vector of terms\n",
    "    rand_start: random seed\n",
    "    \"\"\"\n",
    "    \n",
    "    nmf_time = []\n",
    "    topics_list = []\n",
    "    W_list = []\n",
    "    H_list = []\n",
    "    \n",
    "    i = rand_start\n",
    "    for num_topics in n_topics:\n",
    "\n",
    "        # create model\n",
    "        t1 = time.time()\n",
    "        nmf_model = NMF(n_components=num_topics, random_state = i)\n",
    "        nmf_model.fit_transform(doc_term_matrix)\n",
    "        t2 = time.time()\n",
    "        nmf_time.append(t2-t1)\n",
    "        #print(f\"  Model time: {t2-t1}\", flush=True)\n",
    "        \n",
    "        # create list of topics\n",
    "        topics = list_topics(nmf_model.components_, vectorizer, top_n=10)\n",
    "        topics_list.append(topics)\n",
    "        \n",
    "        # output completion message\n",
    "        i = i+1\n",
    "        #print('Number of topics =', num_topics, \"complete.\", flush=True)\n",
    "        \n",
    "        # save the matrix W and H\n",
    "        W = nmf_model.fit_transform(doc_term_matrix)\n",
    "        W_list.append(W)\n",
    "        H = nmf_model.components_\n",
    "        \n",
    "        # truncate the H matrix: set the weight of the non top n words to zero\n",
    "        #top_n = 10\n",
    "        #for idx, topic in enumerate(H):\n",
    "        #    thresold = numpy.nanmin(topic[topic.argsort()[:-top_n-1:-1]])\n",
    "        #    topic[topic<thresold]=0  \n",
    "        H_list.append(H)\n",
    "\n",
    "    return nmf_time, topics_list, W_list, H_list\n",
    "\n",
    "\n",
    "# Create a new document term matrix using the topic distribution\n",
    "def create_matrix(windows_H, windows_terms):\n",
    "    \"\"\"\n",
    "    Create the topic-term matrix from all window topics that have been added so far.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    windows_H: windiws topic distribution of top n words\n",
    "    windows_terms: windows terms used for each fiscal year\n",
    "    \"\"\"\n",
    "    # Set a list of all terms unique terms across windows (all_terms) and the combine windows terms (all_windows_terms)\n",
    "    all_windows_terms = sum(windows_terms,[])\n",
    "    \n",
    "    # Create a block diagonal matrix of all topics: the number of rows is the same as the length of list_terms\n",
    "    M = block_diag(*windows_H)\n",
    "    \n",
    "    # Identify duplicated terms (columns) and sum them\n",
    "    # The fastest way is to transform M into data frame with\n",
    "    dfM = pd.DataFrame(data = M, columns=all_windows_terms).groupby(level=0, axis=1).sum()\n",
    "    \n",
    "    # Transform back the dataframe to matrix and get the variable names (in the order in the matrix) as the final all terms\n",
    "    M_concat = dfM.to_numpy()\n",
    "    all_terms = list(dfM.columns)\n",
    "    \n",
    "    print('--- New document-terms have been created ---')\n",
    "    \n",
    "    return M_concat, all_terms\n",
    "\n",
    "\n",
    "# function to solve the second stage of the dynamic nmf\n",
    "def second_stage(windows_H, windows_terms, n_topics):\n",
    "    \"\"\"\n",
    "    Build a new document term matrix and run a new nmf model\n",
    "    Parameters:\n",
    "    ----------\n",
    "    windows_H: windiws topic distribution of top n words\n",
    "    windows_terms: windows terms used for each fiscal year\n",
    "    n_topics: list of topics number for the second stage\n",
    "    \"\"\"\n",
    "    batch = 7\n",
    "    \n",
    "    # Build the new document-term matrix\n",
    "    (M, all_terms) = create_matrix(windows_H, windows_terms)\n",
    "    \n",
    "    # Run a second nmf model\n",
    "    (nmf_time,topics_list,W_list,H_list) = nmf_models(doc_term_matrix=M, n_topics=n_topics, vectorizer=all_terms, rand_start = (batch)*len(n_topics))\n",
    "    \n",
    "    print('--- Dynamic nmf: second stage clear ---')\n",
    "    \n",
    "    return M, all_terms, topics_list, W_list,H_list\n",
    "\n",
    "\n",
    "# Track the dynamic of a given topic (option topic)\n",
    "def track_dynamic(topic,W,windows_topic_list):\n",
    "    \"\"\"\n",
    "    Link topics in the first stage with topic in second stage using the matrix W\n",
    "    Parameters:\n",
    "    ----------\n",
    "    topic: topic to track the dynamic\n",
    "    W: weigth matrix from the second stage\n",
    "    windows_topic_list: topic list from the first stage\n",
    "    \"\"\"\n",
    "    # For each topic from the first stage (rows) find the topic in the second stage (columns) with the higher weight\n",
    "    topic_second = []\n",
    "    for i, topic_first in enumerate(W):\n",
    "        topic_second.append(topic_first.argmax())\n",
    "        \n",
    "    # Split topics classification in the first by year\n",
    "    it = iter(topic_second)\n",
    "    topic_first_year = [[next(it) for _ in range(size)] for size in windows_topic]\n",
    "    \n",
    "    # For each topic, identify the correspondance for each year\n",
    "    dynamic_topic_list = []\n",
    "    for y in range(0, len(year)):\n",
    "        topic_year = [i for i, e in enumerate(topic_first_year[y]) if e == topic]\n",
    "        dynamic_topic_list.append(topic_year)\n",
    "\n",
    "    # Compute the list of list of topics (list of year and list of main topic)\n",
    "    dynamic_topic = []\n",
    "    for y in range(0, len(year)):\n",
    "        dynamic_list = dynamic_topic_list[y]\n",
    "        fy_topic = [windows_topic_list[y][dynamic_list[i]] for i in range(0,len(dynamic_list))] \n",
    "        dynamic_topic.append(fy_topic)\n",
    "        \n",
    "    # Print the result in a dataframe\n",
    "    topic_print = []\n",
    "    names = []\n",
    "\n",
    "    # print the dynamic topic\n",
    "    for y in range(0,len(year)):\n",
    "        for t in range(0,len(dynamic_topic[y])):\n",
    "            topic_print.append(dynamic_topic[y][t])\n",
    "            names.append('Year_'+str(year[y])+'_'+str(t))\n",
    "        \n",
    "    df = pd.DataFrame (topic_print).transpose()\n",
    "    df.columns = names\n",
    "    \n",
    "    return df, dynamic_topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>PROJECT_TERMS</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>AGENCY</th>\n",
       "      <th>IC_CENTER</th>\n",
       "      <th>PROJECT_NUMBER</th>\n",
       "      <th>PROJECT_START_DATE</th>\n",
       "      <th>PROJECT_END_DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>BUDGET_END_DATE</th>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <th>FY</th>\n",
       "      <th>FY_TOTAL_COST</th>\n",
       "      <th>FY_TOTAL_COST_SUB_PROJECTS</th>\n",
       "      <th>ORG_COUNT</th>\n",
       "      <th>PI_COUNT</th>\n",
       "      <th>FY_TOTAL_COST_SUM</th>\n",
       "      <th>NUM_RECORDS</th>\n",
       "      <th>final_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89996</td>\n",
       "      <td>This is a project to explore Game-based, Metap...</td>\n",
       "      <td>Achievement; analog; base; Cognitive Science; ...</td>\n",
       "      <td>RUI: CYGAMES: CYBER-ENABLED TEACHING AND LEARN...</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0814512</td>\n",
       "      <td>9/15/2008</td>\n",
       "      <td>8/31/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.076</td>\n",
       "      <td>2008</td>\n",
       "      <td>1999467.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1999467.0</td>\n",
       "      <td>1</td>\n",
       "      <td>project explore game base metaphor enhanced ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89997</td>\n",
       "      <td>Institution: Franklin Institute Science Museum...</td>\n",
       "      <td>Active Learning; Child; Computer software; des...</td>\n",
       "      <td>ARIEL - AUGMENTED REALITY FOR INTERPRETIVE AND...</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0741659</td>\n",
       "      <td>9/15/2008</td>\n",
       "      <td>8/31/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.076</td>\n",
       "      <td>2008</td>\n",
       "      <td>1799699.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1799699.0</td>\n",
       "      <td>1</td>\n",
       "      <td>institution franklin institute science museum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89998</td>\n",
       "      <td>Through programs (including small group conver...</td>\n",
       "      <td>Address; Age; Birth; Brain; Caregivers; Child;...</td>\n",
       "      <td>BRIGHTER FUTURES: PUBLIC DELIBERATION ABOUT TH...</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0813522</td>\n",
       "      <td>9/15/2008</td>\n",
       "      <td>8/31/2011</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.076</td>\n",
       "      <td>2008</td>\n",
       "      <td>1505858.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1505858.0</td>\n",
       "      <td>1</td>\n",
       "      <td>program include small group conversation citiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89999</td>\n",
       "      <td>In partnership with the American Chemical Soci...</td>\n",
       "      <td>Advanced Development; American; Chemicals; Che...</td>\n",
       "      <td>FOSTERING US-INTERNATIONAL COLLABORATIVE PARTN...</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0838627</td>\n",
       "      <td>8/1/2008</td>\n",
       "      <td>12/31/2010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.049</td>\n",
       "      <td>2008</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>partnership american chemical society acs nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90001</td>\n",
       "      <td>The Center for Molecular Interfacing (CMI) wil...</td>\n",
       "      <td>Address; Architecture; Carbon Nanotubes; Catal...</td>\n",
       "      <td>CCI PHASE I: CENTER FOR MOLECULAR INTERFACING</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0847926</td>\n",
       "      <td>10/1/2008</td>\n",
       "      <td>9/30/2011</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.049</td>\n",
       "      <td>2008</td>\n",
       "      <td>1519821.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1519821.0</td>\n",
       "      <td>1</td>\n",
       "      <td>center molecular interfacing cmi enable integr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PROJECT_ID                                           ABSTRACT  \\\n",
       "0      89996  This is a project to explore Game-based, Metap...   \n",
       "1      89997  Institution: Franklin Institute Science Museum...   \n",
       "2      89998  Through programs (including small group conver...   \n",
       "3      89999  In partnership with the American Chemical Soci...   \n",
       "4      90001  The Center for Molecular Interfacing (CMI) wil...   \n",
       "\n",
       "                                       PROJECT_TERMS  \\\n",
       "0  Achievement; analog; base; Cognitive Science; ...   \n",
       "1  Active Learning; Child; Computer software; des...   \n",
       "2  Address; Age; Birth; Brain; Caregivers; Child;...   \n",
       "3  Advanced Development; American; Chemicals; Che...   \n",
       "4  Address; Architecture; Carbon Nanotubes; Catal...   \n",
       "\n",
       "                                       PROJECT_TITLE DEPARTMENT AGENCY  \\\n",
       "0  RUI: CYGAMES: CYBER-ENABLED TEACHING AND LEARN...        NSF    NSF   \n",
       "1  ARIEL - AUGMENTED REALITY FOR INTERPRETIVE AND...        NSF    NSF   \n",
       "2  BRIGHTER FUTURES: PUBLIC DELIBERATION ABOUT TH...        NSF    NSF   \n",
       "3  FOSTERING US-INTERNATIONAL COLLABORATIVE PARTN...        NSF    NSF   \n",
       "4      CCI PHASE I: CENTER FOR MOLECULAR INTERFACING        NSF    NSF   \n",
       "\n",
       "  IC_CENTER PROJECT_NUMBER PROJECT_START_DATE PROJECT_END_DATE  ...  \\\n",
       "0       NaN        0814512          9/15/2008        8/31/2012  ...   \n",
       "1       NaN        0741659          9/15/2008        8/31/2012  ...   \n",
       "2       NaN        0813522          9/15/2008        8/31/2011  ...   \n",
       "3       NaN        0838627           8/1/2008       12/31/2010  ...   \n",
       "4       NaN        0847926          10/1/2008        9/30/2011  ...   \n",
       "\n",
       "  BUDGET_END_DATE CFDA_CODE    FY FY_TOTAL_COST FY_TOTAL_COST_SUB_PROJECTS  \\\n",
       "0             NaN    47.076  2008     1999467.0                        NaN   \n",
       "1             NaN    47.076  2008     1799699.0                        NaN   \n",
       "2             NaN    47.076  2008     1505858.0                        NaN   \n",
       "3             NaN    47.049  2008       51000.0                        NaN   \n",
       "4             NaN    47.049  2008     1519821.0                        NaN   \n",
       "\n",
       "  ORG_COUNT PI_COUNT FY_TOTAL_COST_SUM NUM_RECORDS  \\\n",
       "0         1        1         1999467.0           1   \n",
       "1         1        1         1799699.0           1   \n",
       "2         1        1         1505858.0           1   \n",
       "3         1        1           51000.0           1   \n",
       "4         1        1         1519821.0           1   \n",
       "\n",
       "                                        final_tokens  \n",
       "0  project explore game base metaphor enhanced ga...  \n",
       "1  institution franklin institute science museum ...  \n",
       "2  program include small group conversation citiz...  \n",
       "3  partnership american chemical society acs nati...  \n",
       "4  center molecular interfacing cmi enable integr...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################################################################################################\n",
    "# Load the dataset \n",
    "df = pd.read_pickle(\"/project/biocomplexity/sdad/projects_data/ncses/prd/Paper/FR_meta_and_final_tokens_23DEC21.pkl\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tokens \n",
    "df[\"list_final_tokens\"] = df[\"final_tokens\"].str.split(' ').tolist()\n",
    "year = 2008\n",
    "\n",
    "# build the dictionary id2word\n",
    "docs = df[\"list_final_tokens\"]\n",
    "[dictionary, corpus] = createLDAvars(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################################\n",
    "# Run a dynamic topic model\n",
    "# First stage : use the same list of number of topics for both first and second stage\n",
    "path = '/project/biocomplexity/sdad/projects_data/ncses/prd/Dynamic_Topics_Modelling/nmf_fullabstract/Term_docs_'\n",
    "n_topics = list(range(20,61,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/project/biocomplexity/sdad/projects_data/ncses/prd/Dynamic_Topics_Modelling/nmf_fullabstract/slurm_result/nmf_08.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 7\n",
    "    \n",
    "windows_coherence = []\n",
    "windows_topic_list = []\n",
    "windows_topic = []\n",
    "windows_W = []\n",
    "windows_H = []\n",
    "windows_terms = []\n",
    "    \n",
    "# Run the dynamic nmf for each fiscal year\n",
    "# Load the document-term matrix\n",
    "(tf_idf,tfidf_vectorizer,df) = joblib.load( path+str(year)+'.pkl' )\n",
    "        \n",
    "# save all the term\n",
    "#terms = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "# Solve an nmf model for a given range of topics\n",
    "(nmf_time, topics_list, W_list ,H_list) = nmf_models(doc_term_matrix=tf_idf, n_topics=n_topics, vectorizer=tfidf_vectorizer, rand_start = (batch)*len(n_topics))\n",
    "\n",
    "# save output for the first stage\n",
    "joblib.dump((nmf_time, topics_list, W_list ,H_list), '/project/biocomplexity/sdad/projects_data/ncses/prd/Dynamic_Topics_Modelling/nmf_fullabstract/slurm_result/nmf_08.pkl' )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.coherencemodel.CoherenceModel'>\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-43c5061b8d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_term_rankings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcm_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#coherence.append(cm_value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \"\"\"\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0mconfirmed_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence_per_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfirmed_measures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0msegmented_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mestimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyed_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/topic_coherence/probability_estimation.py\u001b[0m in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using %s to estimate probabilities from sliding windows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36maccumulate\u001b[0;34m(self, texts, window_size)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_all_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36mstart_workers\u001b[0;34m(self, window_size)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatchedWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccumulatingWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/software/standard/core/anaconda/2020.11-py3.8/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mchild_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "# Compute the coherence for each topics\n",
    "coherence = []\n",
    "    \n",
    "for t in range(0,len(n_topics)):\n",
    "    window_term_rankings = topics_list[t]\n",
    "    cm = CoherenceModel(topics=window_term_rankings, dictionary=dictionary, texts=docs, coherence='c_v', topn=10, processes=3)\n",
    "    print(type(cm))\n",
    "    cm_value = cm.get_coherence()\n",
    "    #coherence.append(cm_value)\n",
    "    \n",
    "# find the topics that maximize the coherence\n",
    "max_coherence = numpy.nanmax(coherence)\n",
    "index = coherence.index(max_coherence)\n",
    "topic_select = n_topics[index]\n",
    "fy_topic_list = topics_list[index]\n",
    "W = W_list[index]\n",
    "H = H_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the best model (that maximize the coherence) transform the matrix H (for each topic set the weigth of non top n terms to 0)\n",
    "# select all the unique terms of topics\n",
    "topic_terms = list(set(sum(fy_topic_list,[])))\n",
    "        \n",
    "# select the index of terms that appear in the topics and subset the matrix H to those terms\n",
    "if hasattr(tfidf_vectorizer, 'get_feature_names'):\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "else:\n",
    "    terms = tfidf_vectorizer\n",
    "            \n",
    "indcol = [terms.index(i) for i in topic_terms]\n",
    "subH = H[:,indcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic (rows) set the weigth of terms that are not listed the topic to 0.\n",
    "for i,j in enumerate(subH):\n",
    "    # by row find the index of top_n terms\n",
    "    indtopic = [topic_terms.index(p) for p in fy_topic_list[i]]\n",
    "    notop = [k for k in range(len(topic_terms)) if k not in indtopic]\n",
    "    j[notop]=0\n",
    "\n",
    "# append the result\n",
    "windows_coherence.append(max_coherence)\n",
    "windows_topic_list.append(fy_topic_list)\n",
    "windows_topic.append(topic_select)\n",
    "windows_W.append(W)\n",
    "windows_H.append(subH)\n",
    "windows_terms.append(topic_terms)\n",
    "print('--- windows topic '+str(y)+' solve ---')\n",
    "        \n",
    "print('--- Dynamic nmf: first stage clear ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
