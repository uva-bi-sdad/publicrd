{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-06 16:14:38 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-11-06 16:14:38 INFO: Use device: cpu\n",
      "2020-11-06 16:14:38 INFO: Loading: tokenize\n",
      "2020-11-06 16:14:38 INFO: Loading: pos\n",
      "2020-11-06 16:14:39 INFO: Loading: lemma\n",
      "2020-11-06 16:14:39 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.20673060417175293\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import stanza\n",
    "import gensim\n",
    "import re\n",
    "\n",
    "# This is the same code at the top of Preprocessing_final.  It is ported here to be run as a SLURM script.\n",
    "\n",
    "# load saved df.  df['working_abstract'] contains cleaned text.\n",
    "\n",
    "text = \"objective biophysical basis thermodynamics kinetics multivalency multivalency concerned biologically important interactions multiple receptors multiple ligands interact simultaneously focus work explore synthesis properties groups join monovalent ligands linkers synthesis multivalent ligands\"\n",
    "\n",
    "# Create a stanza pipeline: this pipeline will tokenize, determine pos, \n",
    "# and then lemmatize the token appropriately.\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en',processors='tokenize,pos,lemma',tokenize_batch_size=500,lemma_batch_size=500,\n",
    "                      use_gpu = False)\n",
    "\n",
    "# Note - I thought about keeping the pos PART if the lemma was \"not\", ex) isn't --> be, not.  But due to the fact\n",
    "# that we are using a bag-of-words model that does not take into account word order, this won't matter in the\n",
    "# topic model.  So, this is not kept.\n",
    "\n",
    "\n",
    "def token_pos_lemma(doc, pretokened=False, keep_numbers=True):\n",
    "    \n",
    "    # This function uses the pipeline to tokenize, find POS, and lemmatize a document\n",
    "    \n",
    "    \"\"\"if pretokened, dont use this function, as it hasnt been adapted for it\"\"\"\n",
    "    \n",
    "    assert not pretokened #If these are already tokened per another pipeline, this function won't work correctly\n",
    "    \n",
    "    new_tokens=[]\n",
    "     \n",
    "    processed=nlp(doc)  # this is the line that does the tokenizing, pos, and lemmatizing\n",
    "    \n",
    "    for sent in processed.sentences:\n",
    "        for word in sent.words:\n",
    "            \n",
    "            #If its a regular noun, verb, adj, or adverb, keep lemmatized form\n",
    "            if word.pos in ['NOUN','VERB','ADJ','ADV']:\n",
    "                new_tokens.append(word.lemma)\n",
    "            \n",
    "            #If you decided to retain numbers, their lemma is kept here. \n",
    "            #Note that number catching isnt perfect by this lemmatizing.\n",
    "            elif word.pos=='NUM' and keep_numbers:\n",
    "                new_tokens.append(word.lemma)\n",
    "            \n",
    "            #Exact phrases are kept here with no attempt at lemmatization: e.g. mars does not become mars, \n",
    "            #and hopefully scientific words e.g. chemicals will be tagged as propn, x, or intj if needed\n",
    "            elif word.pos in ['PROPN','X','INTJ']: \n",
    "                new_tokens.append(word.text)\n",
    "            \n",
    "            #Note that no other tokens are kept        \n",
    "       \n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "temp = token_pos_lemma(text)\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"time: {t2-t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['understand',\n",
       " 'objective',\n",
       " 'biophysical',\n",
       " 'basis',\n",
       " 'thermodynamics',\n",
       " 'kinetic',\n",
       " 'multivalency',\n",
       " 'multivalency',\n",
       " 'concerned',\n",
       " 'biologically',\n",
       " 'important',\n",
       " 'interaction',\n",
       " 'multiple',\n",
       " 'receptor',\n",
       " 'multiple',\n",
       " 'ligand',\n",
       " 'interact',\n",
       " 'simultaneously',\n",
       " 'focus',\n",
       " 'work',\n",
       " 'explore',\n",
       " 'synthesis',\n",
       " 'property',\n",
       " 'group',\n",
       " 'join',\n",
       " 'monovalent',\n",
       " 'ligand',\n",
       " 'linker',\n",
       " 'synthesis',\n",
       " 'multivalent',\n",
       " 'ligand']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
