{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval - Search Terms from Raw Text or Processed Text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in entire dataframe\n",
    "\n",
    "df = pd.read_pickle(\"~/dspg20RnD/data/final/final_dataset_7-20.pkl\")\n",
    "\n",
    "df.reset_index(inplace = True)\n",
    "#df.rename(columns={'index':'original index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>original index</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>FY</th>\n",
       "      <th>PROJECT_TERMS</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>AGENCY</th>\n",
       "      <th>IC_CENTER</th>\n",
       "      <th>...</th>\n",
       "      <th>working_abstract</th>\n",
       "      <th>Start_Char</th>\n",
       "      <th>nchar</th>\n",
       "      <th>LAST_CHAR</th>\n",
       "      <th>lemma_abstract</th>\n",
       "      <th>clean_lemmas</th>\n",
       "      <th>stopwds_removed</th>\n",
       "      <th>n_grams_added</th>\n",
       "      <th>final_tokens</th>\n",
       "      <th>final_frqwds_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17608</td>\n",
       "      <td>152242</td>\n",
       "      <td>The multiprotein complex y-secretase proteolyt...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Active Sites; Affect; Alzheimer's Disease; Amy...</td>\n",
       "      <td>STRUCTURE OF SIGNAL PEPTIDE PEPTIDASE</td>\n",
       "      <td>HHS</td>\n",
       "      <td>NIH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>The multiprotein complex y-secretase proteolyt...</td>\n",
       "      <td>T</td>\n",
       "      <td>1402</td>\n",
       "      <td>g</td>\n",
       "      <td>[multiprotein, complex, y-secretase, proteolyt...</td>\n",
       "      <td>[multiprotein, complex, y-secretase, proteolyt...</td>\n",
       "      <td>[multiprotein, complex, y-secretase, proteolyt...</td>\n",
       "      <td>[multiprotein, complex, y-secretase, proteolyt...</td>\n",
       "      <td>[multiprotein, complex, y_secretase, proteolyt...</td>\n",
       "      <td>[multiprotein, y_secretase, proteolytically_cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>111864</td>\n",
       "      <td>190316</td>\n",
       "      <td>DESCRIPTION (provided by applicant):   The Kis...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Affect; Animal Model; Axon; Behavior; Behavior...</td>\n",
       "      <td>ROLE OF KISS1 NEURONS IN THE SEASONAL AND CIRC...</td>\n",
       "      <td>HHS</td>\n",
       "      <td>NIH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>The Kissl gene encodes peptides called kisspep...</td>\n",
       "      <td>T</td>\n",
       "      <td>2553</td>\n",
       "      <td>y</td>\n",
       "      <td>[Kissl, gene, encode, peptide, call, kisspepti...</td>\n",
       "      <td>[kissl, gene, encode, peptide, call, kisspepti...</td>\n",
       "      <td>[kissl, gene, encode, peptide, kisspeptin, bin...</td>\n",
       "      <td>[kissl, gene, encode, peptide, kisspeptin, bin...</td>\n",
       "      <td>[kissl, gene, encode, peptide, kisspeptin, bin...</td>\n",
       "      <td>[kissl, gene, encode, peptide, kisspeptin, bin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22052</td>\n",
       "      <td>154213</td>\n",
       "      <td>DESCRIPTION (provided by applicant): The objec...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Agreement; Antibodies; base; Binding; Biochemi...</td>\n",
       "      <td>CARBONIC ANHYDRASE AS A MODEL TO UNDERSTAND DI...</td>\n",
       "      <td>HHS</td>\n",
       "      <td>NIH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>The objective of this research is to understan...</td>\n",
       "      <td>T</td>\n",
       "      <td>1414</td>\n",
       "      <td>e</td>\n",
       "      <td>[objective, research, be, understand, biophysi...</td>\n",
       "      <td>[objective, research, be, understand, biophysi...</td>\n",
       "      <td>[objective, research, understand, biophysical,...</td>\n",
       "      <td>[objective, research, understand, biophysical,...</td>\n",
       "      <td>[objective, research, understand, biophysical,...</td>\n",
       "      <td>[biophysical, basis, thermodynamics_kinetic, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>35004</td>\n",
       "      <td>159362</td>\n",
       "      <td>Obesity is the cause of many adverse pregnancy...</td>\n",
       "      <td>2008</td>\n",
       "      <td>African; Analysis of Variance; Asians; Birth; ...</td>\n",
       "      <td>OBESITY ON VAGAL TONE AND HBA1C DURING PREGNANCY</td>\n",
       "      <td>HHS</td>\n",
       "      <td>NIH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Obesity is the cause of many adverse pregnancy...</td>\n",
       "      <td>O</td>\n",
       "      <td>1545</td>\n",
       "      <td>d</td>\n",
       "      <td>[obesity, cause, many, adverse, pregnancyoutco...</td>\n",
       "      <td>[obesity, cause, many, adverse, pregnancyoutco...</td>\n",
       "      <td>[obesity, cause, adverse, pregnancyoutcome, re...</td>\n",
       "      <td>[obesity, cause, adverse_pregnancyoutcome, res...</td>\n",
       "      <td>[obesity, cause, adverse_pregnancyoutcome, res...</td>\n",
       "      <td>[obesity, adverse_pregnancyoutcome, great, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>371628</td>\n",
       "      <td>594482</td>\n",
       "      <td>Local potato advisory groups have expressed in...</td>\n",
       "      <td>2010</td>\n",
       "      <td>cost; Health; interest; Manure; Parasitic nema...</td>\n",
       "      <td>PLANT-PARASITIC NEMATODE MANAGEMENT AS A COMPO...</td>\n",
       "      <td>USDA</td>\n",
       "      <td>NIFA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Local potato advisory groups have expressed in...</td>\n",
       "      <td>L</td>\n",
       "      <td>271</td>\n",
       "      <td>s</td>\n",
       "      <td>[local, potato, advisory, group, express, inte...</td>\n",
       "      <td>[local, potato, advisory, group, express, inte...</td>\n",
       "      <td>[local, potato, advisory, group, express, inte...</td>\n",
       "      <td>[local, potato, advisory, group, express, inte...</td>\n",
       "      <td>[local, potato, advisory, group, express, inte...</td>\n",
       "      <td>[local, potato, advisory, express, interest, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  original index PROJECT_ID  \\\n",
       "0      0           17608     152242   \n",
       "1      1          111864     190316   \n",
       "2      2           22052     154213   \n",
       "3      3           35004     159362   \n",
       "4      4          371628     594482   \n",
       "\n",
       "                                            ABSTRACT    FY  \\\n",
       "0  The multiprotein complex y-secretase proteolyt...  2008   \n",
       "1  DESCRIPTION (provided by applicant):   The Kis...  2008   \n",
       "2  DESCRIPTION (provided by applicant): The objec...  2008   \n",
       "3  Obesity is the cause of many adverse pregnancy...  2008   \n",
       "4  Local potato advisory groups have expressed in...  2010   \n",
       "\n",
       "                                       PROJECT_TERMS  \\\n",
       "0  Active Sites; Affect; Alzheimer's Disease; Amy...   \n",
       "1  Affect; Animal Model; Axon; Behavior; Behavior...   \n",
       "2  Agreement; Antibodies; base; Binding; Biochemi...   \n",
       "3  African; Analysis of Variance; Asians; Birth; ...   \n",
       "4  cost; Health; interest; Manure; Parasitic nema...   \n",
       "\n",
       "                                       PROJECT_TITLE DEPARTMENT AGENCY  \\\n",
       "0              STRUCTURE OF SIGNAL PEPTIDE PEPTIDASE        HHS    NIH   \n",
       "1  ROLE OF KISS1 NEURONS IN THE SEASONAL AND CIRC...        HHS    NIH   \n",
       "2  CARBONIC ANHYDRASE AS A MODEL TO UNDERSTAND DI...        HHS    NIH   \n",
       "3   OBESITY ON VAGAL TONE AND HBA1C DURING PREGNANCY        HHS    NIH   \n",
       "4  PLANT-PARASITIC NEMATODE MANAGEMENT AS A COMPO...       USDA   NIFA   \n",
       "\n",
       "  IC_CENTER  ...                                   working_abstract  \\\n",
       "0       NaN  ...  The multiprotein complex y-secretase proteolyt...   \n",
       "1       NaN  ...  The Kissl gene encodes peptides called kisspep...   \n",
       "2       NaN  ...  The objective of this research is to understan...   \n",
       "3       NaN  ...  Obesity is the cause of many adverse pregnancy...   \n",
       "4       NaN  ...  Local potato advisory groups have expressed in...   \n",
       "\n",
       "  Start_Char nchar LAST_CHAR  \\\n",
       "0          T  1402         g   \n",
       "1          T  2553         y   \n",
       "2          T  1414         e   \n",
       "3          O  1545         d   \n",
       "4          L   271         s   \n",
       "\n",
       "                                      lemma_abstract  \\\n",
       "0  [multiprotein, complex, y-secretase, proteolyt...   \n",
       "1  [Kissl, gene, encode, peptide, call, kisspepti...   \n",
       "2  [objective, research, be, understand, biophysi...   \n",
       "3  [obesity, cause, many, adverse, pregnancyoutco...   \n",
       "4  [local, potato, advisory, group, express, inte...   \n",
       "\n",
       "                                        clean_lemmas  \\\n",
       "0  [multiprotein, complex, y-secretase, proteolyt...   \n",
       "1  [kissl, gene, encode, peptide, call, kisspepti...   \n",
       "2  [objective, research, be, understand, biophysi...   \n",
       "3  [obesity, cause, many, adverse, pregnancyoutco...   \n",
       "4  [local, potato, advisory, group, express, inte...   \n",
       "\n",
       "                                     stopwds_removed  \\\n",
       "0  [multiprotein, complex, y-secretase, proteolyt...   \n",
       "1  [kissl, gene, encode, peptide, kisspeptin, bin...   \n",
       "2  [objective, research, understand, biophysical,...   \n",
       "3  [obesity, cause, adverse, pregnancyoutcome, re...   \n",
       "4  [local, potato, advisory, group, express, inte...   \n",
       "\n",
       "                                       n_grams_added  \\\n",
       "0  [multiprotein, complex, y-secretase, proteolyt...   \n",
       "1  [kissl, gene, encode, peptide, kisspeptin, bin...   \n",
       "2  [objective, research, understand, biophysical,...   \n",
       "3  [obesity, cause, adverse_pregnancyoutcome, res...   \n",
       "4  [local, potato, advisory, group, express, inte...   \n",
       "\n",
       "                                        final_tokens  \\\n",
       "0  [multiprotein, complex, y_secretase, proteolyt...   \n",
       "1  [kissl, gene, encode, peptide, kisspeptin, bin...   \n",
       "2  [objective, research, understand, biophysical,...   \n",
       "3  [obesity, cause, adverse_pregnancyoutcome, res...   \n",
       "4  [local, potato, advisory, group, express, inte...   \n",
       "\n",
       "                                final_frqwds_removed  \n",
       "0  [multiprotein, y_secretase, proteolytically_cl...  \n",
       "1  [kissl, gene, encode, peptide, kisspeptin, bin...  \n",
       "2  [biophysical, basis, thermodynamics_kinetic, m...  \n",
       "3  [obesity, adverse_pregnancyoutcome, great, hea...  \n",
       "4  [local, potato, advisory, express, interest, m...  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needed for doc-term matrix creation in Scikit-Learn is one string per document (not a list of strings).  \n",
    "# Original data is already in this form, but not if we search by the tokens instead of the original abstract.\n",
    "\n",
    "#docs = df[\"ABSTRACT\"] \n",
    "tokens = df[\"final_frqwds_removed\"]\n",
    "\n",
    "docs = []  # docs will contain the processed tokens in string form (1 string per abstract)\n",
    "\n",
    "for abstract in tokens:\n",
    "    docs.append(\" \".join(abstract))\n",
    "    \n",
    "docs = pd.Series(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed for all info retrieval approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query vector \n",
    "\n",
    "def create_query(words, terms):\n",
    "    \n",
    "    # words: search query words\n",
    "    # terms: terms in corpus\n",
    "    \n",
    "    q = np.zeros(len(terms))  # number of terms\n",
    "\n",
    "    idx = []\n",
    "    for word in query_words:\n",
    "        idx.append(terms.index(word))\n",
    "\n",
    "    q[idx] = 1\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_abstracts(docs, scores, top_n):\n",
    "    \n",
    "    '''\n",
    "    docs: Series that contains abstract\n",
    "    scores: scores of abstracts\n",
    "    top_n: return the top_n abstracts given by idx, if top_n = -1 return all abstracts\n",
    "    '''\n",
    "    # sort scores in descending order\n",
    "    scores_sorted_idx = np.argsort(scores)[::-1]\n",
    "    \n",
    "    if top_n == -1:\n",
    "        n = sum(scores > 0)\n",
    "        ix = scores_sorted_idx[:n]\n",
    "    else:\n",
    "        ix = scores_sorted_idx[:top_n]\n",
    "    \n",
    "    print(ix[0:10])\n",
    "    \n",
    "    return ix, docs[ix]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_df(abstracts, scores):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"abstracts\"] = abstracts\n",
    "    df[\"scores\"] = scores\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literal Term Matching - Frequency Count Document-Term Matrix\n",
    "\n",
    "This will return all abstracts in the corpus with exact word matches to the query.  \n",
    "\n",
    "Results will be return in sorted order of how high the query scores with each abstract. A high score means more occurences of the query words in the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - we are now using the spaCy stopwords list instead of nltk.  It is more comprehensive.\n",
    "\n",
    "def create_stopwords():\n",
    "      \n",
    "    \"\"\" creates list of stopwords. stop words include the general English list and any additional we see sneaking \n",
    "    through.  \"\"\"\n",
    "    \n",
    "    spacy_stop_words = STOP_WORDS\n",
    "\n",
    "    # more stop words that do not add meaning to topics\n",
    "    additional_stopwords = {'addition', 'specifically', 'similar','including', 'particular', \n",
    "                            'furthermore','include', 'includes','overall', 'finally', 'specific', \n",
    "                            'additional'} \n",
    "           \n",
    "    sw = spacy_stop_words.union(additional_stopwords)\n",
    "    \n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Create document-term matrix based on count frequencies  \n",
    "# when using raw text, it is appropriate to remove stop words, processed text will already have these words removed\n",
    "\n",
    "stop_words = create_stopwords()\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words, min_df=0)\n",
    "doc_term_matrix = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690814, 1277618)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Query Words - list the search terms\n",
    "\n",
    "A query is just a list of words to search for in the corpus.  We will use the same query for all three info retrieval techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'artificialintelligence_ai' in terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'machine_learning' in terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'machine_learn' in terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'supervise' in terms  \n",
    "\n",
    "# need to look at raw text...some bi-terms are coming up, cases could be confusing with lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test string'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"test string\"\n",
    "s2 = \"This is a test string for practice\"\n",
    "\n",
    "s1 in s2\n",
    "\n",
    "s1 or 'is' in s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975\n"
     ]
    }
   ],
   "source": [
    "# tokens with - or strange symbol between words\n",
    "\n",
    "count = 0\n",
    "idx = []\n",
    "\n",
    "for ix, abstract in enumerate(df['ABSTRACT']):\n",
    "    if 'artificial intelligence' in abstract.lower(): \n",
    "        count = count + 1\n",
    "        idx.append(ix)\n",
    "    elif 'artificialintelligence' in abstract.lower(): \n",
    "        count = count + 1\n",
    "        idx.append(ix)\n",
    "    elif 'artificially intelligent' in abstract.lower(): \n",
    "        count = count + 1\n",
    "        idx.append(ix)\n",
    "    #else: \n",
    "        # do nothing\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[206,\n",
       " 207,\n",
       " 2940,\n",
       " 2988,\n",
       " 3035,\n",
       " 3104,\n",
       " 3602,\n",
       " 3717,\n",
       " 4780,\n",
       " 6518,\n",
       " 6774,\n",
       " 6779,\n",
       " 6979,\n",
       " 10300,\n",
       " 10654,\n",
       " 10886,\n",
       " 13746,\n",
       " 14454,\n",
       " 15589,\n",
       " 18434,\n",
       " 18839,\n",
       " 18951,\n",
       " 19512,\n",
       " 19941,\n",
       " 20593,\n",
       " 20611,\n",
       " 20741,\n",
       " 21088,\n",
       " 21093,\n",
       " 21159,\n",
       " 21247,\n",
       " 22248,\n",
       " 22765,\n",
       " 23022,\n",
       " 23259,\n",
       " 23316,\n",
       " 23632,\n",
       " 23668,\n",
       " 24083,\n",
       " 24598,\n",
       " 25110,\n",
       " 25896,\n",
       " 26198,\n",
       " 26868,\n",
       " 26898,\n",
       " 27012,\n",
       " 31055,\n",
       " 31298,\n",
       " 31853,\n",
       " 31989,\n",
       " 32191,\n",
       " 32391,\n",
       " 32834,\n",
       " 33061,\n",
       " 33141,\n",
       " 34123,\n",
       " 35355,\n",
       " 37756,\n",
       " 38934,\n",
       " 40458,\n",
       " 44752,\n",
       " 49120,\n",
       " 51183,\n",
       " 51480,\n",
       " 51605,\n",
       " 52543,\n",
       " 52692,\n",
       " 53118,\n",
       " 53411,\n",
       " 54155,\n",
       " 55952,\n",
       " 57730,\n",
       " 59402,\n",
       " 63563,\n",
       " 63579,\n",
       " 63694,\n",
       " 65342,\n",
       " 65361,\n",
       " 65398,\n",
       " 65588,\n",
       " 65766,\n",
       " 65849,\n",
       " 66156,\n",
       " 66941,\n",
       " 69892,\n",
       " 69964,\n",
       " 70069,\n",
       " 70530,\n",
       " 70532,\n",
       " 71277,\n",
       " 71629,\n",
       " 72723,\n",
       " 72772,\n",
       " 72821,\n",
       " 73117,\n",
       " 73148,\n",
       " 73175,\n",
       " 74076,\n",
       " 75206,\n",
       " 75447,\n",
       " 75942,\n",
       " 75950,\n",
       " 76041,\n",
       " 76276,\n",
       " 76765,\n",
       " 76771,\n",
       " 76835,\n",
       " 77828,\n",
       " 77867,\n",
       " 79039,\n",
       " 79062,\n",
       " 79085,\n",
       " 79240,\n",
       " 79252,\n",
       " 79271,\n",
       " 79322,\n",
       " 79368,\n",
       " 79635,\n",
       " 79694,\n",
       " 79698,\n",
       " 80890,\n",
       " 82236,\n",
       " 82414,\n",
       " 83549,\n",
       " 83580,\n",
       " 83585,\n",
       " 83610,\n",
       " 83778,\n",
       " 83804,\n",
       " 83811,\n",
       " 84258,\n",
       " 84755,\n",
       " 84756,\n",
       " 84853,\n",
       " 85634,\n",
       " 85786,\n",
       " 85814,\n",
       " 85827,\n",
       " 86223,\n",
       " 86725,\n",
       " 88538,\n",
       " 91281,\n",
       " 92398,\n",
       " 94542,\n",
       " 95701,\n",
       " 95864,\n",
       " 95886,\n",
       " 96028,\n",
       " 97164,\n",
       " 97728,\n",
       " 99632,\n",
       " 100100,\n",
       " 101135,\n",
       " 102198,\n",
       " 103706,\n",
       " 104347,\n",
       " 104394,\n",
       " 104765,\n",
       " 104778,\n",
       " 104866,\n",
       " 105123,\n",
       " 105196,\n",
       " 105204,\n",
       " 105213,\n",
       " 105984,\n",
       " 106320,\n",
       " 106939,\n",
       " 107536,\n",
       " 107638,\n",
       " 107864,\n",
       " 108131,\n",
       " 108522,\n",
       " 108692,\n",
       " 108705,\n",
       " 108864,\n",
       " 108881,\n",
       " 109488,\n",
       " 109492,\n",
       " 110153,\n",
       " 110311,\n",
       " 110548,\n",
       " 110581,\n",
       " 110688,\n",
       " 111293,\n",
       " 111456,\n",
       " 111690,\n",
       " 111741,\n",
       " 111902,\n",
       " 112063,\n",
       " 112589,\n",
       " 112954,\n",
       " 113045,\n",
       " 114280,\n",
       " 115149,\n",
       " 115473,\n",
       " 115505,\n",
       " 115771,\n",
       " 115988,\n",
       " 116106,\n",
       " 117145,\n",
       " 119824,\n",
       " 120712,\n",
       " 121022,\n",
       " 121606,\n",
       " 121680,\n",
       " 122098,\n",
       " 122566,\n",
       " 130840,\n",
       " 130849,\n",
       " 138569,\n",
       " 138579,\n",
       " 145096,\n",
       " 145106,\n",
       " 168231,\n",
       " 193034,\n",
       " 193099,\n",
       " 194577,\n",
       " 195378,\n",
       " 195691,\n",
       " 195748,\n",
       " 195818,\n",
       " 196249,\n",
       " 196812,\n",
       " 196879,\n",
       " 197717,\n",
       " 197860,\n",
       " 199025,\n",
       " 201702,\n",
       " 201951,\n",
       " 202240,\n",
       " 203126,\n",
       " 205414,\n",
       " 206672,\n",
       " 207164,\n",
       " 209117,\n",
       " 209737,\n",
       " 209895,\n",
       " 210554,\n",
       " 214415,\n",
       " 214683,\n",
       " 218258,\n",
       " 221970,\n",
       " 222897,\n",
       " 227426,\n",
       " 227863,\n",
       " 230838,\n",
       " 232572,\n",
       " 232814,\n",
       " 233728,\n",
       " 235101,\n",
       " 235196,\n",
       " 235619,\n",
       " 235953,\n",
       " 236036,\n",
       " 237025,\n",
       " 237122,\n",
       " 237324,\n",
       " 238250,\n",
       " 238516,\n",
       " 238803,\n",
       " 239902,\n",
       " 240070,\n",
       " 240222,\n",
       " 240291,\n",
       " 240298,\n",
       " 240397,\n",
       " 242433,\n",
       " 244402,\n",
       " 244560,\n",
       " 244732,\n",
       " 245012,\n",
       " 245276,\n",
       " 245719,\n",
       " 247617,\n",
       " 248107,\n",
       " 249129,\n",
       " 249399,\n",
       " 251209,\n",
       " 251275,\n",
       " 251777,\n",
       " 256583,\n",
       " 259122,\n",
       " 260978,\n",
       " 261070,\n",
       " 261328,\n",
       " 261460,\n",
       " 264584,\n",
       " 264602,\n",
       " 264640,\n",
       " 265005,\n",
       " 269024,\n",
       " 269141,\n",
       " 269246,\n",
       " 269289,\n",
       " 270108,\n",
       " 272144,\n",
       " 273615,\n",
       " 276280,\n",
       " 281125,\n",
       " 281375,\n",
       " 284139,\n",
       " 284229,\n",
       " 286001,\n",
       " 287714,\n",
       " 288038,\n",
       " 288578,\n",
       " 289067,\n",
       " 290006,\n",
       " 290517,\n",
       " 292250,\n",
       " 292485,\n",
       " 293012,\n",
       " 293266,\n",
       " 293635,\n",
       " 294145,\n",
       " 296560,\n",
       " 298149,\n",
       " 298690,\n",
       " 299132,\n",
       " 299829,\n",
       " 300213,\n",
       " 300230,\n",
       " 301388,\n",
       " 301470,\n",
       " 301895,\n",
       " 303650,\n",
       " 307257,\n",
       " 311150,\n",
       " 311390,\n",
       " 312527,\n",
       " 317036,\n",
       " 317957,\n",
       " 319249,\n",
       " 319414,\n",
       " 319454,\n",
       " 320064,\n",
       " 322924,\n",
       " 323245,\n",
       " 323907,\n",
       " 324620,\n",
       " 325020,\n",
       " 326587,\n",
       " 326924,\n",
       " 329914,\n",
       " 329999,\n",
       " 333945,\n",
       " 333977,\n",
       " 336333,\n",
       " 336376,\n",
       " 339370,\n",
       " 343915,\n",
       " 344172,\n",
       " 344191,\n",
       " 344253,\n",
       " 344303,\n",
       " 344494,\n",
       " 344997,\n",
       " 345106,\n",
       " 345819,\n",
       " 346085,\n",
       " 346104,\n",
       " 346297,\n",
       " 346505,\n",
       " 350758,\n",
       " 351789,\n",
       " 352610,\n",
       " 352632,\n",
       " 352956,\n",
       " 354059,\n",
       " 355344,\n",
       " 355957,\n",
       " 356104,\n",
       " 356409,\n",
       " 356494,\n",
       " 356503,\n",
       " 357526,\n",
       " 357527,\n",
       " 357956,\n",
       " 358946,\n",
       " 359072,\n",
       " 359163,\n",
       " 359454,\n",
       " 359778,\n",
       " 359948,\n",
       " 360194,\n",
       " 360282,\n",
       " 360286,\n",
       " 360775,\n",
       " 361051,\n",
       " 362505,\n",
       " 362506,\n",
       " 363209,\n",
       " 363694,\n",
       " 363725,\n",
       " 363755,\n",
       " 363997,\n",
       " 365838,\n",
       " 366786,\n",
       " 366791,\n",
       " 366817,\n",
       " 371381,\n",
       " 371465,\n",
       " 372867,\n",
       " 372892,\n",
       " 373445,\n",
       " 375337,\n",
       " 377156,\n",
       " 377213,\n",
       " 377214,\n",
       " 377385,\n",
       " 378772,\n",
       " 381244,\n",
       " 381525,\n",
       " 381663,\n",
       " 381671,\n",
       " 381795,\n",
       " 382740,\n",
       " 382838,\n",
       " 383348,\n",
       " 385241,\n",
       " 385841,\n",
       " 386530,\n",
       " 386606,\n",
       " 387264,\n",
       " 388415,\n",
       " 388994,\n",
       " 391598,\n",
       " 395568,\n",
       " 395688,\n",
       " 396605,\n",
       " 400034,\n",
       " 400706,\n",
       " 401174,\n",
       " 401389,\n",
       " 403478,\n",
       " 403859,\n",
       " 406490,\n",
       " 408813,\n",
       " 408900,\n",
       " 409010,\n",
       " 411743,\n",
       " 412266,\n",
       " 413950,\n",
       " 416057,\n",
       " 416211,\n",
       " 421905,\n",
       " 421907,\n",
       " 422620,\n",
       " 424488,\n",
       " 424575,\n",
       " 426876,\n",
       " 426933,\n",
       " 427461,\n",
       " 427686,\n",
       " 429296,\n",
       " 429717,\n",
       " 429724,\n",
       " 430115,\n",
       " 433269,\n",
       " 433731,\n",
       " 434279,\n",
       " 434323,\n",
       " 434348,\n",
       " 434575,\n",
       " 434652,\n",
       " 435060,\n",
       " 436097,\n",
       " 436186,\n",
       " 436634,\n",
       " 436852,\n",
       " 437410,\n",
       " 437663,\n",
       " 437748,\n",
       " 437873,\n",
       " 438512,\n",
       " 438923,\n",
       " 440721,\n",
       " 441872,\n",
       " 442316,\n",
       " 443114,\n",
       " 443774,\n",
       " 445407,\n",
       " 446021,\n",
       " 446589,\n",
       " 447503,\n",
       " 450276,\n",
       " 450536,\n",
       " 450681,\n",
       " 450783,\n",
       " 450844,\n",
       " 450854,\n",
       " 451136,\n",
       " 451238,\n",
       " 451279,\n",
       " 451281,\n",
       " 452249,\n",
       " 452888,\n",
       " 453735,\n",
       " 455247,\n",
       " 455793,\n",
       " 455981,\n",
       " 456437,\n",
       " 456778,\n",
       " 457676,\n",
       " 457831,\n",
       " 458541,\n",
       " 458709,\n",
       " 459126,\n",
       " 459331,\n",
       " 459436,\n",
       " 459785,\n",
       " 460199,\n",
       " 460256,\n",
       " 460301,\n",
       " 465703,\n",
       " 466580,\n",
       " 467299,\n",
       " 470090,\n",
       " 470627,\n",
       " 471264,\n",
       " 472993,\n",
       " 473942,\n",
       " 474109,\n",
       " 476102,\n",
       " 477493,\n",
       " 477504,\n",
       " 477862,\n",
       " 479744,\n",
       " 480338,\n",
       " 480340,\n",
       " 480963,\n",
       " 480983,\n",
       " 481290,\n",
       " 481879,\n",
       " 482275,\n",
       " 482430,\n",
       " 483302,\n",
       " 483319,\n",
       " 483331,\n",
       " 483551,\n",
       " 483647,\n",
       " 483788,\n",
       " 484813,\n",
       " 485154,\n",
       " 486359,\n",
       " 489048,\n",
       " 489647,\n",
       " 490365,\n",
       " 490922,\n",
       " 491184,\n",
       " 491328,\n",
       " 491845,\n",
       " 492493,\n",
       " 493139,\n",
       " 493324,\n",
       " 493424,\n",
       " 493494,\n",
       " 493912,\n",
       " 495028,\n",
       " 495573,\n",
       " 497408,\n",
       " 498517,\n",
       " 499106,\n",
       " 499915,\n",
       " 500756,\n",
       " 501444,\n",
       " 502813,\n",
       " 502904,\n",
       " 503013,\n",
       " 503015,\n",
       " 503200,\n",
       " 503284,\n",
       " 504289,\n",
       " 505204,\n",
       " 505229,\n",
       " 505982,\n",
       " 506976,\n",
       " 510205,\n",
       " 510277,\n",
       " 510352,\n",
       " 510373,\n",
       " 510802,\n",
       " 511065,\n",
       " 511454,\n",
       " 512180,\n",
       " 514388,\n",
       " 514586,\n",
       " 514693,\n",
       " 514814,\n",
       " 515019,\n",
       " 515138,\n",
       " 515394,\n",
       " 515790,\n",
       " 516211,\n",
       " 518571,\n",
       " 518717,\n",
       " 518796,\n",
       " 518835,\n",
       " 518846,\n",
       " 519168,\n",
       " 519186,\n",
       " 519188,\n",
       " 519278,\n",
       " 519797,\n",
       " 519845,\n",
       " 519917,\n",
       " 520598,\n",
       " 521421,\n",
       " 521865,\n",
       " 521975,\n",
       " 522298,\n",
       " 522306,\n",
       " 522319,\n",
       " 523493,\n",
       " 524039,\n",
       " 524126,\n",
       " 524473,\n",
       " 524606,\n",
       " 524834,\n",
       " 524959,\n",
       " 525016,\n",
       " 525335,\n",
       " 525369,\n",
       " 525917,\n",
       " 525921,\n",
       " 525927,\n",
       " 525958,\n",
       " 526023,\n",
       " 526231,\n",
       " 526333,\n",
       " 526661,\n",
       " 526740,\n",
       " 527059,\n",
       " 527069,\n",
       " 527165,\n",
       " 527167,\n",
       " 527675,\n",
       " 528385,\n",
       " 528444,\n",
       " 528653,\n",
       " 528890,\n",
       " 528925,\n",
       " 528937,\n",
       " 529912,\n",
       " 530454,\n",
       " 530561,\n",
       " 531025,\n",
       " 531128,\n",
       " 531157,\n",
       " 531251,\n",
       " 531311,\n",
       " 531391,\n",
       " 531425,\n",
       " 531477,\n",
       " 531511,\n",
       " 531638,\n",
       " 531779,\n",
       " 531810,\n",
       " 531913,\n",
       " 531977,\n",
       " 532551,\n",
       " 532593,\n",
       " 532712,\n",
       " 533305,\n",
       " 533333,\n",
       " 533404,\n",
       " 533467,\n",
       " 533508,\n",
       " 533739,\n",
       " 535636,\n",
       " 536664,\n",
       " 536874,\n",
       " 538772,\n",
       " 539158,\n",
       " 547226,\n",
       " 548677,\n",
       " 551040,\n",
       " 551881,\n",
       " 552214,\n",
       " 552819,\n",
       " 553089,\n",
       " 553356,\n",
       " 554198,\n",
       " 556615,\n",
       " 559141,\n",
       " 559806,\n",
       " 559913,\n",
       " 562542,\n",
       " 563758,\n",
       " 564292,\n",
       " 564341,\n",
       " 564474,\n",
       " 564778,\n",
       " 565158,\n",
       " 565686,\n",
       " 565693,\n",
       " 566661,\n",
       " 566663,\n",
       " 567006,\n",
       " 567130,\n",
       " 567475,\n",
       " 569525,\n",
       " 569741,\n",
       " 569760,\n",
       " 571378,\n",
       " 572388,\n",
       " 574060,\n",
       " 574553,\n",
       " 574842,\n",
       " 575345,\n",
       " 575394,\n",
       " 575677,\n",
       " 576238,\n",
       " 577073,\n",
       " 577842,\n",
       " 579557,\n",
       " 581817,\n",
       " 582106,\n",
       " 582455,\n",
       " 582528,\n",
       " 583319,\n",
       " 587203,\n",
       " 588785,\n",
       " 589025,\n",
       " 589053,\n",
       " 589061,\n",
       " 589624,\n",
       " 590120,\n",
       " 592041,\n",
       " 592115,\n",
       " 592635,\n",
       " 593259,\n",
       " 594783,\n",
       " 594899,\n",
       " 595150,\n",
       " 595267,\n",
       " 595312,\n",
       " 595434,\n",
       " 595497,\n",
       " 595642,\n",
       " 597484,\n",
       " 597911,\n",
       " 598644,\n",
       " 601681,\n",
       " 602334,\n",
       " 603062,\n",
       " 604761,\n",
       " 606778,\n",
       " 607346,\n",
       " 607629,\n",
       " 607959,\n",
       " 609363,\n",
       " 609442,\n",
       " 609663,\n",
       " 609668,\n",
       " 610115,\n",
       " 610125,\n",
       " 610311,\n",
       " 610894,\n",
       " 611657,\n",
       " 611753,\n",
       " 613577,\n",
       " 613649,\n",
       " 613818,\n",
       " 613873,\n",
       " 614174,\n",
       " 614323,\n",
       " 615505,\n",
       " 616896,\n",
       " 617192,\n",
       " 617204,\n",
       " 617211,\n",
       " 617353,\n",
       " 617401,\n",
       " 618532,\n",
       " 618596,\n",
       " 619089,\n",
       " 619946,\n",
       " 620616,\n",
       " 620684,\n",
       " 621183,\n",
       " 621664,\n",
       " 622262,\n",
       " 622456,\n",
       " 623061,\n",
       " 623295,\n",
       " 623306,\n",
       " 623323,\n",
       " 623590,\n",
       " 623884,\n",
       " 623889,\n",
       " 624019,\n",
       " 624094,\n",
       " 624123,\n",
       " 624138,\n",
       " 624878,\n",
       " 624993,\n",
       " 625683,\n",
       " 626817,\n",
       " 626858,\n",
       " 627150,\n",
       " 627151,\n",
       " 627529,\n",
       " 627631,\n",
       " 628085,\n",
       " 628425,\n",
       " 628591,\n",
       " 628627,\n",
       " 628651,\n",
       " 628718,\n",
       " 628766,\n",
       " 628795,\n",
       " 628808,\n",
       " 629742,\n",
       " 630113,\n",
       " 630920,\n",
       " 632241,\n",
       " 633792,\n",
       " 633843,\n",
       " 633853,\n",
       " 633918,\n",
       " 634154,\n",
       " 634435,\n",
       " 634456,\n",
       " 634609,\n",
       " 634695,\n",
       " 634726,\n",
       " 635048,\n",
       " 635367,\n",
       " 635478,\n",
       " 635571,\n",
       " 635772,\n",
       " 635774,\n",
       " 635814,\n",
       " 636327,\n",
       " 636355,\n",
       " 636363,\n",
       " 636416,\n",
       " 636438,\n",
       " 636446,\n",
       " 636447,\n",
       " 636454,\n",
       " 636509,\n",
       " 636510,\n",
       " 636523,\n",
       " 636598,\n",
       " 636611,\n",
       " 636715,\n",
       " 637044,\n",
       " 637106,\n",
       " 637123,\n",
       " 637192,\n",
       " 637467,\n",
       " 637593,\n",
       " 637651,\n",
       " 637778,\n",
       " 638620,\n",
       " 638676,\n",
       " 638814,\n",
       " 639099,\n",
       " 639174,\n",
       " 639235,\n",
       " 639289,\n",
       " 639424,\n",
       " 639437,\n",
       " 639595,\n",
       " 640425,\n",
       " 640581,\n",
       " 640668,\n",
       " 640717,\n",
       " 640733,\n",
       " 640763,\n",
       " 640845,\n",
       " 640993,\n",
       " 641091,\n",
       " 641276,\n",
       " 641377,\n",
       " 641400,\n",
       " 641407,\n",
       " 641413,\n",
       " 641452,\n",
       " 641581,\n",
       " 641903,\n",
       " 642014,\n",
       " 642060,\n",
       " 642062,\n",
       " 642157,\n",
       " 642212,\n",
       " 642224,\n",
       " 642245,\n",
       " 642273,\n",
       " 642349,\n",
       " 642491,\n",
       " 642602,\n",
       " 642643,\n",
       " 642796,\n",
       " 642864,\n",
       " 642883,\n",
       " 642976,\n",
       " 642993,\n",
       " 643720,\n",
       " 644833,\n",
       " 644981,\n",
       " 645080,\n",
       " 646360,\n",
       " 646440,\n",
       " 648864,\n",
       " 650002,\n",
       " 652799,\n",
       " 655904,\n",
       " 656460,\n",
       " 656930,\n",
       " 657245,\n",
       " 658006,\n",
       " 658083,\n",
       " 661311,\n",
       " 662876,\n",
       " 663560,\n",
       " 664246,\n",
       " 664313,\n",
       " 665659,\n",
       " 667118,\n",
       " 668442,\n",
       " 668977,\n",
       " 671146,\n",
       " 674481,\n",
       " 676250,\n",
       " 677564,\n",
       " 678770,\n",
       " 679216,\n",
       " 679285,\n",
       " 679764,\n",
       " 680152,\n",
       " 680684,\n",
       " 681495,\n",
       " 683283,\n",
       " 683614,\n",
       " 684891,\n",
       " 685543,\n",
       " 685882,\n",
       " 686367,\n",
       " 686372,\n",
       " 687449,\n",
       " 687480,\n",
       " 687809,\n",
       " 687843,\n",
       " 687998,\n",
       " 688524,\n",
       " 688703,\n",
       " 688705,\n",
       " 688731,\n",
       " 688743,\n",
       " 688857,\n",
       " 689061,\n",
       " 689140,\n",
       " 689142,\n",
       " 689229,\n",
       " 689267,\n",
       " 689276,\n",
       " 689319,\n",
       " 689348,\n",
       " 689359,\n",
       " 689367,\n",
       " 689368,\n",
       " 689379,\n",
       " 689392,\n",
       " 689401,\n",
       " 689458,\n",
       " 689534,\n",
       " 689591,\n",
       " 689626,\n",
       " 689629,\n",
       " 689759,\n",
       " 689813,\n",
       " 689838]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The multiprotein complex y-secretase proteolytically cleaves the intramembrane region of amyloid precursorprotein (APP), which in turn forms the plaques found in Alzheimer's disease (AD) patients. The catalyticcomponent of Y-secretase is the intramembrane aspartyl protease (IAP) called presenilin. Mutations inpresenilin are directly linked to familial early-onset AD. Another known member of the IAP family is signalpeptide peptidase (SPP), which functions to further proteolyze remnant signal peptides after they have beencleaved by signal peptidase. Knowledge of the biochemistry and function of individual SPPs are onlybeginning to be elucidated, and homologues are found in all kingdoms of life. Presenilin and SPP exhibitsignificant sequence similarity, strongly suggesting they share structural and catalytic features. Thus, amolecular understanding of the more tractable SPP will likely impact drug design for presenilin and y-secretase. The goal of this proposal is to express, characterize, and solve the crystal structure of anextremophilic bacterial SPP ortholog by itself, with a transition-state analog inhibitor and with a substratemimic. In addition, drug candidates will be screened in silico. This first structure of an intramembraneprotease will provide critical insight into the biochemistry of intramembrane proteolysis and enable structure-based AD drug development and screening.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ABSTRACT'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE QUERY WORDS HERE\n",
    "\n",
    "query_words = ['intelligence_ai']\n",
    "'''\n",
    "['artificial_intelligence', 'artificial_intelligence_ai', \n",
    "               'artificial_intelligence_machine_learning', 'artificialintelligence', 'artificially_intelligent',\n",
    "               'artificial_intelligence_aaai', 'artificial_intelligence_ijcai', 'artificialintelligence_ai',\n",
    "               'artificialintelligent'\n",
    "              ]\n",
    "'''\n",
    "\n",
    "#'ai', 'artificial', 'intelligence' \n",
    "        \n",
    "q = create_query(query_words, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the score for each document against the query. Docs with more occurences of the query words \n",
    "# will score higher\n",
    "\n",
    "f_scores = doc_term_matrix.dot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(f_scores >0)  # how many abstracts include at least one of the query words\n",
    "\n",
    "# some are being left off from raw counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "f_scores_sorted = np.sort(f_scores)[::-1]\n",
    "f_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[245852 513841 652799 451621 239767 446589 298690]\n"
     ]
    }
   ],
   "source": [
    "f_idx, f_top_abstracts = return_top_abstracts(docs, f_scores, -1)  # CHANGE NUMBER OF TOP DOCS RETURNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245852    need 2200 americans die cardiovascular cvd day...\n",
       "513841    online eyewire prove volunteer motivate recons...\n",
       "652799    small_business_innovation sbir image processin...\n",
       "451621    partner socially_assistive_robot sar person_de...\n",
       "239767    diabete chronic presently prevent cure treat i...\n",
       "446589    perceptual assessment hypernasality consider s...\n",
       "298690    visual question answer vqa empower people answ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_scores_sorted[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The goal of a visual question answering (VQA) system is to empower people to find the answer to any question about any image. For example, a VQA system could enable blind people to address daily visual challenges such as learning whether a pair of socks match or learning what type of food is in a can. VQA services could also facilitate the creation of smarter environments, say to monitor how many defective products are on a factory assembly line at any given time. A limitation of existing VQA systems is that they do not account for the fact that a visual question may elicit different answers from different people. VQA systems could save time and reduce user frustration if they empowered users to anticipate and resolve any answer disagreements that may arise. Blind and sighted people could more rapidly and accurately learn about the diversity of human perspectives on the visual world. VQA services also could teach people how to ask visual questions that elicit the desired answer diversity.This project will create artificial intelligence (AI) models that can account for the possible diversity of answers inherent in crowd intelligence. Specifically, AI models will be designed to predict when, why, and how human answer disagreement occurs, which in turn will enable new designs for human-computer partnerships. This is challenging because it necessitates designing frameworks that simultaneously model and synthesize different and potentially conflicting perceptions of images and language for the many possible causes of disagreement. To ensure that the AI models generalize across a broad range of applications, an existing corpus of over one million visual questions asked by blind and sighted people will be used to create annotated datasets that indicate when, why, and how much answer disagreement arises. Methods will then be developed for automatically predicting directly from a visual question how much answer diversity will arise from a crowd, and why disagreement arises when it does. Finally, a system will be designed for guiding visually-impaired users to more quickly formulate visual questions so they can receive a single, unambiguous crowd response (e.g., guide the person to better frame the visual content of interest with a mobile phone camera). User studies with blind users will be conducted to empirically test the efficacy of the new system, with a focus on uncovering human-based issues in real-world, real-time situations.This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# others - artificial intelligence typo\n",
    "# 245852, 451621: analytic, augmented intelligence\n",
    "\n",
    "df['ABSTRACT'][298690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visual question answer vqa empower people answer question image example vqa enable blind people daily visual challenge learn pair sock match learn food vqa facilitate creation smarter environment monitor defective product factory assembly line limitation exist vqa account fact visual question elicit answer people vqa save user frustration empower user anticipate resolve answer disagreement arise blind_sight people rapidly accurately learn diversity human perspective visual world vqa teach people ask visual question elicit desire answer diversity create artificial_intelligence_ai account possible diversity answer inherent crowd intelligence_ai predict human answer disagreement occur turn enable human computer partnership challenging necessitate framework simultaneously synthesize potentially conflict perception image language possible disagreement ensure ai generalize broad range exist corpus million visual question ask blind_sight people create annotate dataset indicate answer disagreement arise automatically predict directly visual question answer diversity arise crowd disagreement arise guide visually_impaired user quickly formulate visual question receive single unambiguous crowd eg guide person better frame visual content interest mobile_phone camera user blind user empirically test efficacy uncover human issue real world real situation'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_top_abstracts.iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare idx and f_idx\n",
    "\n",
    "s_idx = set(idx)\n",
    "sf_idx = set(f_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{110153,\n",
       " 201951,\n",
       " 261460,\n",
       " 299829,\n",
       " 326924,\n",
       " 359163,\n",
       " 446589,\n",
       " 459436,\n",
       " 525369,\n",
       " 614323,\n",
       " 639595,\n",
       " 652799}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = s_idx.difference(sf_idx)\n",
    "print(len(t))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{22760, 265721, 427170, 464665, 622143, 641499, 688120, 689487}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = sf_idx.difference(s_idx)\n",
    "print(len(t2))\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.iloc[446589,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptual assessment of hypernasality is considered a critical component when evaluating the speech ofchildren with cleft lip and/or palate (CLP). However, most speech-language pathologists (SLPs) do not receiveformal training for perceptual evaluation of speech and, as a result, research shows that the subjective ratingsare inherently biased to the perceiver and exhibit considerable variability. In this project, we aim to develop anartificial intelligence (AI) algorithm that automatically evaluates speech along four dimensions deemed to becritically important by the Americleft Speech Outcomes Group (ASOG), namely speech acceptability,articulation, hypernasality, and audible nasal emissions. The AI algorithm in this project is based on an existingdatabase of speech collected as a part of an NIH-funded project to develop reliable speech outcomes byimproving the reliability of perceptual ratings by training clinicians (NIDCR DE019-01235, PI: Kathy Chapman).This database contains speech samples from 125 5-7 year olds along with multiple perceptual rating for eachspeech sample. The clinicians participating in this study were successfully trained using a new protocol fromthe Americleft Speech Outcomes Group and they exhibit excellent inter-clinician reliability. In SA1 we will develop an AI algorithm that automatically learns the relationship between acomprehensive set of speech acoustics and the average of the ASOG-trained expert ratings for each of thefour perceptual dimensions. This approach is based on technology that the PIs have successfully used toevaluate dysarthric speech. Unique to these algorithms is modeling of perceptual judgments of trained expertsusing tools from statistical signal processing and AI. The output of the algorithms will map to a clinically-relevant scale, rather than to norm-referenced values that may or may not be meaningful. In SA2, we willevaluate the tool on new data by collecting new speech samples using a mobile app at a partner clinic usingthe same protocol as in the original study. Every collected sample will be further evaluated by ASOG trainedclinicians. We will use this data to evaluate the accuracy of the AI model by comparing the model's predictionswith the average of ASOG-trained experts. Preliminary results show promise that the proposed approach willyield a successful tool for accurately characterizing perceptual dimensions in the speech of children with CLP.These results indicate that a number of acoustic features that have been developed previously by the PIsaccurately capture differences in hypernasality and articulation between the speech of three children with CLP(with varying severity). Furthermore, we show the success of our approach on a different, but related, task:objective evaluation of dysarthric speech. We show that an algorithm that automatically rates hypernasalityperforms on par with the judgment of human evaluators. The results of the proposed research will form thebasis for a subsequent R01 proposal for the development and evaluation of a clinical tool to objectivelyquantify and track speech production in children with CLP.\n",
      "['perceptual', 'assessment', 'hypernasality', 'consider', 'speech', 'ofchild', 'cleft_lip_palate_clp', 'speech', 'language_pathologist_slp', 'receiveformal', 'perceptual', 'evaluation', 'speech', 'subjective', 'ratingsare', 'inherently', 'biased', 'perceiver', 'exhibit', 'considerable', 'variability', 'anartificial', 'intelligence_ai', 'algorithm', 'automatically', 'speech', 'dimension', 'deem', 'becritically', 'americleft', 'speech', 'outcomes', 'asog', 'speech', 'acceptability', 'articulation', 'hypernasality', 'audible', 'nasal', 'emission', 'ai', 'algorithm', 'existingdatabase', 'speech', 'collect', 'nih', 'fund', 'reliable', 'speech', 'byimprove', 'reliability', 'perceptual', 'rating', 'clinician', 'nidcr', 'de019', 'pi', 'kathy', 'chapman', 'database', 'contain', 'speech', 'old', 'perceptual', 'rating', 'eachspeech', 'clinician', 'participate', 'successfully', 'train', 'protocol', 'americleft', 'speech', 'outcomes', 'exhibit', 'excellent', 'inter_clinician', 'reliability', 'sa1', 'ai', 'algorithm', 'automatically', 'learn', 'relationship', 'acomprehensive', 'set', 'speech', 'acoustic', 'average', 'asog', 'train', 'expert', 'rating', 'perceptual', 'dimension', 'technology', 'pis', 'successfully', 'toevaluate', 'dysarthric_speech', 'unique', 'algorithm', 'modeling', 'perceptual_judgment', 'train', 'expertsusing', 'statistical', 'processing', 'ai', 'output', 'algorithm', 'map', 'clinically', 'relevant', 'scale', 'norm', 'reference', 'value', 'meaningful', 'sa2', 'willevaluate', 'collect', 'speech', 'mobile_app', 'partner', 'clinic', 'usingthe', 'protocol', 'original', 'collect', 'asog', 'trainedclinician', 'accuracy', 'ai', 'predictionswith', 'average', 'asog', 'train', 'expert', 'preliminary', 'promise', 'willyield', 'successful', 'accurately', 'perceptual', 'dimension', 'speech', 'child', 'clp', 'indicate', 'acoustic', 'feature', 'previously', 'pisaccurately', 'capture', 'difference', 'hypernasality', 'articulation', 'speech', 'child', 'clp', 'vary', 'severity', 'success', 'related', 'task', 'evaluation', 'dysarthric_speech', 'algorithm', 'automatically', 'hypernasalityperform', 'par', 'judgment', 'human', 'evaluator', 'thebasis', 'subsequent', 'r01', 'evaluation', 'clinical', 'objectivelyquantify', 'track', 'speech', 'production', 'child', 'clp']\n"
     ]
    }
   ],
   "source": [
    "print(temp['ABSTRACT'])\n",
    "print(temp[\"final_frqwds_removed\"])\n",
    "\n",
    "# 20611, 23632: raw had Artificial Intelligence -- which didn't match with \"artificial intelligence\"\n",
    "#265721-622143 has \"artificial-intelligence\" in raw, not \"artificial intelligence\"\n",
    "#641499 has a strange character for space in raw\n",
    "#689487 has \"artificially-intelligent\" in raw text\n",
    "\n",
    "# tokenize raw text - lowercase, this will take care of hyphens or other strange symbols\n",
    "\n",
    "#2940, 13746: has token artificial_intelligence_ai not \"artificial_intelligence\"\n",
    "#689813: has token \"artificial_intelligence_machine_learning\"\n",
    "\n",
    "#110153, 446589: typo with \"anartificial intelligence\" in raw\n",
    "#201951, 261460: artificial intelligence removed from raw because it was part of the title -- need to undo this and add titles in\n",
    "\n",
    "\n",
    "# need to look at list of tokens!\n",
    "# check token 'intelligence_ai' - 7 docs have this, some are typos, \n",
    "#     others will be picked up by artificial_intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                                    261466\n",
       "original index                                                           302297\n",
       "PROJECT_ID                                                               131540\n",
       "ABSTRACT                      This award will help to subsidize the particip...\n",
       "FY                                                                         2010\n",
       "PROJECT_TERMS                 Artificial Intelligence; Award; Commit; Educat...\n",
       "PROJECT_TITLE                 THE FOURTH NORTHEAST STUDENT COLLOQUIUM ON ART...\n",
       "DEPARTMENT                                                                  NSF\n",
       "AGENCY                                                                      NSF\n",
       "IC_CENTER                                                                   NaN\n",
       "PROJECT_NUMBER                                                          1036017\n",
       "PROJECT_START_DATE                                                    5/15/2010\n",
       "PROJECT_END_DATE                                                      4/30/2011\n",
       "CONTACT_PI_PROJECT_LEADER                                    MCCALLUM, ANDREW K\n",
       "OTHER_PIS                                                  LEARNED-MILLER, ERIK\n",
       "CONGRESSIONAL_DISTRICT                                                       01\n",
       "DUNS_NUMBER                                                           153926712\n",
       "ORGANIZATION_NAME                           UNIVERSITY OF MASSACHUSETTS AMHERST\n",
       "ORGANIZATION_CITY                                                       AMHERST\n",
       "ORGANIZATION_STATE                                                           MA\n",
       "ORGANIZATION_ZIP                                                     01003-9242\n",
       "ORGANIZATION_COUNTRY                                              UNITED STATES\n",
       "BUDGET_START_DATE                                                           NaN\n",
       "BUDGET_END_DATE                                                             NaN\n",
       "CFDA_CODE                                                                47.070\n",
       "FY.y                                                                       2010\n",
       "FY_TOTAL_COST                                                             16181\n",
       "FY_TOTAL_COST_SUB_PROJECTS                                                  NaN\n",
       "ORG_COUNT                                                                     1\n",
       "PI_COUNT                                                                      1\n",
       "working_abstract              This award will help to subsidize the particip...\n",
       "Start_Char                                                                    T\n",
       "nchar                                                                       917\n",
       "LAST_CHAR                                                                     y\n",
       "lemma_abstract                [award, help, subsidize, participation, gradua...\n",
       "clean_lemmas                  [award, help, subsidize, participation, gradua...\n",
       "stopwds_removed               [award, help, subsidize, participation, gradua...\n",
       "n_grams_added                 [award, help, subsidize, participation, gradua...\n",
       "final_tokens                  [award, help, subsidize, participation, gradua...\n",
       "final_frqwds_removed          [award, subsidize, participation, graduate, st...\n",
       "Name: 261460, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_' < \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intelligencecapability',\n",
       " 'intelligenceframework',\n",
       " 'intelligenceoutperform',\n",
       " 'intelligences',\n",
       " 'intelligencesimulation',\n",
       " 'intelligent',\n",
       " 'intelligent_robots_systems_iros',\n",
       " 'intelligent_tire',\n",
       " 'intelligent_transportation',\n",
       " 'intelligent_transportation_systems',\n",
       " 'intelligent_tutor',\n",
       " 'intelligent_tutoring',\n",
       " 'intelligent_tutoring_systems',\n",
       " 'intelligentcognitive',\n",
       " 'intelligentdimension',\n",
       " 'intelligentdisplay',\n",
       " 'intelligentelectrochemical',\n",
       " 'intelligenthearing',\n",
       " 'intelligently',\n",
       " 'intelligentlyanalyze',\n",
       " 'intelligentlydesigned',\n",
       " 'intelligentlyengineer',\n",
       " 'intelligentpatient',\n",
       " 'intelligentreminders',\n",
       " 'intelligentresearch',\n",
       " 'intelligentrobotics',\n",
       " 'intelligentsensor',\n",
       " 'intelligentsepsis',\n",
       " 'intelligentshared',\n",
       " 'intelligentsia',\n",
       " 'intelligentsystem',\n",
       " 'intelligenttechnological',\n",
       " 'intelligenttherapeutic',\n",
       " 'intelligenttherapy',\n",
       " 'intelligenttransportation',\n",
       " 'intelligenttutoring',\n",
       " 'intelligentvehicle',\n",
       " 'intelligenza',\n",
       " 'intelligibiity',\n",
       " 'intelligibility',\n",
       " 'intelligibility_naturalness',\n",
       " 'intelligibility_speech',\n",
       " 'intelligibilitychange',\n",
       " 'intelligibilitydifticulty',\n",
       " 'intelligibilityimprovements',\n",
       " 'intelligibilityin',\n",
       " 'intelligibilityto',\n",
       " 'intelligible',\n",
       " 'intelligible_speech',\n",
       " 'intelligibly',\n",
       " 'intelligiblyavailable',\n",
       " 'intellihealth',\n",
       " 'intellisense',\n",
       " 'intellistretch',\n",
       " 'intellitence',\n",
       " 'intellius',\n",
       " 'intelliwheels',\n",
       " 'intelluride',\n",
       " 'intelmed',\n",
       " 'intelnship',\n",
       " 'intelomerase',\n",
       " 'intelomere',\n",
       " 'intels',\n",
       " 'intelymec',\n",
       " 'intem',\n",
       " 'intemafional',\n",
       " 'intemal',\n",
       " 'intemal_advisory_committee',\n",
       " 'intemal_extemal',\n",
       " 'intemal_external_advisory',\n",
       " 'intemaladvisor',\n",
       " 'intemaladvisory',\n",
       " 'intemali',\n",
       " 'intemalization',\n",
       " 'intemalize',\n",
       " 'intemalreflection',\n",
       " 'intemationai',\n",
       " 'intemational',\n",
       " 'intemationalclinical',\n",
       " 'intemationally',\n",
       " 'intemediate',\n",
       " 'intemet',\n",
       " 'intemeuron',\n",
       " 'intemnediate',\n",
       " 'intemperate',\n",
       " 'intemperature',\n",
       " 'intemporal',\n",
       " 'intemporolimbic',\n",
       " 'intemship',\n",
       " 'inten',\n",
       " 'inten_enfion',\n",
       " 'inten_ention',\n",
       " 'inten_iscipiinary',\n",
       " 'inten_tionally',\n",
       " 'intenaali7ation',\n",
       " 'intenationally',\n",
       " 'intend',\n",
       " 'intend_autologous_reinfusion',\n",
       " 'intende',\n",
       " 'intendeda']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[587800:587900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artificial_intelligence',\n",
    "# 'artificial_intelligence_ai',\n",
    "# 'artificial_intelligence_machine_learning',\n",
    "# artificialintelligence\n",
    "# artificially_intelligent\n",
    "\n",
    "# 'artificial_intelligence_aaai',\n",
    "# 'artificial_intelligence_ijcai',\n",
    "# 'artificialintelligence_ai',\n",
    "# 'artificialintelligent',\n",
    "\n",
    "#'intelligence_ai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Document-Term Matrix\n",
    "\n",
    "This approach is similar to Literal Term Matching using frequency counts in the document-term matrix.  However, instead of using frequency counts, the entries of the document-term matrix are weighted using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/software/standard/compiler/gcc/9.2.0/jupyter_conda/2019.10-py3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Find doc-term matrix using TF-IDF weighting\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(lowercase=True, stop_words=stop_words, min_df=20)\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_terms = tf_idf_vectorizer.get_feature_names()  # these terms are the same as the terms created from the \n",
    "                                                      # frequency count document-term matrix, so we do not need to\n",
    "                                                      # recreate the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_terms == terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the score for each document against the query. Docs with more occurences of the query words \n",
    "# will score higher\n",
    "\n",
    "tf_idf_scores = tf_idf.dot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tf_idf_scores >0)   # how many abstracts include at least one of the query words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54210581, 0.53685946, 0.40581146, 0.39018629, 0.37190372,\n",
       "       0.34989933, 0.34834546, 0.34467873, 0.33076975, 0.31210131])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "tf_idf_scores_sorted = np.sort(tf_idf_scores)[::-1]\n",
    "tf_idf_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[293012  10300 493912 235101 581817  51183 531638 292485 378772 202240]\n"
     ]
    }
   ],
   "source": [
    "tfidf_idx, tfidf_top_abstracts = return_top_abstracts(docs, tf_idf_scores, -1)  # CHANGE NUMBER OF TOP DOCS RETURNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293012    grant participation undergraduate student hold...\n",
       "10300     graduate student attend workshop organize conj...\n",
       "493912    unique interdisciplinary team computer scienti...\n",
       "235101    phd student artificial_intelligence opportunit...\n",
       "581817    live maintain dimensional shape embryonic poor...\n",
       "                                ...                        \n",
       "490365    intellectual_merit self assembly individual ar...\n",
       "636454    sbir ii create scalable virtual learning assis...\n",
       "622262    anxiety disorder common psychiatric disorder y...\n",
       "528444    human brain currently powerful processor man r...\n",
       "498517    human infant confront world fill ambiguity fea...\n",
       "Length: 500, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anxiety disorder common psychiatric disorder youth lifetime prevalence range general population anxiety disorder social anxiety disorder sad youth short impairment likelihood substance_abuse limited academic achievement attenuate occupational impaired miss social relationship emerge social skill formal peer generalization session homework_assignment efficacy element skill generalization element peer generalization homework_assignment difficult implement traditional clinical setting limit optimal dissemination youth need setting eg school outpatient facility recenly complete sttr validate interactive virtual environment ve solve need intensive behavioral practice opportunity skill generalization ve pegasys vr intensive practice social skill need formal peer clinic solution intensive parental involvement home solution indicate implement ve environment set accessible credible feasible parent clinician child participate examination indicate statistically improvement sad symptom success need automation acceptance adoption clinician socially anxiety youth ii sttr incorporate artificial_intelligence natural language ves clinic practice expand homework solution opportunity reinforce skill acquisition generalization practice vivo setting game theory technology randomize youth age test pegasys vr traditional behavioral youth social anxiety disorder maintain month pegasys vr clinically efficacious offer sustainable cost__effective easily rapidy disseminate'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_top_abstracts.iloc[497]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing (LSI) Approach\n",
    "\n",
    "LSI Uses the TF-IDF matrix.  LSI is a tecnique that utilizes a truncated Singular Value Decomposition of the document-term matrix.  Basically, LSI still returns relevant documents to the query; however some of the documents returned may not include the exact search terms!  LSI is finding the latent or hidden relationships in the terms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Truncated SVD of the TF-IDF matrix\n",
    "\n",
    "lsa = TruncatedSVD(n_components=500, random_state=1)  # CHANGE THE NUMBER OF COMPONENTS - NOTE: MORE COMPONENTS \n",
    "                                                      # GIVES YOU A MORE ACCURATE APPROXIMATION OF THE DOC-TERM \n",
    "                                                      # MATRIX, BUT IS ALSO MORE EXPENSIVE AND MAY NOT LEAD TO THE \n",
    "                                                      # BEST INFO RETRIEVAL RESULTS.\n",
    "USigma = lsa.fit_transform(tf_idf)\n",
    "Vtrans = lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform query to be in same space as documents\n",
    "\n",
    "q = q.reshape(1,-1)\n",
    "qhat = lsa.transform(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500)\n",
      "(690814, 500)\n",
      "(500, 93578)\n"
     ]
    }
   ],
   "source": [
    "print(qhat.shape)\n",
    "print(USigma.shape)\n",
    "print(Vtrans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_scores = pairwise_distances(qhat, USigma, metric='cosine', n_jobs=20)  # CHANGE N_JOBS TO BE NUMBER OF CORES - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 690814)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99133602, 0.98604572, 0.9832503 , ..., 0.95342406, 0.97755251,\n",
       "        1.00443007]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99133602, 0.98604572, 0.9832503 , ..., 0.95342406, 0.97755251,\n",
       "       1.00443007])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690814"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lsa_scores[0] > 0)  # how many abstracts scored above 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.14581175, 1.13709072, 1.13687709, 1.13399649, 1.13048978,\n",
       "       1.12849743, 1.12779594, 1.12625484, 1.12600067, 1.12432865])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort scores in descending order\n",
    "\n",
    "lsa_scores_sorted = np.sort(lsa_scores[0])[::-1]\n",
    "lsa_scores_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[521029 423786 306306 689195 286505 541903 290054 485773 156876 421426]\n"
     ]
    }
   ],
   "source": [
    "lsa_idx, lsa_top_abstracts = return_top_abstracts(docs, lsa_scores[0], 500)  # CHANGE NUMBER OF TOP DOCS RETURNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521029    background hivcontinuestobeapressingpublicheal...\n",
       "423786    award pi postdoctoral supervision dr meers opp...\n",
       "306306    wave instability neutral dynamo windy windy wa...\n",
       "689195    math anxiety disproportionately feel woman wom...\n",
       "286505    evidence practice ebps increasingly implement ...\n",
       "                                ...                        \n",
       "350515    administrative adm strong consistent scientifi...\n",
       "345974    administrative adm strong consistent scientifi...\n",
       "601476    listener combine auditory spatial binaural_cue...\n",
       "490137    biomedical infrastructure biomedical health he...\n",
       "107885    barrier limit exchange nutrient organism water...\n",
       "Length: 500, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_top_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'purpose qualitative quantitative safety_tolerability arikace placebo'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_top_abstracts.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pandemics corpus\n",
    "\n",
    "We use the results of our three information retrieval techniques to create a new, smaller corpus that only contains abstracts relevant to the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ix = np.concatenate([f_idx, idx]) #tfidf_idx]) #, lsa_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_idx = np.unique(docs_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983,)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lim_docs = [tokens[i] for i in docs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create case-study corpuses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_corpus = df.loc[docs_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983, 40)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ai_corpus.to_pickle(\"./ai_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_docs = ai_corpus[\"final_frqwds_removed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needed for LDA, NMF (all from Scikit-Learn) is one string per document (not a list of strings)\n",
    "\n",
    "text = []\n",
    "\n",
    "for token_list in lim_docs:\n",
    "    text.append(\" \".join(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lim_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with relevant pandemic abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function slightly modified from https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):  # loop through each row of H.  idx = row index.  topic = actual row\n",
    "        print(\"\\nTopic %d:\" % (idx))\n",
    "        #print([(vectorizer.get_feature_names()[i], topic[i])  # printing out words corresponding to indices found in next line\n",
    "                        #for i in topic.argsort()[:-top_n - 1:-1]])  # finding indices of top words in topic\n",
    "            \n",
    "        print_list = [(vectorizer.get_feature_names()[i], topic[i])  \n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for item in print_list:\n",
    "            print(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF document-term matrix for the pandemics corpus \n",
    "\n",
    "# TRY DIFFERENT PARAMETERS IN THE TF-IDF DOC-TERM MATRIX SET-UP\n",
    "nmf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=3, lowercase=True) #, max_features=int(len(lim_docs)/2))\n",
    "\n",
    "nmf_tf_idf = nmf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983, 4575)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling with NMF\n",
    "\n",
    "nmf_model = NMF(n_components=30, random_state=1)  # TRY DIFFERENT NUMBERS OF TOPICS\n",
    "W = nmf_model.fit_transform(nmf_tf_idf)\n",
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "('student', 4.335909739666965)\n",
      "('teacher', 1.509393141768934)\n",
      "('assessment', 0.9903665447854408)\n",
      "('reu', 0.9338575933603623)\n",
      "('school', 0.9002331305445779)\n",
      "('undergraduate', 0.8133692927150127)\n",
      "('writing', 0.6977814444129545)\n",
      "('science', 0.683545392370001)\n",
      "('summer', 0.6331649985215962)\n",
      "('course', 0.6294736552184661)\n",
      "\n",
      "Topic 1:\n",
      "('doctoral', 0.9016190799488903)\n",
      "('conference', 0.7023612891211701)\n",
      "('consortium', 0.6522387602303537)\n",
      "('student', 0.621216650497578)\n",
      "('international', 0.31419941629679143)\n",
      "('participation', 0.27779084833626644)\n",
      "('participant', 0.27413291634371595)\n",
      "('career', 0.24542863357250755)\n",
      "('travel', 0.24514402449292355)\n",
      "('mentor', 0.2446137303974457)\n",
      "\n",
      "Topic 2:\n",
      "('patient', 1.6060208535521074)\n",
      "('clinical', 0.7098923767707945)\n",
      "('care', 0.701236389939995)\n",
      "('health', 0.48025674862465845)\n",
      "('healthcare', 0.2932686605274337)\n",
      "('provider', 0.2829204942693648)\n",
      "('nurse', 0.2801699764530682)\n",
      "('medical', 0.25447451725416687)\n",
      "('medication', 0.222507494128966)\n",
      "('clinician', 0.19938290139630896)\n",
      "\n",
      "Topic 3:\n",
      "('deep', 1.515214156818411)\n",
      "('network', 1.146621652754097)\n",
      "('neural', 0.6108274152468359)\n",
      "('learning', 0.44311481560490695)\n",
      "('architecture', 0.35997504960344745)\n",
      "('cnn', 0.27573496370750633)\n",
      "('representation', 0.22386153784383211)\n",
      "('train', 0.1975520813966142)\n",
      "('processing', 0.19730364437836226)\n",
      "('biological', 0.19718193996909053)\n",
      "\n",
      "Topic 4:\n",
      "('robot', 1.8626131160750266)\n",
      "('human', 1.7706261717913876)\n",
      "('interaction', 0.5079761607978589)\n",
      "('cognitive', 0.3867685721162997)\n",
      "('robotics', 0.38304662730428674)\n",
      "('machine', 0.3762437965777363)\n",
      "('task', 0.36377684305229996)\n",
      "('planning', 0.36083167379411757)\n",
      "('robotic', 0.35565380892040316)\n",
      "('hri', 0.3276664610563176)\n",
      "\n",
      "Topic 5:\n",
      "('melanoma', 1.1815185111058397)\n",
      "('spore', 0.7652048375136891)\n",
      "('skin', 0.5501479481879499)\n",
      "('cancer', 0.5428766236035876)\n",
      "('yale', 0.49001802938477507)\n",
      "('modifier', 0.4377183899604806)\n",
      "('epigenetic', 0.4377183899604806)\n",
      "('specimen', 0.4004322522917759)\n",
      "('basal', 0.29983347473085004)\n",
      "('dermatology', 0.2852866773722672)\n",
      "\n",
      "Topic 6:\n",
      "('engineering', 1.2004959837756535)\n",
      "('institute', 0.537432012700441)\n",
      "('science', 0.44910873661423756)\n",
      "('advanced', 0.39092909904884804)\n",
      "('medicine', 0.3438567327959325)\n",
      "('reverse', 0.3344012563850857)\n",
      "('engineer', 0.31654858098737176)\n",
      "('brain', 0.31258387995572184)\n",
      "('emerge', 0.3072716621956401)\n",
      "('grand_challenges', 0.2989874703437151)\n",
      "\n",
      "Topic 7:\n",
      "('brain', 1.3398904361349522)\n",
      "('neural', 0.8649780400965267)\n",
      "('neuron', 0.5432779909390199)\n",
      "('cognitive', 0.5319938358998865)\n",
      "('imaging', 0.454355182563681)\n",
      "('neuroscience', 0.41114558937192774)\n",
      "('connectivity', 0.35235041680833834)\n",
      "('cortical', 0.31134110137926213)\n",
      "('task', 0.297914411134475)\n",
      "('circuit', 0.25097374343300494)\n",
      "\n",
      "Topic 8:\n",
      "('algorithm', 1.0490535750017433)\n",
      "('optimization', 0.46018056008317415)\n",
      "('theory', 0.31992127292023287)\n",
      "('machine_learning', 0.3188917606257058)\n",
      "('stochastic', 0.27876128693914615)\n",
      "('decision', 0.2687885178725545)\n",
      "('approximation', 0.2614514325053263)\n",
      "('statistical', 0.2586137298772484)\n",
      "('efficient', 0.2586029268541886)\n",
      "('theoretical', 0.23638557744274136)\n",
      "\n",
      "Topic 9:\n",
      "('language', 1.4569323160327694)\n",
      "('natural', 0.46396729164846756)\n",
      "('word', 0.38074414751859564)\n",
      "('linguistic', 0.3209861367499323)\n",
      "('representation', 0.3183707374723998)\n",
      "('umr', 0.30344949689076905)\n",
      "('human', 0.26755097977690445)\n",
      "('meaning', 0.25333371964805)\n",
      "('processing', 0.22554243282694708)\n",
      "('text', 0.2239674029579825)\n",
      "\n",
      "Topic 10:\n",
      "('visual', 1.3891057179982342)\n",
      "('scene', 0.7352149339296276)\n",
      "('object', 0.6326412726976071)\n",
      "('image', 0.5078017386770983)\n",
      "('vision', 0.3958688936652072)\n",
      "('recognition', 0.3428277748921833)\n",
      "('video', 0.17937728416345702)\n",
      "('shape', 0.16726018790137048)\n",
      "('recognize', 0.16608143819191915)\n",
      "('world', 0.15734479673308616)\n",
      "\n",
      "Topic 11:\n",
      "('cancer', 1.2373883960736551)\n",
      "('breast', 0.9184081716049669)\n",
      "('image', 0.44499732964505473)\n",
      "('imaging', 0.4434116668540392)\n",
      "('clinical', 0.41097691703222516)\n",
      "('tumor', 0.373959738496806)\n",
      "('mammography', 0.2945829566629121)\n",
      "('tissue', 0.26103747296540025)\n",
      "('patient', 0.1997238008169716)\n",
      "('radiologist', 0.1930336591348941)\n",
      "\n",
      "Topic 12:\n",
      "('chemical', 0.8467657866661426)\n",
      "('molecule', 0.5605095685438021)\n",
      "('protein', 0.533706129369477)\n",
      "('drug', 0.491978500959734)\n",
      "('cell', 0.4390586193505782)\n",
      "('property', 0.4183599858510052)\n",
      "('c_elegans', 0.36762211271000217)\n",
      "('molecular', 0.34976559397898077)\n",
      "('reaction', 0.34378562214809544)\n",
      "('chemistry', 0.3056603644488349)\n",
      "\n",
      "Topic 13:\n",
      "('ai', 2.1385192428367668)\n",
      "('eaai', 0.5037331766236386)\n",
      "('symposium', 0.37308354078335493)\n",
      "('artificial_intelligence_ai', 0.32024739894382565)\n",
      "('aaai', 0.23953921039390924)\n",
      "('educational', 0.2340281823772876)\n",
      "('teaching', 0.18742817777705792)\n",
      "('dc', 0.17829238151665655)\n",
      "('education', 0.1709763129859165)\n",
      "('teach', 0.14863175732906508)\n",
      "\n",
      "Topic 14:\n",
      "('social', 1.1980895366378503)\n",
      "('child', 0.279120414403144)\n",
      "('individual', 0.269895330862386)\n",
      "('user', 0.23362969684120063)\n",
      "('autism', 0.2246144010447914)\n",
      "('behavioral', 0.2196664881214236)\n",
      "('interaction', 0.20403365038940477)\n",
      "('asd', 0.18450194664847494)\n",
      "('facial', 0.1667670810133953)\n",
      "('health', 0.162763312902055)\n",
      "\n",
      "Topic 15:\n",
      "('game', 1.5612979918130607)\n",
      "('player', 0.39868001392675523)\n",
      "('environment', 0.326093554158753)\n",
      "('rts_game', 0.2787205623182139)\n",
      "('virtual', 0.26604151298257694)\n",
      "('stem', 0.1859094880415775)\n",
      "('self', 0.18569212625818451)\n",
      "('assembly', 0.18310702607665955)\n",
      "('strategic', 0.1717814093984322)\n",
      "('real', 0.16452839421720802)\n",
      "\n",
      "Topic 16:\n",
      "('workshop', 1.6557347886306786)\n",
      "('participant', 0.2695103108393186)\n",
      "('science', 0.24166821773536484)\n",
      "('acl', 0.2164768535316525)\n",
      "('meeting', 0.20761881068184596)\n",
      "('challenge', 0.2005617979232989)\n",
      "('hri', 0.19442196770031964)\n",
      "('security', 0.18520734506033423)\n",
      "('linguistic', 0.17875135456710922)\n",
      "('conference', 0.17700401697705143)\n",
      "\n",
      "Topic 17:\n",
      "('agent', 1.2678509062980259)\n",
      "('preference', 0.3774659982027293)\n",
      "('decision', 0.338946084191654)\n",
      "('choice', 0.3210809445547778)\n",
      "('pi', 0.23825768784562237)\n",
      "('theory', 0.2203670521117717)\n",
      "('belief', 0.16793360406409857)\n",
      "('domain', 0.16392048639914)\n",
      "('reasoning', 0.1569215790252354)\n",
      "('alternative', 0.15088674234805885)\n",
      "\n",
      "Topic 18:\n",
      "('trainee', 0.8521655420593881)\n",
      "('neuroscience', 0.678456451260377)\n",
      "('graduate', 0.34189764498074604)\n",
      "('nyu', 0.33741506296325496)\n",
      "('faculty', 0.3109972095985799)\n",
      "('computational', 0.23186514867362334)\n",
      "('request', 0.23110506578237824)\n",
      "('degree', 0.2200570001498062)\n",
      "('bmi', 0.2046168340171619)\n",
      "('cns', 0.18958431734419626)\n",
      "\n",
      "Topic 19:\n",
      "('psychosis', 0.8900640266132822)\n",
      "('speech', 0.8596974392878067)\n",
      "('chr', 0.6551720285237954)\n",
      "('disorder', 0.5517679584463715)\n",
      "('schizophrenia', 0.5406739980816501)\n",
      "('onset', 0.44803586703972786)\n",
      "('automated', 0.3461284399703358)\n",
      "('cohort', 0.3243900515297882)\n",
      "('emotion', 0.2870938389071475)\n",
      "('think', 0.28693315127491625)\n",
      "\n",
      "Topic 20:\n",
      "('technology', 0.6754968669737162)\n",
      "('manufacturing', 0.4135735712255623)\n",
      "('sensor', 0.38202808523860593)\n",
      "('industry', 0.3794861785142333)\n",
      "('food', 0.2983227532434293)\n",
      "('device', 0.2783867542946357)\n",
      "('smart', 0.2677221184308212)\n",
      "('energy', 0.2568682523471844)\n",
      "('commercial', 0.2547822927673777)\n",
      "('automation', 0.23988303344143908)\n",
      "\n",
      "Topic 21:\n",
      "('logic', 1.5077718321310594)\n",
      "('solver', 1.0880077616572972)\n",
      "('infrastructure', 0.7329325309145848)\n",
      "('solving', 0.5688261124533219)\n",
      "('formula', 0.45604256206072324)\n",
      "('logical', 0.2987793900564518)\n",
      "('piece', 0.25653068852634775)\n",
      "('verification', 0.24081643874641817)\n",
      "('security', 0.2297614638069956)\n",
      "('library', 0.1833596096517764)\n",
      "\n",
      "Topic 22:\n",
      "('hiv', 1.2421255793786268)\n",
      "('disclosure', 0.38825087996629626)\n",
      "('youth', 0.3555716684832814)\n",
      "('adolescent', 0.32745502175751795)\n",
      "('drug', 0.3255665310878675)\n",
      "('video_game', 0.27238438391512004)\n",
      "('prevention', 0.26270077681742177)\n",
      "('sex', 0.24126083266596432)\n",
      "('tweet', 0.22972692588631594)\n",
      "('sexual', 0.1778554281969909)\n",
      "\n",
      "Topic 23:\n",
      "('narrative', 1.2012459956212136)\n",
      "('creativity', 0.24652727210692182)\n",
      "('clinical', 0.22015050275725984)\n",
      "('character', 0.2093878339928132)\n",
      "('computational', 0.19525789397144044)\n",
      "('story', 0.18283217615173927)\n",
      "('create', 0.17808877496348846)\n",
      "('film', 0.15307210637076435)\n",
      "('generation', 0.14106670413828812)\n",
      "('temporal', 0.1405336017264389)\n",
      "\n",
      "Topic 24:\n",
      "('conference', 0.8244731641998883)\n",
      "('iui', 0.6114014793466289)\n",
      "('interface', 0.5809059653148733)\n",
      "('student', 0.29999863451561115)\n",
      "('participant', 0.24467722470424577)\n",
      "('interaction', 0.22928301296453807)\n",
      "('computer', 0.21602513321985603)\n",
      "('user', 0.2040535627001602)\n",
      "('encourage', 0.20198812105968553)\n",
      "('funding', 0.18698509644000946)\n",
      "\n",
      "Topic 25:\n",
      "('computing', 0.9407298868472904)\n",
      "('architecture', 0.41232459562946155)\n",
      "('hardware', 0.32504459657404916)\n",
      "('memory', 0.2909105075984323)\n",
      "('device', 0.24686291947171174)\n",
      "('performance', 0.2372007790147406)\n",
      "('computation', 0.22593899495837588)\n",
      "('energy', 0.2065516256234933)\n",
      "('computer', 0.20248187008709498)\n",
      "('software', 0.19615912093453694)\n",
      "\n",
      "Topic 26:\n",
      "('ad', 1.159003879542754)\n",
      "('adrd', 0.5313881266988985)\n",
      "('ryan', 0.5187359792115598)\n",
      "('alzheimer', 0.30261902329675194)\n",
      "('dementia', 0.24359432068607945)\n",
      "('sar', 0.18840332762398837)\n",
      "('caregiver', 0.18133129539934922)\n",
      "('socially_assistive_robot', 0.16217013761343274)\n",
      "('care', 0.1555204735551966)\n",
      "('person', 0.15438799560457356)\n",
      "\n",
      "Topic 27:\n",
      "('psoriasis', 0.6604979691079156)\n",
      "('crp', 0.5405460907256172)\n",
      "('cort', 0.46159502247778533)\n",
      "('amc', 0.4600697464672351)\n",
      "('drug', 0.30782405764765153)\n",
      "('pmc', 0.2801419288937877)\n",
      "('ac', 0.20390336831776168)\n",
      "('clinical', 0.17364086779359342)\n",
      "('dataset', 0.16024382943576165)\n",
      "('murine', 0.15614186265687155)\n",
      "\n",
      "Topic 28:\n",
      "('learning', 0.953903725105865)\n",
      "('learn', 0.7116269797151666)\n",
      "('environment', 0.2291879236578043)\n",
      "('infant', 0.1999038659536686)\n",
      "('learner', 0.19376918091345555)\n",
      "('science', 0.1496009766218272)\n",
      "('reinforcement', 0.13846327179953505)\n",
      "('analytics', 0.12981547471712845)\n",
      "('education', 0.12836631731638004)\n",
      "('machine_learning', 0.11947804788396929)\n",
      "\n",
      "Topic 29:\n",
      "('search', 0.9466877472877819)\n",
      "('combinatorial', 0.4875457384361694)\n",
      "('symposium', 0.4065204842070663)\n",
      "('planning', 0.3352625337465513)\n",
      "('heuristic', 0.31717846929092924)\n",
      "('socs', 0.25633872261319696)\n",
      "('optimization', 0.21430914736743345)\n",
      "('robotics', 0.1941589596042567)\n",
      "('path', 0.17026330103950613)\n",
      "('computer', 0.16911447094458668)\n"
     ]
    }
   ],
   "source": [
    "print_topics(nmf_model, nmf_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step - look at documents containing topics. ex) like breast cancer (topic 17)\n",
    "\n",
    "b_cancer_docs = W[:, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(b_cancer_docs > 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41751745497347914"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score = max(b_cancer_docs)\n",
    "max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00241766, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01645926,\n",
       "       0.        , 0.01659395, 0.00395674, 0.00144645, 0.        ,\n",
       "       0.05852488, 0.        , 0.        , 0.00039138, 0.        ,\n",
       "       0.00990916, 0.00176791, 0.        , 0.        , 0.00355259,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03259026, 0.0022023 ,\n",
       "       0.00289476, 0.03592832, 0.00386881, 0.00439733, 0.00685444])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_cancer_docs[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drug', 'lengthy', 'expensive', 'undertaking', 'failure', 'drug', 'exceed', 'successful', 'drug', 'cover', 'cost', 'failure', 'prescription', 'drug', 'price', 'escalate', 'alarming', 'sign', 'stop', 'need', 'successful', 'drug', 'cover', 'failure', 'mean', 'pharmaceutical_company', 'primarily', 'devote', 'pursue', 'drug', 'candidate', 'population', 'company', 'earn', 'return_investment', 'small', 'portion', 'populace', 'nearly', 'oncology', 'cardiovascular', 'immunology', 'practice', 'usually', 'modeling', 'small', 'molecule', 'try', 'adapt', 'biologic', 'scientist', 'prediction', 'feasibility', 'optimal', 'drug', 'property', 'waste', 'pursue', 'chance', 'clinical', 'reimburse', 'payor', 'apply', 'biomath', 'value', 'question', 'middle', 'drug', 'pipeline', 'couple', 'quantitative', 'pharmacology', 'performance', 'computing', 'sophisticated', 'mathematical', 'algorithm', 'prove', 'predict', 'optimal', 'drug', 'property', 'enter', 'clinic', 'past', 'offer', 'pharma_biotech', 'alike', 'rave', 'review', 'inquiry', 'license', 'software', 'fund', 'proprietary', 'algorithm', 'toolset', 'stable', 'standardize', 'software', 'platform', 'automatically', 'validate', 'glp', 'biologic', 'internal', 'pharmacology', 'heart', 'toolset', 'build', 'kronecker', 'bio', 'open', 'source', 'biophysical', 'computational', 'engine', 'cogdevelop', 'founder', 'pursue', 'phd', 'biological', 'engineering', 'computer', 'science', 'artificial_intelligence', 'lab', 'massachusetts', 'institute', 'technology', 'robust', 'platform', 'currently', 'raw', 'pharmaceutical', 'industry', 'limit', 'adoption', 'lack', 'usability', 'contro', 'glp', 'validation', 'presentation', 'layer', 'underlying', 'computational', 'functionality', 'easily', 'access', 'utilize', 'capital', 'requirement', 'typical', 'software', 'achieve', 'build', 'software', 'platform', 'step', 'concerted', 'push', 'biologic', 'segment', 'currently', 'seed', 'offer', 'gain', 'reputation', 'firm', 'deliver', 'value', 'complete', 'second', 'round', 'fundraising', 'raise', 'total', 'round', 'grant', 'fundraising', 'ensure', 'able', 'roll', 'assist', 'drug', 'company', 'deliver', 'bestinclass', 'biologic', 'meet', 'unmet', 'medical', 'need', 'accelerate', 'timeline', 'patient', 'better', 'life', 'better', 'faster_cheaper', 'drug', 'truly', 'winwinin']\n"
     ]
    }
   ],
   "source": [
    "print(lim_docs.iloc[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(b_cancer_docs == max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([275, 809]),)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00108239, 0.        , 0.0053626 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00339288, 0.        , 0.        , 0.        ,\n",
       "       0.46861872, 0.        , 0.32673669, 0.        , 0.        ])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_cancer_docs[230:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drug', 'lengthy', 'expensive', 'undertaking', 'failure', 'drug', 'exceed', 'successful', 'drug', 'cover', 'cost', 'failure', 'prescription', 'drug', 'price', 'escalate', 'alarming', 'sign', 'stop', 'need', 'successful', 'drug', 'cover', 'failure', 'mean', 'pharmaceutical_company', 'primarily', 'devote', 'pursue', 'drug', 'candidate', 'population', 'company', 'earn', 'return_investment', 'small', 'portion', 'populace', 'nearly', 'oncology', 'cardiovascular', 'immunology', 'practice', 'usually', 'modeling', 'small', 'molecule', 'try', 'adapt', 'biologic', 'scientist', 'prediction', 'feasibility', 'optimal', 'drug', 'property', 'waste', 'pursue', 'chance', 'clinical', 'reimburse', 'payor', 'apply', 'biomath', 'value', 'question', 'middle', 'drug', 'pipeline', 'couple', 'quantitative', 'pharmacology', 'performance', 'computing', 'sophisticated', 'mathematical', 'algorithm', 'prove', 'predict', 'optimal', 'drug', 'property', 'enter', 'clinic', 'past', 'offer', 'pharma_biotech', 'alike', 'rave', 'review', 'inquiry', 'license', 'software', 'fund', 'proprietary', 'algorithm', 'toolset', 'stable', 'standardize', 'software', 'platform', 'automatically', 'validate', 'glp', 'biologic', 'internal', 'pharmacology', 'heart', 'toolset', 'build', 'kronecker', 'bio', 'open', 'source', 'biophysical', 'computational', 'engine', 'cogdevelop', 'founder', 'pursue', 'phd', 'biological', 'engineering', 'computer', 'science', 'artificial_intelligence', 'lab', 'massachusetts', 'institute', 'technology', 'robust', 'platform', 'currently', 'raw', 'pharmaceutical', 'industry', 'limit', 'adoption', 'lack', 'usability', 'contro', 'glp', 'validation', 'presentation', 'layer', 'underlying', 'computational', 'functionality', 'easily', 'access', 'utilize', 'capital', 'requirement', 'typical', 'software', 'achieve', 'build', 'software', 'platform', 'step', 'concerted', 'push', 'biologic', 'segment', 'currently', 'seed', 'offer', 'gain', 'reputation', 'firm', 'deliver', 'value', 'complete', 'second', 'round', 'fundraising', 'raise', 'total', 'round', 'grant', 'fundraising', 'ensure', 'able', 'roll', 'assist', 'drug', 'company', 'deliver', 'bestinclass', 'biologic', 'meet', 'unmet', 'medical', 'need', 'accelerate', 'timeline', 'patient', 'better', 'life', 'better', 'faster_cheaper', 'drug', 'truly', 'winwinin']\n"
     ]
    }
   ],
   "source": [
    "print(lim_docs.iloc[25]) # breast cancer topic with AI component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff   DESCRIPTION (provided by applicant):  Drug development is a very lengthy and expensive undertaking. Failure rate for novel drugs exceeds 95%. Therefore, successful drugs must cover the costs of these failures. As such, prescription drug prices have escalated at an alarming rate and show no signs of stopping. The need for successful drugs to cover failures also means that pharmaceutical companies primarily devote resources to pursuing drug candidates that have a large enough population to allow the company to earn a return on its investment. Thus, diseases that affect only a small portion of the populace are not investigated nearly as much as, say, oncology, cardiovascular or immunology. Current practice usually involves taking modeling techniques developed for small molecule research and trying to adapt them to biologics. However, this approach, more often than not, does not provide the scientist with predictions around feasibility and optimal drug properties, resulting in wasted effort pursuing leads that haveno chance of making it through clinical trials, or to be reimbursed by payors. Applied BioMath has developed tools that address high value questions in the middle of the drug development pipeline. By coupling quantitative systems pharmacology techniques with high performance computing and sophisticated mathematical algorithms, we have proven an ability to predict optimal drug properties years before entering the clinic. For the past two years we have been offering our services to pharma and biotechs alike, to rave reviews. We have also been approached with inquiries to license our software. This project will fund the development of our proprietary algorithms and toolsets into a stable, standardized software platform, that can be automatically validated for GLP, for by biologics to develop their internal systems pharmacology models. At its heart, our toolsets are built on Kronecker Bio, an open source biophysical computational engine co\\xaddeveloped by one of our Founders while pursuing his PhD in Biological Engineering with the Computer Science and Artificial Intelligence Lab from the Massachusetts Institute of Technology. This robust platform is currently in use, in its raw form, in the pharmaceutical industry but is limited in its adoption due to its lack of usability, quality contro and GLP validation. This project will focus on the application and presentation layer, allowing the underlying computational functionality to be easily accessed, utilized and understood, so capital requirements are less than a typical software development project. Achieving our goal of building this software platform is only the first step. What follows is a concerted push into the biologics segment, which we are currently seeding through our services offering and gaining a reputation as a firm that delivers high value on time. We have completed our second round of fundraising, raising a total of $1.8m between both rounds. This grant, plus the additional fundraising, will ensure that we are able to roll out our tools and assist drug companies in delivering best\\xadin\\xadclass biologics, that meet unmet medical need, on an accelerated timeline to provide patients with a better quality of life. Better, faster, cheaper drugs... truly a win\\xadwin\\xadin.'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_corpus[\"ABSTRACT\"].iloc[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                                     20611\n",
       "original index                                                           891756\n",
       "PROJECT_ID                                                               933594\n",
       "ABSTRACT                      ï»¿   DESCRIPTION (provided by applicant):  Drug...\n",
       "FY                                                                         2017\n",
       "PROJECT_TERMS                 absorption; Address; Adoption; Affect; Algorit...\n",
       "PROJECT_TITLE                 A QUANTITATIVE SYSTEMS PHARMACOLOGY SOFTWARE P...\n",
       "DEPARTMENT                                                                  HHS\n",
       "AGENCY                                                                      NIH\n",
       "IC_CENTER                                                                 NIGMS\n",
       "PROJECT_NUMBER                                                  5R44GM116214-02\n",
       "PROJECT_START_DATE                                                     5/1/2016\n",
       "PROJECT_END_DATE                                                      1/31/2019\n",
       "CONTACT_PI_PROJECT_LEADER                                         APGAR, JOSHUA\n",
       "OTHER_PIS                                                                   NaN\n",
       "CONGRESSIONAL_DISTRICT                                                       05\n",
       "DUNS_NUMBER                                                           079422941\n",
       "ORGANIZATION_NAME                                          APPLIED BIOMATH  LLC\n",
       "ORGANIZATION_CITY                                                       LINCOLN\n",
       "ORGANIZATION_STATE                                                           MA\n",
       "ORGANIZATION_ZIP                                                      017731125\n",
       "ORGANIZATION_COUNTRY                                              UNITED STATES\n",
       "BUDGET_START_DATE                                                      2/1/2017\n",
       "BUDGET_END_DATE                                                       1/31/2019\n",
       "CFDA_CODE                                                                93.859\n",
       "FY.y                                                                       2017\n",
       "FY_TOTAL_COST                                                       1.01665e+06\n",
       "FY_TOTAL_COST_SUB_PROJECTS                                                  NaN\n",
       "ORG_COUNT                                                                     1\n",
       "PI_COUNT                                                                      1\n",
       "working_abstract              Drug development is a very lengthy and expensi...\n",
       "Start_Char                                                                    D\n",
       "nchar                                                                      3243\n",
       "LAST_CHAR                                                                     n\n",
       "lemma_abstract                [drug, development, very, lengthy, expensive, ...\n",
       "clean_lemmas                  [drug, development, very, lengthy, expensive, ...\n",
       "stopwds_removed               [drug, development, lengthy, expensive, undert...\n",
       "n_grams_added                 [drug, development, lengthy, expensive, undert...\n",
       "final_tokens                  [drug, development, lengthy, expensive, undert...\n",
       "final_frqwds_removed          [drug, lengthy, expensive, undertaking, failur...\n",
       "Name: 20611, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_corpus.iloc[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                                    628125\n",
       "original index                                                           844332\n",
       "PROJECT_ID                                                               819994\n",
       "ABSTRACT                      Social â€œbig dataâ€ holds information with wide-...\n",
       "FY                                                                         2016\n",
       "PROJECT_TERMS                 Address; Affect; AIDS prevention; Algorithms; ...\n",
       "PROJECT_TITLE                 MINING REAL-TIME SOCIAL MEDIA BIG DATA TO MONI...\n",
       "DEPARTMENT                                                                  HHS\n",
       "AGENCY                                                                      NIH\n",
       "IC_CENTER                                                                 NIAID\n",
       "PROJECT_NUMBER                                                1R56AI125105-01A1\n",
       "PROJECT_START_DATE                                                     9/1/2016\n",
       "PROJECT_END_DATE                                                      8/31/2019\n",
       "CONTACT_PI_PROJECT_LEADER                                           YOUNG, SEAN\n",
       "OTHER_PIS                                         CONDIE, TYSON  ; WANG, WEI  ;\n",
       "CONGRESSIONAL_DISTRICT                                                       33\n",
       "DUNS_NUMBER                                                           092530369\n",
       "ORGANIZATION_NAME                          UNIVERSITY OF CALIFORNIA LOS ANGELES\n",
       "ORGANIZATION_CITY                                                   LOS ANGELES\n",
       "ORGANIZATION_STATE                                                           CA\n",
       "ORGANIZATION_ZIP                                                      900952000\n",
       "ORGANIZATION_COUNTRY                                              UNITED STATES\n",
       "BUDGET_START_DATE                                                      9/1/2016\n",
       "BUDGET_END_DATE                                                       8/31/2019\n",
       "CFDA_CODE                                                                93.855\n",
       "FY.y                                                                       2016\n",
       "FY_TOTAL_COST                                                            671438\n",
       "FY_TOTAL_COST_SUB_PROJECTS                                                  NaN\n",
       "ORG_COUNT                                                                     1\n",
       "PI_COUNT                                                                      1\n",
       "working_abstract              Social â€œbig dataâ€ holds information with wide-...\n",
       "Start_Char                                                                    S\n",
       "nchar                                                                      3053\n",
       "LAST_CHAR                                                                     V\n",
       "lemma_abstract                [social, big, data, hold, information, wide, r...\n",
       "clean_lemmas                  [social, big, data, hold, information, wide, r...\n",
       "stopwds_removed               [social, big, data, hold, information, wide, r...\n",
       "n_grams_added                 [social, big, data, hold, information, wide, r...\n",
       "final_tokens                  [social, big, data, hold, information, wide, r...\n",
       "final_frqwds_removed          [social, big, hold, wide, range, implication, ...\n",
       "Name: 628085, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_corpus.iloc[809]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY TOPIC MODELING WITH LDA\n",
    "\n",
    "# create document-term matrix\n",
    "\n",
    "lda_vectorizer = CountVectorizer(max_df=1.0, min_df=3, lowercase=True)\n",
    "lda_dtm = lda_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "num_topics = 30\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, doc_topic_prior = 1/num_topics, \n",
    "                                      topic_word_prior=0.1, n_jobs=39, random_state = 0)\n",
    "doc_top_dist = lda_model.fit_transform(lda_dtm)\n",
    "top_term_dist = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "('protein', 225.55502749264147)\n",
      "('receptor', 183.95759402950594)\n",
      "('cell', 172.4504639973329)\n",
      "('bind', 108.84685928736633)\n",
      "('structural', 83.23407993256878)\n",
      "('virus', 81.06300054201091)\n",
      "('viral', 75.5561411680632)\n",
      "('interaction', 68.26001146683492)\n",
      "('human', 67.85457954982031)\n",
      "('host', 64.82284755171038)\n",
      "\n",
      "Topic 1:\n",
      "('subset', 81.29454728313978)\n",
      "('cd4', 66.23840025065098)\n",
      "('memory', 62.37910850799191)\n",
      "('effector', 40.5848650803486)\n",
      "('viral', 40.11634209137843)\n",
      "('cell', 36.6079502522241)\n",
      "('rna', 36.412992443476384)\n",
      "('orf', 35.0999999999997)\n",
      "('protection', 33.610323276130394)\n",
      "('protein', 29.212279854306338)\n",
      "\n",
      "Topic 2:\n",
      "('cell', 202.03669046303705)\n",
      "('death', 163.99491597475762)\n",
      "('protein', 84.11074125989704)\n",
      "('human', 76.15368948404043)\n",
      "('receptor', 61.47717954877514)\n",
      "('cycle', 58.31210018295787)\n",
      "('autophagy', 45.205793172633825)\n",
      "('cellular', 39.855071437383906)\n",
      "('virus', 39.49494201595987)\n",
      "('play', 34.583390658500534)\n",
      "\n",
      "Topic 3:\n",
      "('virus', 503.9476289736178)\n",
      "('viral', 325.2321535714292)\n",
      "('host', 286.15035135611254)\n",
      "('protein', 256.0762166468321)\n",
      "('rna', 248.50619917303135)\n",
      "('replication', 245.83348386659708)\n",
      "('coronavirus', 192.01133936013733)\n",
      "('cell', 191.60339705724633)\n",
      "('gene', 152.11975127500102)\n",
      "('inhibitor', 145.26787553860322)\n",
      "\n",
      "Topic 4:\n",
      "('hcv', 936.1753590840242)\n",
      "('hiv', 632.951690355523)\n",
      "('infection', 134.9779998086495)\n",
      "('patient', 129.20551468069908)\n",
      "('care', 101.4077745557858)\n",
      "('infect', 89.42085859532881)\n",
      "('drug', 87.35339680209223)\n",
      "('testing', 83.95306041199797)\n",
      "('therapy', 67.12313233292302)\n",
      "('pwid', 57.09998924511843)\n",
      "\n",
      "Topic 5:\n",
      "('macrophage', 31.248062120572182)\n",
      "('autophagy', 25.548084297365087)\n",
      "('lymph_node', 24.57677967487654)\n",
      "('mhc', 22.256418456312527)\n",
      "('inflammasome', 18.38049756732289)\n",
      "('allele', 13.797361678592972)\n",
      "('dpp4', 12.611532939763322)\n",
      "('stress', 12.381605657078323)\n",
      "('lymphocyte', 12.378169577119507)\n",
      "('ko', 11.581209770269776)\n",
      "\n",
      "Topic 6:\n",
      "('cell', 934.7630243171636)\n",
      "('kshv', 644.071509136028)\n",
      "('ebv', 249.03681019003906)\n",
      "('infection', 206.40151230095978)\n",
      "('tumor', 204.29310230359386)\n",
      "('ks', 164.44968704007013)\n",
      "('gene', 157.9034254506393)\n",
      "('patient', 137.32721670022917)\n",
      "('latency', 119.06444569548658)\n",
      "('infect', 118.52913997672619)\n",
      "\n",
      "Topic 7:\n",
      "('infection', 37.96606498605185)\n",
      "('hiv', 33.76821571557734)\n",
      "('ebv', 31.115520854869917)\n",
      "('ctl', 28.45277437853249)\n",
      "('blood', 24.656756917921147)\n",
      "('lymphoma', 22.28779492866996)\n",
      "('aids', 22.26071367147582)\n",
      "('cancer', 20.803584883813166)\n",
      "('inflammation', 20.214829806497576)\n",
      "('cell', 19.58788170362728)\n",
      "\n",
      "Topic 8:\n",
      "('patient', 54.54788106866963)\n",
      "('ebv', 44.73114870784985)\n",
      "('therapy', 41.96609261330226)\n",
      "('ribosome', 39.766882924657075)\n",
      "('npc', 34.712694079240485)\n",
      "('hiv', 34.42578022429774)\n",
      "('kshv', 25.76576034198771)\n",
      "('cell', 24.210821678231724)\n",
      "('viral', 20.1858803027162)\n",
      "('combine', 18.58042345210228)\n",
      "\n",
      "Topic 9:\n",
      "('dna', 49.092469057059134)\n",
      "('molecule', 36.05519013665967)\n",
      "('interaction', 28.521474390341215)\n",
      "('cnt', 25.099998160903862)\n",
      "('drug', 24.451244905273363)\n",
      "('marker', 17.25102149760146)\n",
      "('molecular', 16.403091079555782)\n",
      "('neurocognitive', 13.579437937030622)\n",
      "('biology', 13.45022424178504)\n",
      "('pharmacokinetic', 13.09999362818956)\n",
      "\n",
      "Topic 10:\n",
      "('virus', 281.5459801216521)\n",
      "('vector', 205.3719205987978)\n",
      "('vaccine', 159.45759715332724)\n",
      "('infection', 137.34190539828347)\n",
      "('ndv', 127.93951750647453)\n",
      "('hpiv3', 106.09999824759721)\n",
      "('dose', 96.94269498581012)\n",
      "('express', 91.02185874747664)\n",
      "('respiratory', 85.16365186595259)\n",
      "('protein', 83.9930629823828)\n",
      "\n",
      "Topic 11:\n",
      "('influenza', 147.31641992929428)\n",
      "('patient', 120.39671583081108)\n",
      "('clinical', 98.13476379246524)\n",
      "('emerge', 60.77197942644498)\n",
      "('infection', 55.714783774117606)\n",
      "('protocol', 54.53067060905437)\n",
      "('care', 40.94011646530207)\n",
      "('infectious', 37.581736532400356)\n",
      "('dose', 37.5082034001803)\n",
      "('virus', 35.96013982812435)\n",
      "\n",
      "Topic 12:\n",
      "('virus', 153.62964944511606)\n",
      "('human', 114.47456736113963)\n",
      "('infectious', 108.83793302623653)\n",
      "('health', 62.14013558425235)\n",
      "('epidemiology', 58.83806564875385)\n",
      "('public', 58.45553037079896)\n",
      "('pathogen', 58.141758887593724)\n",
      "('emerge', 57.96085605948189)\n",
      "('respiratory', 56.86762371306408)\n",
      "('infection', 56.31796475597069)\n",
      "\n",
      "Topic 13:\n",
      "('human', 53.35607439223278)\n",
      "('inactivation', 51.95940358475925)\n",
      "('plasma', 51.34206354501669)\n",
      "('virus', 51.221905871453465)\n",
      "('cfi', 45.099999701806894)\n",
      "('technology', 38.71534007295505)\n",
      "('bacteria', 37.76394849442186)\n",
      "('pathogen', 34.90475793724826)\n",
      "('blood', 29.9358126139684)\n",
      "('unit', 29.779111494776974)\n",
      "\n",
      "Topic 14:\n",
      "('cell', 372.6554349855349)\n",
      "('virus', 300.92800316166296)\n",
      "('mouse', 274.4906554047676)\n",
      "('sars_cov', 230.4177664236629)\n",
      "('infection', 197.66947006316775)\n",
      "('protein', 187.08252776826131)\n",
      "('human', 156.96552151397535)\n",
      "('lung', 153.20635617159826)\n",
      "('sar_cov', 137.91357387274857)\n",
      "('sars', 137.08053338965914)\n",
      "\n",
      "Topic 15:\n",
      "('gene', 237.3553797841826)\n",
      "('genetic', 119.5861349439917)\n",
      "('npc', 112.96660802507539)\n",
      "('individual', 76.39456684063575)\n",
      "('population', 68.57990551455192)\n",
      "('ebv', 64.13629588534057)\n",
      "('level', 63.53760362045235)\n",
      "('cell', 61.82857778036647)\n",
      "('immune', 57.99535564937193)\n",
      "('inference', 57.09998399875331)\n",
      "\n",
      "Topic 16:\n",
      "('hiv', 315.8619665259027)\n",
      "('hsv', 182.24248900403467)\n",
      "('health', 141.7827974379656)\n",
      "('infection', 116.0088514380458)\n",
      "('spread', 80.2945145425962)\n",
      "('individual', 77.77331587568254)\n",
      "('art', 71.57568484876215)\n",
      "('neighborhood', 66.09996601859086)\n",
      "('public', 58.81363331552422)\n",
      "('influence', 53.536044672485716)\n",
      "\n",
      "Topic 17:\n",
      "('cell', 15.278114860965227)\n",
      "('dna', 14.230090464312385)\n",
      "('mir', 11.711383186000122)\n",
      "('coronaviruse', 10.376286892084652)\n",
      "('gs_5734', 9.735086425040814)\n",
      "('human', 9.567253566693791)\n",
      "('stage', 9.542991336591268)\n",
      "('fracture', 9.34752412787078)\n",
      "('virus', 9.271969074533509)\n",
      "('infect', 8.78741304706353)\n",
      "\n",
      "Topic 18:\n",
      "('immune', 343.6097573757179)\n",
      "('antibiotic', 162.10065477411436)\n",
      "('gene', 132.9284518710922)\n",
      "('host', 132.27881107032914)\n",
      "('immunity', 114.35949761792645)\n",
      "('virus', 101.27540891709802)\n",
      "('transmission', 89.90178372669166)\n",
      "('vaccination', 83.13574563361934)\n",
      "('protective', 83.02891297545558)\n",
      "('polygenic', 80.09999999999806)\n",
      "\n",
      "Topic 19:\n",
      "('vaccine', 212.13853388844507)\n",
      "('antibody', 183.14176871524614)\n",
      "('mers_cov', 177.3943510644298)\n",
      "('infection', 154.24324638952956)\n",
      "('human', 138.604975872332)\n",
      "('virus', 125.26640753973862)\n",
      "('cell', 111.07428269505291)\n",
      "('animal', 105.31857413541228)\n",
      "('viral', 76.92133426775077)\n",
      "('efficacy', 76.24320024843422)\n",
      "\n",
      "Topic 20:\n",
      "('ace2', 74.64692958681516)\n",
      "('emerge', 55.65097439618435)\n",
      "('virus', 51.99525711664124)\n",
      "('human', 48.52380729596921)\n",
      "('fusion', 44.23932249436229)\n",
      "('peptide', 41.21774842846524)\n",
      "('infection', 36.40422202073073)\n",
      "('epithelial', 36.12178324690375)\n",
      "('protein', 35.39467814022834)\n",
      "('sars_cov', 35.2646229591111)\n",
      "\n",
      "Topic 21:\n",
      "('vaccine', 188.9054227002202)\n",
      "('mouse', 133.75458246858923)\n",
      "('protein', 110.31113831780557)\n",
      "('cell', 67.07456911062577)\n",
      "('virus', 66.002068214547)\n",
      "('sars_cov', 64.54526990855365)\n",
      "('infect', 56.588147308767866)\n",
      "('infection', 55.1059901032736)\n",
      "('sar_cov', 53.69954355338436)\n",
      "('rbd', 50.16731090890117)\n",
      "\n",
      "Topic 22:\n",
      "('hla', 94.76255567810914)\n",
      "('cell', 92.00596645621278)\n",
      "('antigen', 48.3786975638055)\n",
      "('cytc', 48.09999999999929)\n",
      "('molecule', 45.72369785803854)\n",
      "('ebv', 41.82026741406932)\n",
      "('trophoblast', 40.0999999999993)\n",
      "('test', 34.3828288128813)\n",
      "('patient', 33.26100258863145)\n",
      "('phosphorylation', 29.45951867508582)\n",
      "\n",
      "Topic 23:\n",
      "('influenza', 95.28592397287079)\n",
      "('lana', 72.064505714747)\n",
      "('c_mybp_c', 50.09999999999928)\n",
      "('interaction', 43.824858986166184)\n",
      "('molecule', 42.587728420167856)\n",
      "('structural', 34.87695528651681)\n",
      "('protein', 33.19628529201318)\n",
      "('phosphorylation', 30.453933448601486)\n",
      "('repression', 24.099965112891258)\n",
      "('cardiac', 20.44293899747181)\n",
      "\n",
      "Topic 24:\n",
      "('hrv', 50.09999999999904)\n",
      "('virus', 40.933177344196956)\n",
      "('respiratory', 32.4464878641917)\n",
      "('assay', 29.073062821791673)\n",
      "('infection', 26.663916212356988)\n",
      "('malawi', 25.09998802599216)\n",
      "('pla2g2d', 24.099998554495706)\n",
      "('viral', 23.111823058109575)\n",
      "('asthma', 23.09760740712724)\n",
      "('population', 23.092783119638568)\n",
      "\n",
      "Topic 25:\n",
      "('rna', 196.59215346629395)\n",
      "('virus', 81.91558681690859)\n",
      "('fidelity', 77.0408929802264)\n",
      "('molecule', 58.05456406158865)\n",
      "('cov', 55.63374376091291)\n",
      "('exon', 52.099990699525385)\n",
      "('genome', 50.72242428813553)\n",
      "('pdk1', 48.099999999998886)\n",
      "('protein', 40.90658911188318)\n",
      "('vivo', 38.96468485936381)\n",
      "\n",
      "Topic 26:\n",
      "('ks', 263.2347563169589)\n",
      "('art', 125.54703358141448)\n",
      "('patient', 67.44834059092155)\n",
      "('hiv', 54.60260816366088)\n",
      "('clinical', 44.78909910409095)\n",
      "('survival', 29.606473220194474)\n",
      "('africa', 28.53622453605498)\n",
      "('cancer', 28.462595269547066)\n",
      "('antiretroviral', 25.904922270545423)\n",
      "('care', 24.894611560269013)\n",
      "\n",
      "Topic 27:\n",
      "('cell', 219.8952930931712)\n",
      "('clinical', 61.15605808536564)\n",
      "('cd8_t', 60.944181092314544)\n",
      "('splicing', 57.099990858616216)\n",
      "('hsv', 55.42689654307695)\n",
      "('hiv', 52.90090117682046)\n",
      "('cancer', 51.39939835147358)\n",
      "('vaccination', 50.74765888690318)\n",
      "('tumor', 49.075981432919995)\n",
      "('vector', 45.18522355150008)\n",
      "\n",
      "Topic 28:\n",
      "('protein', 125.91509064143841)\n",
      "('virus', 117.5683409093777)\n",
      "('cell', 91.89723430510497)\n",
      "('infection', 84.39740344190295)\n",
      "('mouse', 75.6367047918581)\n",
      "('inhibitor', 65.60368950914612)\n",
      "('gene', 59.85410278029665)\n",
      "('mhv', 54.09227060111421)\n",
      "('viral', 54.08003211512107)\n",
      "('human', 52.4846760214769)\n",
      "\n",
      "Topic 29:\n",
      "('lab', 174.89846594617197)\n",
      "('dna', 88.13822719069428)\n",
      "('ebv', 74.89085870605918)\n",
      "('testing', 51.10952784452906)\n",
      "('cell', 43.208660960539106)\n",
      "('assay', 42.27223819058267)\n",
      "('laboratory', 41.0959338536628)\n",
      "('blood', 39.73378493078292)\n",
      "('individual', 39.16535523199175)\n",
      "('test', 37.06729556172424)\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model, lda_vectorizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
