{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import tomotopy as tp\n",
    "\n",
    "import sys\n",
    "\n",
    "import gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression as lm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.collections as mcol\n",
    "from matplotlib.legend_handler import HandlerLineCollection, HandlerTuple\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../dspg20RnD/data/final/dashboard_data/corona_corpus.pkl\")\n",
    "docs = df[\"final_frqwds_removed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get topic word distributions\n",
    "def list_topics_hpam(mdl, top_n):\n",
    "    \n",
    "    topic_words = []\n",
    "    \n",
    "    for k in range(1 + mdl.k1 + mdl.k2):\n",
    "        topic_words.append([words[0] for words in mdl.get_topic_words(k, top_n)])\n",
    "    \n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hPAM topic dist\n",
    "# function to get level of each topic as well as top 10 words in dist\n",
    "def sub_topic_dist(mdl, top_n):\n",
    "    \n",
    "    sub_topics = []\n",
    "    topic_words = []\n",
    "    \n",
    "    topic_words.append([-1, 0, mdl.get_topic_words(0, top_n = top_n)])\n",
    "    \n",
    "    for k in range(1, 1+mdl.k1):\n",
    "        topic_words.append([0, k, mdl.get_topic_words(k, top_n = top_n)])\n",
    "        \n",
    "    for p in range(1+mdl.k1, 1+mdl.k1+mdl.k2):\n",
    "        topic_words.append([1, p, mdl.get_topic_words(p, top_n = top_n)])\n",
    "        \n",
    "    topic_words_df = pd.DataFrame(topic_words)\n",
    "    topic_words_df.columns = ['parent_level', 'topic_id', 'Top 10 words']\n",
    "    \n",
    "    for l in range(mdl.k1):\n",
    "        subtopics = mdl.get_sub_topics(l, top_n = 3)\n",
    "        sub_topics.append(subtopics)\n",
    "      \n",
    "    sub_topic_df = pd.DataFrame(sub_topics)\n",
    "\n",
    "    return topic_words_df, sub_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTCvars(docs):\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "    # Create Corpus (Term Document Frequency)\n",
    "\n",
    "    #Creates a count for each unique word appearing in the document, where the word_id is substituted for the word\n",
    "    corpus = [id2word.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    return id2word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict, docs_corpus = createTCvars(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal hPAM parameters\n",
    "alpha = 0.3\n",
    "eta = 0.3\n",
    "min_cf = 50\n",
    "rm_top = 0\n",
    "k1 = 7\n",
    "k2 = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hierarchical PA model                                                                                 \n",
    "mdl = tp.HPAModel(tw = tp.TermWeight.IDF, \n",
    "                  min_cf = min_cf, \n",
    "                  rm_top = rm_top,\n",
    "                  k1 = k1, \n",
    "                  k2 = k2, \n",
    "                  alpha = alpha, \n",
    "                  eta = eta,\n",
    "                  seed = 123)\n",
    "\n",
    "# load docs\n",
    "for abstracts in docs:\n",
    "    mdl.add_doc(abstracts)\n",
    "\n",
    "# setup model                                                                                                                          \n",
    "mdl.burn_in = 100\n",
    "mdl.train(0)\n",
    "\n",
    "# train model\n",
    "for i in range(0, 1000, 10):\n",
    "    mdl.train(10)\n",
    "    #print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "\n",
    "# create list of topics\n",
    "topics = list_topics_hpam(mdl, top_n = 10)\n",
    "\n",
    "# calculate topic coherence\n",
    "cm = CoherenceModel(topics = topics, corpus = docs_corpus, dictionary = docs_dict, \n",
    "                    texts = docs, coherence = 'c_v', processes = 8)\n",
    "\n",
    "cv = cm.get_coherence()\n",
    "\n",
    "# get topic distributions\n",
    "topic_words_df, sub_topic_df = sub_topic_dist(mdl, top_n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract candidates for auto topic labeling\n",
    "extractor = tp.label.PMIExtractor(min_cf = min_cf, min_df = 0, max_len = 5, max_cand = 10000)\n",
    "cands = extractor.extract(mdl)\n",
    "\n",
    "# ranking the candidates of labels for a specific topic\n",
    "labeler = tp.label.FoRelevance(mdl, cands, min_df = 0, smoothing = 1e-2, mu = 0.25)\n",
    "\n",
    "label_lst = []\n",
    "for p in range(1+mdl.k1+mdl.k2):\n",
    "    label_lst.append(label for label, score in labeler.get_topic_labels(p, top_n = 5))\n",
    "\n",
    "label_df = pd.DataFrame(label_lst)\n",
    "label_df.columns = ['Label 1', 'Label 2', 'Label 3', 'Label 4', 'Label 5']\n",
    "\n",
    "topic_words_label_df = pd.concat([topic_words_df.reset_index(drop = True), label_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_label_df.to_csv(r'hpam_results/corona/hpam_corona_optimal_topic_words.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_topic_df.to_csv(r'hpam_results/hpam_corona_optimal_subtopic.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract highest probability topic for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 1 + mdl.k1 + mdl.k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc_topic(mdl):\n",
    "    \n",
    "    doc_topic = []\n",
    "    \n",
    "    for d in range(len(mdl.docs)):\n",
    "        doc_topic.append(list(mdl.docs[d].get_topic_dist()))\n",
    "    \n",
    "    return pd.DataFrame(doc_topic, columns=[str(i) for i in range(num_topics)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_mat = create_doc_topic(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_topic = list(map(int, doc_topic_mat.idxmax(axis = 1)))\n",
    "max_topic_prob = list(doc_topic_mat.max(axis = 1))\n",
    "\n",
    "max_topic_mat = pd.DataFrame({\n",
    "'topic_id': max_topic,\n",
    "'max_topic_prob': max_topic_prob\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_topic_mat = max_topic_mat.merge(topic_words_label_df[['topic_id', \"Top 10 words\"]],\n",
    "                                    on = 'topic_id',\n",
    "                                    how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_topic_mat['doc'] = list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_topic_mat.to_csv('hpam_results/corona/hpam_corona_optimal_doc_max_topic.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot hottest and coldest topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYear(a):   \n",
    "    a = str(a)\n",
    "    if a.find(\"/\"):\n",
    "        splitdate = a.split(\"/\")\n",
    "        if len(splitdate) == 3:\n",
    "            a = splitdate[2]\n",
    "        else:\n",
    "            a = splitdate[0]\n",
    "    year = str(a)\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['START_YEAR'] = df['PROJECT_START_DATE'].apply(getYear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_mat['START_YEAR'] = list(df['START_YEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_frame = doc_topic_mat.groupby(\"START_YEAR\").mean().reset_index()\n",
    "topic_frame = topic_frame.sort_values(by = \"START_YEAR\")\n",
    "topic_frame[\"START_YEAR\"] = topic_frame[\"START_YEAR\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter topic_frame for years 2010 - 2019\n",
    "topic_pr_2010_2019 = topic_frame[topic_frame[\"START_YEAR\"] > 2009] \n",
    "topic_pr_2010_2019 = topic_pr_2010_2019[topic_pr_2010_2019[\"START_YEAR\"] < 2020] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_slopes = []\n",
    "topic_slopes_se = []\n",
    "topic_results = {}\n",
    "\n",
    "lm_x = topic_pr_2010_2019[\"START_YEAR\"].values.reshape(-1,1)\n",
    "lm_x = sm.add_constant(lm_x)\n",
    "\n",
    "for i in range(1, num_topics+1):\n",
    "\n",
    "    linear_fit = sm.OLS(topic_pr_2010_2019.iloc[:,i].values.reshape(-1,1),lm_x).fit()\n",
    "    \n",
    "    topic_slopes.append(linear_fit.params[1])\n",
    "    topic_slopes_se.append(linear_fit.bse[1])\n",
    "    topic_results[i] = linear_fit.params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_cold = []\n",
    "\n",
    "for slope in topic_slopes:\n",
    "    if slope > 0:\n",
    "        hot_cold.append(\"hot\")\n",
    "    else:\n",
    "        hot_cold.append(\"cold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandlerDashedLines(HandlerLineCollection):\n",
    "    def create_artists(self, legend, orig_handle,\n",
    "                       xdescent, ydescent, width, height, fontsize, trans):\n",
    "        # figure out how many lines there are\n",
    "        numlines = len(orig_handle.get_segments())\n",
    "        xdata, xdata_marker = self.get_xdata(legend, xdescent, ydescent,\n",
    "                                             width, height, fontsize)\n",
    "        leglines = []\n",
    "        # divide the vertical space where the lines will go\n",
    "        # into equal parts based on the number of lines\n",
    "        ydata = np.full_like(xdata, height / (numlines + 1))\n",
    "        # for each line, create the line at the proper location\n",
    "        # and set the dash pattern\n",
    "        for i in range(numlines):\n",
    "            legline = Line2D(xdata, ydata * (numlines - i) - ydescent)\n",
    "            self.update_prop(legline, orig_handle, legend)\n",
    "            # set color, dash pattern, and linewidth to that\n",
    "            # of the lines in linecollection\n",
    "            try:\n",
    "                color = orig_handle.get_colors()[i]\n",
    "            except IndexError:\n",
    "                color = orig_handle.get_colors()[0]\n",
    "            try:\n",
    "                dashes = orig_handle.get_dashes()[i]\n",
    "            except IndexError:\n",
    "                dashes = orig_handle.get_dashes()[0]\n",
    "            try:\n",
    "                lw = orig_handle.get_linewidths()[i]\n",
    "            except IndexError:\n",
    "                lw = orig_handle.get_linewidths()[0]\n",
    "            if dashes[0] is not None:\n",
    "                legline.set_dashes(dashes[1])\n",
    "            legline.set_color(color)\n",
    "            legline.set_transform(trans)\n",
    "            legline.set_linewidth(lw)\n",
    "            leglines.append(legline)\n",
    "        return leglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_legend(mdl, top_n = 5):\n",
    "\n",
    "    topic_words = []\n",
    "    \n",
    "    for k in range(1 + mdl.k1 + mdl.k2):\n",
    "        topic_words.append([words[0] for words in mdl.get_topic_words(k, top_n)])\n",
    "        \n",
    "    str_wds = []\n",
    "    \n",
    "    for wds in topic_words:\n",
    "        str_wds.append(\", \".join(wds))   \n",
    "        \n",
    "    return str_wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(x):\n",
    "    return sorted(x.items(), key=lambda l: l[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_slopes = sort_dict(topic_results)\n",
    "top_slopes = [x[0] for x in sort_slopes[:10]]\n",
    "bottom_slopes = [x[0] for x in sort_slopes[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = create_legend(mdl, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Graphs\n",
    "color_list = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# hottest topics\n",
    "\n",
    "line_return = []\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('Top Ten Hottest Coronavirus Topics', fontsize=20)\n",
    "ax = fig.add_subplot(111)\n",
    "line = [[(0, 0)]]\n",
    "i = 0\n",
    "for n in top_slopes:\n",
    "    linear_fit = lm().fit(topic_pr_2010_2019[\"START_YEAR\"].values.reshape(-1,1), topic_pr_2010_2019.iloc[:,n].values.reshape(-1,1))\n",
    "    ax.plot(topic_pr_2010_2019[\"START_YEAR\"], (topic_pr_2010_2019[\"START_YEAR\"]*linear_fit.coef_[0][0])+linear_fit.intercept_,linestyle = 'dashed', color = color_list[i])\n",
    "    #ax.plot(topic_wts_2010_2019[\"START_YEAR\"], topic_wts_2010_2019.iloc[:,n], '-o')\n",
    "    #ax.errorbar(topic_wts_2010_2019[\"START_YEAR\"], topic_wts_2010_2019.iloc[:,n], \n",
    "                #yerr = np.array(topic_wts_se_2010_2019.iloc[:,n-1]), fmt = \"o\")\n",
    "    line_return.append(mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=[color_list[i],color_list[i]]))\n",
    "    i+=1\n",
    "\n",
    "plt.xticks(np.arange(topic_pr_2010_2019[\"START_YEAR\"].min(), topic_pr_2010_2019[\"START_YEAR\"].max()+1, 1.0), fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Year', fontsize=20)\n",
    "plt.ylabel('Mean probability', fontsize=20)\n",
    "\n",
    "# set up the proxy artist\n",
    "lc = mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=['blue','blue'])\n",
    "lc2 = mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=['orange','orange'])\n",
    "\n",
    "# create the legend\n",
    "plt.legend(line_return, [leg[x-1] for x in top_slopes], handler_map={type(line_return[0]): HandlerDashedLines()},\n",
    "          handlelength=2, handleheight=2,bbox_to_anchor=(1.05, 0.7, 0.3, 0.2), loc='upper left', fontsize = 16)\n",
    "plt.savefig(\"hpam_results/corona/hpam_corona_optimal_hot_topics.png\", dpi = 800, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# coldest topics\n",
    "\n",
    "line_return = []\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('Top Ten Coldest Coronavirus Topics', fontsize=20)\n",
    "ax = fig.add_subplot(111)\n",
    "i = 0\n",
    "for n in bottom_slopes:\n",
    "    linear_fit = lm().fit(topic_pr_2010_2019[\"START_YEAR\"].values.reshape(-1,1), topic_pr_2010_2019.iloc[:,n].values.reshape(-1,1))\n",
    "    ax.plot(topic_pr_2010_2019[\"START_YEAR\"], (topic_pr_2010_2019[\"START_YEAR\"]*linear_fit.coef_[0][0])+linear_fit.intercept_,linestyle = 'dashed', color = color_list[i])\n",
    "    #ax.plot(topic_wts_2010_2019[\"START_YEAR\"], topic_wts_2010_2019.iloc[:,n], '-o', color=color_list[i+5])\n",
    "    #ax.scatter(topic_wts_2010_2019[\"START_YEAR\"], topic_wts_2010_2019.iloc[:,n], color=color_list[i+5])\n",
    "    #ax.errorbar(topic_wts_2010_2019[\"START_YEAR\"], topic_wts_2010_2019.iloc[:,n], \n",
    "                #yerr = np.array(topic_wts_se_2010_2019.iloc[:,n-1]), fmt = \"o\",\n",
    "                #color=color_list[i+5])\n",
    "    line_return.append(mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=[color_list[i],color_list[i]]))\n",
    "    i+=1\n",
    "\n",
    "plt.xticks(np.arange(topic_pr_2010_2019[\"START_YEAR\"].min(), topic_pr_2010_2019[\"START_YEAR\"].max()+1, 1.0), fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Year', fontsize=20)\n",
    "plt.ylabel('Mean probability', fontsize=20)\n",
    "\n",
    "# set up the proxy artist\n",
    "lc = mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=['blue','blue'])\n",
    "lc2 = mcol.LineCollection(2 * line, linestyles=['solid','dashed'], colors=['orange','orange'])\n",
    "\n",
    "# create the legend\n",
    "plt.legend(line_return, [leg[x-1] for x in bottom_slopes], handler_map={type(line_return[0]): HandlerDashedLines()},\n",
    "          handlelength=2, handleheight=2,bbox_to_anchor=(1.05, 0.7, 0.3, 0.2), loc='upper left', fontsize = 16)\n",
    "\n",
    "plt.savefig(\"hpam_results/corona/hpam_corona_optimal_cold_topics.png\", dpi = 800, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
